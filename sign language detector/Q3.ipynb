{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8542ae29",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "1.   Full Name: Farhad Fallah \n",
    "2.   Student Number: 97102214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9aadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader ,Dataset \n",
    "import string\n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf8520f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27450</th>\n",
       "      <td>13</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>192</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>...</td>\n",
       "      <td>132</td>\n",
       "      <td>165</td>\n",
       "      <td>99</td>\n",
       "      <td>77</td>\n",
       "      <td>52</td>\n",
       "      <td>200</td>\n",
       "      <td>234</td>\n",
       "      <td>200</td>\n",
       "      <td>222</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27451</th>\n",
       "      <td>23</td>\n",
       "      <td>151</td>\n",
       "      <td>154</td>\n",
       "      <td>157</td>\n",
       "      <td>158</td>\n",
       "      <td>160</td>\n",
       "      <td>161</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>...</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>196</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27452</th>\n",
       "      <td>18</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>173</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>196</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27453</th>\n",
       "      <td>17</td>\n",
       "      <td>177</td>\n",
       "      <td>181</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>191</td>\n",
       "      <td>191</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>56</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>102</td>\n",
       "      <td>79</td>\n",
       "      <td>47</td>\n",
       "      <td>64</td>\n",
       "      <td>87</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27454</th>\n",
       "      <td>23</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>182</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>182</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>132</td>\n",
       "      <td>170</td>\n",
       "      <td>194</td>\n",
       "      <td>214</td>\n",
       "      <td>203</td>\n",
       "      <td>197</td>\n",
       "      <td>205</td>\n",
       "      <td>209</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27455 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0          3     107     118     127     134     139     143     146     150   \n",
       "1          6     155     157     156     156     156     157     156     158   \n",
       "2          2     187     188     188     187     187     186     187     188   \n",
       "3          2     211     211     212     212     211     210     211     210   \n",
       "4         13     164     167     170     172     176     179     180     184   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "27450     13     189     189     190     190     192     193     193     193   \n",
       "27451     23     151     154     157     158     160     161     163     164   \n",
       "27452     18     174     174     174     174     174     175     175     174   \n",
       "27453     17     177     181     184     185     187     189     190     191   \n",
       "27454     23     179     180     180     180     182     181     182     183   \n",
       "\n",
       "       pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0         153  ...       207       207       207       207       206   \n",
       "1         158  ...        69       149       128        87        94   \n",
       "2         187  ...       202       201       200       199       198   \n",
       "3         210  ...       235       234       233       231       230   \n",
       "4         185  ...        92       105       105       108       133   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "27450     193  ...       132       165        99        77        52   \n",
       "27451     166  ...       198       198       198       198       198   \n",
       "27452     173  ...       121       196       209       208       206   \n",
       "27453     191  ...       119        56        27        58       102   \n",
       "27454     182  ...       108       132       170       194       214   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0           206       206       204       203       202  \n",
       "1           163       175       103       135       149  \n",
       "2           199       198       195       194       195  \n",
       "3           226       225       222       229       163  \n",
       "4           163       157       163       164       179  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "27450       200       234       200       222       225  \n",
       "27451       196       195       195       195       194  \n",
       "27452       204       203       202       200       200  \n",
       "27453        79        47        64        87        93  \n",
       "27454       203       197       205       209       215  \n",
       "\n",
       "[27455 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('HW2_data\\Q3_train.csv')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b78e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1605ddab760>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = df_train.to_numpy() \n",
    "train.shape\n",
    "data = train[2,1:].reshape(28,28)\n",
    "plt.imshow(data,cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28023a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = string.ascii_lowercase\n",
    "odict = OrderedDict()\n",
    "mydict = {}\n",
    "for i,s in enumerate(st):\n",
    "    mydict[i] = s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6448f",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### displaying 9 random images with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c10348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(3,3 , figsize =(5,5))\n",
    "\n",
    "a = np.random.randint(len(train),size=9)\n",
    "\n",
    "sample = train[a,:]\n",
    "for i,row in enumerate(sample):\n",
    "    label = mydict[row[0]]\n",
    "    img = row[1:].reshape(28,28)\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(img,cmap='gray')\n",
    "    plt.title(label)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817535cb",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "#### at first we define our dataset and nerual network structure \n",
    "#### then we choose our optimizer, one time train our model with SGD and then try Adam\n",
    "#### as we can see in results we reached  more accuracy in less epochs when we used Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2211b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train, transform=None, target_transform=None):\n",
    "        #df = pd.read_csv(annotations_file)\n",
    "        self.labels = torch.tensor(train[:,0])\n",
    "        self.features = torch.tensor(train[:,1:])\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        feature = self.features[idx,:]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            feature = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6310d04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21964 5491\n"
     ]
    }
   ],
   "source": [
    "training_rate = 0.8\n",
    "train_data = CustomDataset(train[0:int(training_rate * len(train))])\n",
    "val_data = CustomDataset(train[int(training_rate * len(train)):])\n",
    "print(len(train_data) , len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff3a271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ef5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 25),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        #y_pred = logits.argmax(dim=1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd002a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=25, bias=True)\n",
      "    (7): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97cbc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        #print(y.shape)\n",
    "        X = X.float()\n",
    "        pred = model(X)\n",
    "        \n",
    "        #print(pred)\n",
    "        #print(type(y))\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            \n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e4fa7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.231572  [    0/21964]\n",
      "loss: 3.208072  [ 5000/21964]\n",
      "loss: 3.113997  [10000/21964]\n",
      "loss: 3.135655  [15000/21964]\n",
      "loss: 3.116054  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 18.4%, Avg loss: 3.112272 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.114867  [    0/21964]\n",
      "loss: 3.172269  [ 5000/21964]\n",
      "loss: 3.069210  [10000/21964]\n",
      "loss: 3.184251  [15000/21964]\n",
      "loss: 3.043111  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 23.3%, Avg loss: 3.065649 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.139724  [    0/21964]\n",
      "loss: 3.109845  [ 5000/21964]\n",
      "loss: 3.061316  [10000/21964]\n",
      "loss: 3.051459  [15000/21964]\n",
      "loss: 3.061553  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 3.050256 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.063191  [    0/21964]\n",
      "loss: 3.066593  [ 5000/21964]\n",
      "loss: 3.083307  [10000/21964]\n",
      "loss: 3.067957  [15000/21964]\n",
      "loss: 3.075191  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 24.1%, Avg loss: 3.054752 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.102479  [    0/21964]\n",
      "loss: 3.044686  [ 5000/21964]\n",
      "loss: 3.035858  [10000/21964]\n",
      "loss: 3.045129  [15000/21964]\n",
      "loss: 3.035302  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 25.4%, Avg loss: 3.035083 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.012790  [    0/21964]\n",
      "loss: 3.023903  [ 5000/21964]\n",
      "loss: 3.033702  [10000/21964]\n",
      "loss: 3.038377  [15000/21964]\n",
      "loss: 3.105625  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 29.4%, Avg loss: 3.000872 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.981217  [    0/21964]\n",
      "loss: 3.052360  [ 5000/21964]\n",
      "loss: 2.931616  [10000/21964]\n",
      "loss: 3.008825  [15000/21964]\n",
      "loss: 3.042048  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 29.6%, Avg loss: 2.995325 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.941315  [    0/21964]\n",
      "loss: 3.036155  [ 5000/21964]\n",
      "loss: 2.999051  [10000/21964]\n",
      "loss: 2.981521  [15000/21964]\n",
      "loss: 3.093792  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 29.8%, Avg loss: 2.992038 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.952474  [    0/21964]\n",
      "loss: 2.973412  [ 5000/21964]\n",
      "loss: 3.020280  [10000/21964]\n",
      "loss: 3.001460  [15000/21964]\n",
      "loss: 2.985663  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 29.9%, Avg loss: 2.987235 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.013549  [    0/21964]\n",
      "loss: 2.984834  [ 5000/21964]\n",
      "loss: 3.031005  [10000/21964]\n",
      "loss: 2.989308  [15000/21964]\n",
      "loss: 2.993591  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 29.9%, Avg loss: 2.986525 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 3.031935  [    0/21964]\n",
      "loss: 3.016058  [ 5000/21964]\n",
      "loss: 2.982350  [10000/21964]\n",
      "loss: 2.994967  [15000/21964]\n",
      "loss: 2.937521  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.0%, Avg loss: 2.984981 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 3.002534  [    0/21964]\n",
      "loss: 3.003843  [ 5000/21964]\n",
      "loss: 3.033131  [10000/21964]\n",
      "loss: 2.913827  [15000/21964]\n",
      "loss: 3.090515  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.0%, Avg loss: 2.983632 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.923582  [    0/21964]\n",
      "loss: 2.961485  [ 5000/21964]\n",
      "loss: 3.011379  [10000/21964]\n",
      "loss: 2.934127  [15000/21964]\n",
      "loss: 3.020051  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.0%, Avg loss: 2.983968 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 3.051368  [    0/21964]\n",
      "loss: 2.984113  [ 5000/21964]\n",
      "loss: 2.992759  [10000/21964]\n",
      "loss: 2.904012  [15000/21964]\n",
      "loss: 3.026786  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.0%, Avg loss: 2.982968 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.922255  [    0/21964]\n",
      "loss: 2.971630  [ 5000/21964]\n",
      "loss: 2.942549  [10000/21964]\n",
      "loss: 2.999084  [15000/21964]\n",
      "loss: 2.964545  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.982211 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 3.020453  [    0/21964]\n",
      "loss: 3.029382  [ 5000/21964]\n",
      "loss: 3.014307  [10000/21964]\n",
      "loss: 2.999619  [15000/21964]\n",
      "loss: 3.021437  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.981390 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 3.059201  [    0/21964]\n",
      "loss: 2.970583  [ 5000/21964]\n",
      "loss: 3.011733  [10000/21964]\n",
      "loss: 2.990779  [15000/21964]\n",
      "loss: 3.039545  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.980991 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 3.047625  [    0/21964]\n",
      "loss: 2.939704  [ 5000/21964]\n",
      "loss: 3.023679  [10000/21964]\n",
      "loss: 3.098728  [15000/21964]\n",
      "loss: 2.900732  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.980608 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.941196  [    0/21964]\n",
      "loss: 3.049266  [ 5000/21964]\n",
      "loss: 2.975006  [10000/21964]\n",
      "loss: 2.939193  [15000/21964]\n",
      "loss: 3.027558  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.980585 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.942664  [    0/21964]\n",
      "loss: 3.008588  [ 5000/21964]\n",
      "loss: 3.019724  [10000/21964]\n",
      "loss: 2.961474  [15000/21964]\n",
      "loss: 2.932674  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.979911 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 3.001529  [    0/21964]\n",
      "loss: 3.010126  [ 5000/21964]\n",
      "loss: 2.980991  [10000/21964]\n",
      "loss: 2.930244  [15000/21964]\n",
      "loss: 3.108166  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.979899 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.970715  [    0/21964]\n",
      "loss: 3.001090  [ 5000/21964]\n",
      "loss: 2.912167  [10000/21964]\n",
      "loss: 3.077880  [15000/21964]\n",
      "loss: 3.008640  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.979669 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 3.020347  [    0/21964]\n",
      "loss: 3.038718  [ 5000/21964]\n",
      "loss: 3.027749  [10000/21964]\n",
      "loss: 3.021248  [15000/21964]\n",
      "loss: 2.949813  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.979475 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.960100  [    0/21964]\n",
      "loss: 3.019025  [ 5000/21964]\n",
      "loss: 2.929408  [10000/21964]\n",
      "loss: 3.017966  [15000/21964]\n",
      "loss: 3.037973  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.979684 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 3.037248  [    0/21964]\n",
      "loss: 3.028961  [ 5000/21964]\n",
      "loss: 2.950876  [10000/21964]\n",
      "loss: 2.979886  [15000/21964]\n",
      "loss: 3.008606  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.978892 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 3.097202  [    0/21964]\n",
      "loss: 3.009165  [ 5000/21964]\n",
      "loss: 3.038077  [10000/21964]\n",
      "loss: 2.960508  [15000/21964]\n",
      "loss: 2.981485  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.978928 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 3.008449  [    0/21964]\n",
      "loss: 3.037838  [ 5000/21964]\n",
      "loss: 3.038254  [10000/21964]\n",
      "loss: 2.968623  [15000/21964]\n",
      "loss: 3.046962  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.978594 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.999102  [    0/21964]\n",
      "loss: 2.949281  [ 5000/21964]\n",
      "loss: 2.958857  [10000/21964]\n",
      "loss: 2.959491  [15000/21964]\n",
      "loss: 3.017614  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.978764 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.997482  [    0/21964]\n",
      "loss: 2.968574  [ 5000/21964]\n",
      "loss: 3.048019  [10000/21964]\n",
      "loss: 2.960178  [15000/21964]\n",
      "loss: 3.077522  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 2.978523 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 3.019500  [    0/21964]\n",
      "loss: 2.987289  [ 5000/21964]\n",
      "loss: 3.029203  [10000/21964]\n",
      "loss: 3.010564  [15000/21964]\n",
      "loss: 2.981555  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 2.961660 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 3.017053  [    0/21964]\n",
      "loss: 2.978273  [ 5000/21964]\n",
      "loss: 3.051521  [10000/21964]\n",
      "loss: 2.890369  [15000/21964]\n",
      "loss: 3.064008  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 2.952139 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.942875  [    0/21964]\n",
      "loss: 2.921896  [ 5000/21964]\n",
      "loss: 2.954860  [10000/21964]\n",
      "loss: 3.023065  [15000/21964]\n",
      "loss: 2.935756  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 2.949876 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.964571  [    0/21964]\n",
      "loss: 3.011512  [ 5000/21964]\n",
      "loss: 3.001032  [10000/21964]\n",
      "loss: 2.961818  [15000/21964]\n",
      "loss: 2.999343  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 2.946247 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.893061  [    0/21964]\n",
      "loss: 2.964222  [ 5000/21964]\n",
      "loss: 3.002488  [10000/21964]\n",
      "loss: 2.968104  [15000/21964]\n",
      "loss: 2.994710  [20000/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 2.949026 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.938175  [    0/21964]\n",
      "loss: 2.988118  [ 5000/21964]\n",
      "loss: 2.909990  [10000/21964]\n",
      "loss: 2.909602  [15000/21964]\n",
      "loss: 2.922662  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 2.944954 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 3.017778  [    0/21964]\n",
      "loss: 2.952032  [ 5000/21964]\n",
      "loss: 3.029471  [10000/21964]\n",
      "loss: 3.059601  [15000/21964]\n",
      "loss: 2.969605  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 2.945296 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.925529  [    0/21964]\n",
      "loss: 2.983582  [ 5000/21964]\n",
      "loss: 2.888410  [10000/21964]\n",
      "loss: 2.989375  [15000/21964]\n",
      "loss: 2.958378  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 2.942191 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.922944  [    0/21964]\n",
      "loss: 3.038594  [ 5000/21964]\n",
      "loss: 2.910111  [10000/21964]\n",
      "loss: 2.896136  [15000/21964]\n",
      "loss: 2.901203  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 2.942596 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.980931  [    0/21964]\n",
      "loss: 2.981857  [ 5000/21964]\n",
      "loss: 3.021684  [10000/21964]\n",
      "loss: 2.971932  [15000/21964]\n",
      "loss: 2.970876  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 2.940886 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.912079  [    0/21964]\n",
      "loss: 2.934341  [ 5000/21964]\n",
      "loss: 2.989296  [10000/21964]\n",
      "loss: 2.833142  [15000/21964]\n",
      "loss: 2.972594  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 2.913454 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.938487  [    0/21964]\n",
      "loss: 2.993769  [ 5000/21964]\n",
      "loss: 3.020560  [10000/21964]\n",
      "loss: 2.898226  [15000/21964]\n",
      "loss: 2.885469  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 2.908463 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.864372  [    0/21964]\n",
      "loss: 2.941588  [ 5000/21964]\n",
      "loss: 2.920066  [10000/21964]\n",
      "loss: 2.972191  [15000/21964]\n",
      "loss: 3.003207  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.903842 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.964302  [    0/21964]\n",
      "loss: 2.916013  [ 5000/21964]\n",
      "loss: 2.933245  [10000/21964]\n",
      "loss: 3.025188  [15000/21964]\n",
      "loss: 2.951104  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.900605 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.958863  [    0/21964]\n",
      "loss: 2.875381  [ 5000/21964]\n",
      "loss: 2.934742  [10000/21964]\n",
      "loss: 2.935257  [15000/21964]\n",
      "loss: 2.953037  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.900470 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.930027  [    0/21964]\n",
      "loss: 2.963111  [ 5000/21964]\n",
      "loss: 2.990272  [10000/21964]\n",
      "loss: 2.893387  [15000/21964]\n",
      "loss: 2.941254  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.899036 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.864666  [    0/21964]\n",
      "loss: 2.868204  [ 5000/21964]\n",
      "loss: 2.905399  [10000/21964]\n",
      "loss: 2.895558  [15000/21964]\n",
      "loss: 2.874018  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.898530 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.919353  [    0/21964]\n",
      "loss: 2.881328  [ 5000/21964]\n",
      "loss: 2.893497  [10000/21964]\n",
      "loss: 2.903712  [15000/21964]\n",
      "loss: 2.959559  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.897435 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.909229  [    0/21964]\n",
      "loss: 2.900957  [ 5000/21964]\n",
      "loss: 2.899752  [10000/21964]\n",
      "loss: 2.960671  [15000/21964]\n",
      "loss: 2.911942  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.899622 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.945075  [    0/21964]\n",
      "loss: 2.824550  [ 5000/21964]\n",
      "loss: 2.943660  [10000/21964]\n",
      "loss: 2.905486  [15000/21964]\n",
      "loss: 2.960402  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.895935 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.960172  [    0/21964]\n",
      "loss: 2.853442  [ 5000/21964]\n",
      "loss: 2.963022  [10000/21964]\n",
      "loss: 2.893263  [15000/21964]\n",
      "loss: 2.872196  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 2.894541 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.990237  [    0/21964]\n",
      "loss: 2.852332  [ 5000/21964]\n",
      "loss: 2.941126  [10000/21964]\n",
      "loss: 2.924741  [15000/21964]\n",
      "loss: 2.835242  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 2.893492 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.940365  [    0/21964]\n",
      "loss: 2.962553  [ 5000/21964]\n",
      "loss: 2.880029  [10000/21964]\n",
      "loss: 2.956019  [15000/21964]\n",
      "loss: 2.904192  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 40.5%, Avg loss: 2.884892 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.869753  [    0/21964]\n",
      "loss: 2.899561  [ 5000/21964]\n",
      "loss: 2.857707  [10000/21964]\n",
      "loss: 2.826413  [15000/21964]\n",
      "loss: 2.912706  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 2.869773 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.881843  [    0/21964]\n",
      "loss: 2.884034  [ 5000/21964]\n",
      "loss: 2.947076  [10000/21964]\n",
      "loss: 2.904298  [15000/21964]\n",
      "loss: 2.819063  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 2.865727 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 3.006827  [    0/21964]\n",
      "loss: 2.855379  [ 5000/21964]\n",
      "loss: 2.880296  [10000/21964]\n",
      "loss: 2.932332  [15000/21964]\n",
      "loss: 2.859945  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 2.871836 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.816929  [    0/21964]\n",
      "loss: 2.962265  [ 5000/21964]\n",
      "loss: 2.910048  [10000/21964]\n",
      "loss: 2.931131  [15000/21964]\n",
      "loss: 2.856864  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 42.5%, Avg loss: 2.860834 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.824162  [    0/21964]\n",
      "loss: 2.832678  [ 5000/21964]\n",
      "loss: 2.864493  [10000/21964]\n",
      "loss: 2.874441  [15000/21964]\n",
      "loss: 2.855791  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 2.876211 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.911093  [    0/21964]\n",
      "loss: 2.846562  [ 5000/21964]\n",
      "loss: 2.907925  [10000/21964]\n",
      "loss: 2.883247  [15000/21964]\n",
      "loss: 2.870313  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 42.7%, Avg loss: 2.858787 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.929244  [    0/21964]\n",
      "loss: 2.836914  [ 5000/21964]\n",
      "loss: 2.818393  [10000/21964]\n",
      "loss: 2.796355  [15000/21964]\n",
      "loss: 2.872809  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 45.9%, Avg loss: 2.837095 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.841641  [    0/21964]\n",
      "loss: 2.861636  [ 5000/21964]\n",
      "loss: 2.866444  [10000/21964]\n",
      "loss: 2.889747  [15000/21964]\n",
      "loss: 2.815897  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 2.841196 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 2.875840  [    0/21964]\n",
      "loss: 2.794255  [ 5000/21964]\n",
      "loss: 2.791137  [10000/21964]\n",
      "loss: 2.893322  [15000/21964]\n",
      "loss: 2.809517  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 2.838795 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 2.842017  [    0/21964]\n",
      "loss: 2.866039  [ 5000/21964]\n",
      "loss: 2.867820  [10000/21964]\n",
      "loss: 2.849491  [15000/21964]\n",
      "loss: 2.886110  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.5%, Avg loss: 2.827879 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 2.857434  [    0/21964]\n",
      "loss: 2.803506  [ 5000/21964]\n",
      "loss: 2.819407  [10000/21964]\n",
      "loss: 2.890609  [15000/21964]\n",
      "loss: 2.859343  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.1%, Avg loss: 2.839118 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 2.798245  [    0/21964]\n",
      "loss: 2.763396  [ 5000/21964]\n",
      "loss: 2.869881  [10000/21964]\n",
      "loss: 2.891324  [15000/21964]\n",
      "loss: 2.846786  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.5%, Avg loss: 2.824834 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 2.864284  [    0/21964]\n",
      "loss: 2.797174  [ 5000/21964]\n",
      "loss: 2.828861  [10000/21964]\n",
      "loss: 2.876150  [15000/21964]\n",
      "loss: 2.920481  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 2.822280 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 2.826006  [    0/21964]\n",
      "loss: 2.912210  [ 5000/21964]\n",
      "loss: 2.837689  [10000/21964]\n",
      "loss: 2.825201  [15000/21964]\n",
      "loss: 2.809800  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 2.820025 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 2.728462  [    0/21964]\n",
      "loss: 2.753849  [ 5000/21964]\n",
      "loss: 2.891226  [10000/21964]\n",
      "loss: 2.790389  [15000/21964]\n",
      "loss: 2.834280  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.819786 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 2.850355  [    0/21964]\n",
      "loss: 2.789143  [ 5000/21964]\n",
      "loss: 2.796563  [10000/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.814534  [15000/21964]\n",
      "loss: 2.866592  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.819075 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 2.805330  [    0/21964]\n",
      "loss: 2.824183  [ 5000/21964]\n",
      "loss: 2.870671  [10000/21964]\n",
      "loss: 2.901091  [15000/21964]\n",
      "loss: 2.842489  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.818499 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 2.883023  [    0/21964]\n",
      "loss: 2.786233  [ 5000/21964]\n",
      "loss: 2.774708  [10000/21964]\n",
      "loss: 2.802916  [15000/21964]\n",
      "loss: 2.815097  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.817022 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 2.862158  [    0/21964]\n",
      "loss: 2.762586  [ 5000/21964]\n",
      "loss: 2.903108  [10000/21964]\n",
      "loss: 2.804833  [15000/21964]\n",
      "loss: 2.828205  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.816478 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 2.784041  [    0/21964]\n",
      "loss: 2.802330  [ 5000/21964]\n",
      "loss: 2.833432  [10000/21964]\n",
      "loss: 2.863770  [15000/21964]\n",
      "loss: 2.902521  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.816560 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 2.814280  [    0/21964]\n",
      "loss: 2.782077  [ 5000/21964]\n",
      "loss: 2.779010  [10000/21964]\n",
      "loss: 2.813995  [15000/21964]\n",
      "loss: 2.875592  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.817934 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 2.873114  [    0/21964]\n",
      "loss: 2.772227  [ 5000/21964]\n",
      "loss: 2.826319  [10000/21964]\n",
      "loss: 2.853858  [15000/21964]\n",
      "loss: 2.851505  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.815915 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 2.786070  [    0/21964]\n",
      "loss: 2.821971  [ 5000/21964]\n",
      "loss: 2.796200  [10000/21964]\n",
      "loss: 2.786012  [15000/21964]\n",
      "loss: 2.852190  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.815500 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 2.864129  [    0/21964]\n",
      "loss: 2.938496  [ 5000/21964]\n",
      "loss: 2.910742  [10000/21964]\n",
      "loss: 2.766326  [15000/21964]\n",
      "loss: 2.783811  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.816043 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 2.820939  [    0/21964]\n",
      "loss: 2.794702  [ 5000/21964]\n",
      "loss: 2.831845  [10000/21964]\n",
      "loss: 2.823130  [15000/21964]\n",
      "loss: 2.763396  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.814795 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 2.947525  [    0/21964]\n",
      "loss: 2.774258  [ 5000/21964]\n",
      "loss: 2.743292  [10000/21964]\n",
      "loss: 2.860954  [15000/21964]\n",
      "loss: 2.781611  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.815396 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 2.881403  [    0/21964]\n",
      "loss: 2.814338  [ 5000/21964]\n",
      "loss: 2.892394  [10000/21964]\n",
      "loss: 2.752905  [15000/21964]\n",
      "loss: 2.881142  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.814438 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 2.820130  [    0/21964]\n",
      "loss: 2.783572  [ 5000/21964]\n",
      "loss: 2.862077  [10000/21964]\n",
      "loss: 2.813074  [15000/21964]\n",
      "loss: 2.810948  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.814435 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 2.849849  [    0/21964]\n",
      "loss: 2.908440  [ 5000/21964]\n",
      "loss: 2.784184  [10000/21964]\n",
      "loss: 2.713045  [15000/21964]\n",
      "loss: 2.771769  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.814705 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 2.790126  [    0/21964]\n",
      "loss: 2.761539  [ 5000/21964]\n",
      "loss: 2.792133  [10000/21964]\n",
      "loss: 2.772910  [15000/21964]\n",
      "loss: 2.762383  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.814088 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 2.780748  [    0/21964]\n",
      "loss: 2.842044  [ 5000/21964]\n",
      "loss: 2.810430  [10000/21964]\n",
      "loss: 2.785047  [15000/21964]\n",
      "loss: 2.862441  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.813689 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 2.829005  [    0/21964]\n",
      "loss: 2.860583  [ 5000/21964]\n",
      "loss: 2.958977  [10000/21964]\n",
      "loss: 2.796105  [15000/21964]\n",
      "loss: 2.812322  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.813730 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 2.831968  [    0/21964]\n",
      "loss: 2.853683  [ 5000/21964]\n",
      "loss: 2.715971  [10000/21964]\n",
      "loss: 2.811625  [15000/21964]\n",
      "loss: 2.723418  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.813532 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 2.753516  [    0/21964]\n",
      "loss: 2.762632  [ 5000/21964]\n",
      "loss: 2.908366  [10000/21964]\n",
      "loss: 2.791780  [15000/21964]\n",
      "loss: 2.831731  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.813235 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 2.830257  [    0/21964]\n",
      "loss: 2.848360  [ 5000/21964]\n",
      "loss: 2.820835  [10000/21964]\n",
      "loss: 2.742950  [15000/21964]\n",
      "loss: 2.917722  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.813149 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 2.859231  [    0/21964]\n",
      "loss: 2.771926  [ 5000/21964]\n",
      "loss: 2.790819  [10000/21964]\n",
      "loss: 2.857884  [15000/21964]\n",
      "loss: 2.704022  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 47.4%, Avg loss: 2.814021 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 2.799916  [    0/21964]\n",
      "loss: 2.758571  [ 5000/21964]\n",
      "loss: 2.798290  [10000/21964]\n",
      "loss: 2.786564  [15000/21964]\n",
      "loss: 2.840910  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 2.799462 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 2.766798  [    0/21964]\n",
      "loss: 2.796933  [ 5000/21964]\n",
      "loss: 2.775189  [10000/21964]\n",
      "loss: 2.907897  [15000/21964]\n",
      "loss: 2.894451  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 49.9%, Avg loss: 2.794481 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 2.720911  [    0/21964]\n",
      "loss: 2.745268  [ 5000/21964]\n",
      "loss: 2.806302  [10000/21964]\n",
      "loss: 2.775929  [15000/21964]\n",
      "loss: 2.702039  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 2.791865 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 2.674533  [    0/21964]\n",
      "loss: 2.895329  [ 5000/21964]\n",
      "loss: 2.782433  [10000/21964]\n",
      "loss: 2.753986  [15000/21964]\n",
      "loss: 2.817462  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.788017 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 2.889375  [    0/21964]\n",
      "loss: 2.782596  [ 5000/21964]\n",
      "loss: 2.897522  [10000/21964]\n",
      "loss: 2.804079  [15000/21964]\n",
      "loss: 2.845607  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 48.6%, Avg loss: 2.798888 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 2.791394  [    0/21964]\n",
      "loss: 2.764254  [ 5000/21964]\n",
      "loss: 2.793553  [10000/21964]\n",
      "loss: 2.814226  [15000/21964]\n",
      "loss: 2.813601  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.783405 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 2.829977  [    0/21964]\n",
      "loss: 2.769379  [ 5000/21964]\n",
      "loss: 2.782672  [10000/21964]\n",
      "loss: 2.826647  [15000/21964]\n",
      "loss: 2.885917  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 2.785399 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 2.844457  [    0/21964]\n",
      "loss: 2.776443  [ 5000/21964]\n",
      "loss: 2.929878  [10000/21964]\n",
      "loss: 2.873648  [15000/21964]\n",
      "loss: 2.700220  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.781352 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 2.862412  [    0/21964]\n",
      "loss: 2.758458  [ 5000/21964]\n",
      "loss: 2.765458  [10000/21964]\n",
      "loss: 2.764522  [15000/21964]\n",
      "loss: 2.746828  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.782518 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 2.791369  [    0/21964]\n",
      "loss: 2.815156  [ 5000/21964]\n",
      "loss: 2.832375  [10000/21964]\n",
      "loss: 2.793073  [15000/21964]\n",
      "loss: 2.806102  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.780375 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 2.698359  [    0/21964]\n",
      "loss: 2.913034  [ 5000/21964]\n",
      "loss: 2.859833  [10000/21964]\n",
      "loss: 2.745116  [15000/21964]\n",
      "loss: 2.775436  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.780449 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 2.832457  [    0/21964]\n",
      "loss: 2.791333  [ 5000/21964]\n",
      "loss: 2.766472  [10000/21964]\n",
      "loss: 2.850688  [15000/21964]\n",
      "loss: 2.816087  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.779704 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 2.822875  [    0/21964]\n",
      "loss: 2.776331  [ 5000/21964]\n",
      "loss: 2.774192  [10000/21964]\n",
      "loss: 2.734665  [15000/21964]\n",
      "loss: 2.734080  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 2.786866 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 2.768206  [    0/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.833530  [ 5000/21964]\n",
      "loss: 2.882855  [10000/21964]\n",
      "loss: 2.781440  [15000/21964]\n",
      "loss: 2.784547  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.779615 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 2.814541  [    0/21964]\n",
      "loss: 2.802546  [ 5000/21964]\n",
      "loss: 2.842551  [10000/21964]\n",
      "loss: 2.842002  [15000/21964]\n",
      "loss: 2.752487  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.779235 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 2.766874  [    0/21964]\n",
      "loss: 2.785306  [ 5000/21964]\n",
      "loss: 2.861473  [10000/21964]\n",
      "loss: 2.880366  [15000/21964]\n",
      "loss: 2.793817  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.778582 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 2.803759  [    0/21964]\n",
      "loss: 2.862850  [ 5000/21964]\n",
      "loss: 2.782261  [10000/21964]\n",
      "loss: 2.772625  [15000/21964]\n",
      "loss: 2.770616  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.779115 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 2.822397  [    0/21964]\n",
      "loss: 2.763022  [ 5000/21964]\n",
      "loss: 2.693346  [10000/21964]\n",
      "loss: 2.782032  [15000/21964]\n",
      "loss: 2.764058  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.778208 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 2.793340  [    0/21964]\n",
      "loss: 2.849539  [ 5000/21964]\n",
      "loss: 2.848752  [10000/21964]\n",
      "loss: 2.843764  [15000/21964]\n",
      "loss: 2.850321  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.778279 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 2.811558  [    0/21964]\n",
      "loss: 2.771696  [ 5000/21964]\n",
      "loss: 2.722649  [10000/21964]\n",
      "loss: 2.850425  [15000/21964]\n",
      "loss: 2.761768  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.777994 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 2.752682  [    0/21964]\n",
      "loss: 2.850179  [ 5000/21964]\n",
      "loss: 2.829266  [10000/21964]\n",
      "loss: 2.792326  [15000/21964]\n",
      "loss: 2.750829  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.778222 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 2.910590  [    0/21964]\n",
      "loss: 2.742006  [ 5000/21964]\n",
      "loss: 2.841200  [10000/21964]\n",
      "loss: 2.791963  [15000/21964]\n",
      "loss: 2.839432  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.778082 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 2.744402  [    0/21964]\n",
      "loss: 2.860183  [ 5000/21964]\n",
      "loss: 2.783031  [10000/21964]\n",
      "loss: 2.772098  [15000/21964]\n",
      "loss: 2.821985  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.777793 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 2.791306  [    0/21964]\n",
      "loss: 2.671875  [ 5000/21964]\n",
      "loss: 2.730928  [10000/21964]\n",
      "loss: 2.685037  [15000/21964]\n",
      "loss: 2.820032  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.777543 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 2.850106  [    0/21964]\n",
      "loss: 2.899157  [ 5000/21964]\n",
      "loss: 2.762459  [10000/21964]\n",
      "loss: 2.740679  [15000/21964]\n",
      "loss: 2.809346  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.777368 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 2.848437  [    0/21964]\n",
      "loss: 2.820354  [ 5000/21964]\n",
      "loss: 2.811435  [10000/21964]\n",
      "loss: 2.721070  [15000/21964]\n",
      "loss: 2.742738  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.777386 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 2.811505  [    0/21964]\n",
      "loss: 2.840241  [ 5000/21964]\n",
      "loss: 2.713839  [10000/21964]\n",
      "loss: 2.830587  [15000/21964]\n",
      "loss: 2.898923  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.777416 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 2.743328  [    0/21964]\n",
      "loss: 2.889271  [ 5000/21964]\n",
      "loss: 2.751849  [10000/21964]\n",
      "loss: 2.740096  [15000/21964]\n",
      "loss: 2.772866  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.777214 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 2.809558  [    0/21964]\n",
      "loss: 2.839772  [ 5000/21964]\n",
      "loss: 2.780803  [10000/21964]\n",
      "loss: 2.810264  [15000/21964]\n",
      "loss: 2.760127  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.777181 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 2.771878  [    0/21964]\n",
      "loss: 2.780478  [ 5000/21964]\n",
      "loss: 2.741873  [10000/21964]\n",
      "loss: 2.809011  [15000/21964]\n",
      "loss: 2.799837  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776996 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 2.751571  [    0/21964]\n",
      "loss: 2.789751  [ 5000/21964]\n",
      "loss: 2.907232  [10000/21964]\n",
      "loss: 2.733214  [15000/21964]\n",
      "loss: 2.820280  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776932 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 2.878166  [    0/21964]\n",
      "loss: 2.780164  [ 5000/21964]\n",
      "loss: 2.741549  [10000/21964]\n",
      "loss: 2.809115  [15000/21964]\n",
      "loss: 2.809700  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.777000 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 2.731546  [    0/21964]\n",
      "loss: 2.830824  [ 5000/21964]\n",
      "loss: 2.869283  [10000/21964]\n",
      "loss: 2.810745  [15000/21964]\n",
      "loss: 2.761563  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776836 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 2.789681  [    0/21964]\n",
      "loss: 2.838665  [ 5000/21964]\n",
      "loss: 2.741384  [10000/21964]\n",
      "loss: 2.791251  [15000/21964]\n",
      "loss: 2.711776  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776618 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 2.721291  [    0/21964]\n",
      "loss: 2.819894  [ 5000/21964]\n",
      "loss: 2.789685  [10000/21964]\n",
      "loss: 2.771372  [15000/21964]\n",
      "loss: 2.819817  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776832 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 2.712112  [    0/21964]\n",
      "loss: 2.732222  [ 5000/21964]\n",
      "loss: 2.829268  [10000/21964]\n",
      "loss: 2.671357  [15000/21964]\n",
      "loss: 2.751339  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776756 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 2.849799  [    0/21964]\n",
      "loss: 2.849297  [ 5000/21964]\n",
      "loss: 2.820938  [10000/21964]\n",
      "loss: 2.800860  [15000/21964]\n",
      "loss: 2.779808  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776416 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 2.731519  [    0/21964]\n",
      "loss: 2.759953  [ 5000/21964]\n",
      "loss: 2.700940  [10000/21964]\n",
      "loss: 2.751260  [15000/21964]\n",
      "loss: 2.633171  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776471 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 2.819630  [    0/21964]\n",
      "loss: 2.780269  [ 5000/21964]\n",
      "loss: 2.829975  [10000/21964]\n",
      "loss: 2.850689  [15000/21964]\n",
      "loss: 2.829690  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776347 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 2.741020  [    0/21964]\n",
      "loss: 2.858108  [ 5000/21964]\n",
      "loss: 2.818775  [10000/21964]\n",
      "loss: 2.799762  [15000/21964]\n",
      "loss: 2.858713  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776363 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 2.828844  [    0/21964]\n",
      "loss: 2.761038  [ 5000/21964]\n",
      "loss: 2.741648  [10000/21964]\n",
      "loss: 2.847911  [15000/21964]\n",
      "loss: 2.779985  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776205 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 2.719822  [    0/21964]\n",
      "loss: 2.761539  [ 5000/21964]\n",
      "loss: 2.751387  [10000/21964]\n",
      "loss: 2.790836  [15000/21964]\n",
      "loss: 2.761662  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776140 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 2.769967  [    0/21964]\n",
      "loss: 2.837908  [ 5000/21964]\n",
      "loss: 2.710702  [10000/21964]\n",
      "loss: 2.751060  [15000/21964]\n",
      "loss: 2.760929  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776361 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 2.848897  [    0/21964]\n",
      "loss: 2.789313  [ 5000/21964]\n",
      "loss: 2.809918  [10000/21964]\n",
      "loss: 2.800497  [15000/21964]\n",
      "loss: 2.780446  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776167 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 2.800080  [    0/21964]\n",
      "loss: 2.847942  [ 5000/21964]\n",
      "loss: 2.829659  [10000/21964]\n",
      "loss: 2.770541  [15000/21964]\n",
      "loss: 2.750572  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776152 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 2.790611  [    0/21964]\n",
      "loss: 2.749633  [ 5000/21964]\n",
      "loss: 2.750794  [10000/21964]\n",
      "loss: 2.789819  [15000/21964]\n",
      "loss: 2.838398  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776246 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 2.750502  [    0/21964]\n",
      "loss: 2.818681  [ 5000/21964]\n",
      "loss: 2.896515  [10000/21964]\n",
      "loss: 2.858306  [15000/21964]\n",
      "loss: 2.770461  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776015 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 2.827939  [    0/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.730242  [ 5000/21964]\n",
      "loss: 2.779452  [10000/21964]\n",
      "loss: 2.800034  [15000/21964]\n",
      "loss: 2.740095  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.776063 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 2.828998  [    0/21964]\n",
      "loss: 2.810530  [ 5000/21964]\n",
      "loss: 2.829161  [10000/21964]\n",
      "loss: 2.848200  [15000/21964]\n",
      "loss: 2.750429  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.775932 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 2.749861  [    0/21964]\n",
      "loss: 2.800394  [ 5000/21964]\n",
      "loss: 2.780023  [10000/21964]\n",
      "loss: 2.829825  [15000/21964]\n",
      "loss: 2.937275  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.775881 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 2.768527  [    0/21964]\n",
      "loss: 2.789861  [ 5000/21964]\n",
      "loss: 2.818626  [10000/21964]\n",
      "loss: 2.798889  [15000/21964]\n",
      "loss: 2.740366  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.775971 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 2.712239  [    0/21964]\n",
      "loss: 2.798361  [ 5000/21964]\n",
      "loss: 2.857634  [10000/21964]\n",
      "loss: 2.729684  [15000/21964]\n",
      "loss: 2.838238  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.775893 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 2.799987  [    0/21964]\n",
      "loss: 2.749380  [ 5000/21964]\n",
      "loss: 2.808139  [10000/21964]\n",
      "loss: 2.789499  [15000/21964]\n",
      "loss: 2.798144  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.775919 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 2.778768  [    0/21964]\n",
      "loss: 2.751591  [ 5000/21964]\n",
      "loss: 2.838470  [10000/21964]\n",
      "loss: 2.789760  [15000/21964]\n",
      "loss: 2.858287  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.775782 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 2.750123  [    0/21964]\n",
      "loss: 2.827690  [ 5000/21964]\n",
      "loss: 2.779486  [10000/21964]\n",
      "loss: 2.749922  [15000/21964]\n",
      "loss: 2.769361  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.775958 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 2.798673  [    0/21964]\n",
      "loss: 2.740108  [ 5000/21964]\n",
      "loss: 2.750695  [10000/21964]\n",
      "loss: 2.760598  [15000/21964]\n",
      "loss: 2.790991  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.775593 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 2.809024  [    0/21964]\n",
      "loss: 2.740250  [ 5000/21964]\n",
      "loss: 2.819708  [10000/21964]\n",
      "loss: 2.818633  [15000/21964]\n",
      "loss: 2.769820  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.775660 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 2.759979  [    0/21964]\n",
      "loss: 2.769332  [ 5000/21964]\n",
      "loss: 2.809297  [10000/21964]\n",
      "loss: 2.739707  [15000/21964]\n",
      "loss: 2.711545  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.775557 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 2.789223  [    0/21964]\n",
      "loss: 2.837391  [ 5000/21964]\n",
      "loss: 2.799384  [10000/21964]\n",
      "loss: 2.742279  [15000/21964]\n",
      "loss: 2.771400  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 2.780931 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 2.886576  [    0/21964]\n",
      "loss: 2.789659  [ 5000/21964]\n",
      "loss: 2.788120  [10000/21964]\n",
      "loss: 2.742676  [15000/21964]\n",
      "loss: 2.755683  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 2.752244 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 2.730609  [    0/21964]\n",
      "loss: 2.694818  [ 5000/21964]\n",
      "loss: 2.717510  [10000/21964]\n",
      "loss: 2.764838  [15000/21964]\n",
      "loss: 2.734642  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 2.748278 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 2.767058  [    0/21964]\n",
      "loss: 2.798656  [ 5000/21964]\n",
      "loss: 2.731168  [10000/21964]\n",
      "loss: 2.607886  [15000/21964]\n",
      "loss: 2.794170  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 2.745872 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 2.774375  [    0/21964]\n",
      "loss: 2.830897  [ 5000/21964]\n",
      "loss: 2.696785  [10000/21964]\n",
      "loss: 2.832347  [15000/21964]\n",
      "loss: 2.724323  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 2.743889 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 2.777414  [    0/21964]\n",
      "loss: 2.773003  [ 5000/21964]\n",
      "loss: 2.742357  [10000/21964]\n",
      "loss: 2.712420  [15000/21964]\n",
      "loss: 2.811617  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 2.745156 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 2.755372  [    0/21964]\n",
      "loss: 2.852005  [ 5000/21964]\n",
      "loss: 2.762758  [10000/21964]\n",
      "loss: 2.754494  [15000/21964]\n",
      "loss: 2.857679  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 2.741741 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 2.765477  [    0/21964]\n",
      "loss: 2.649014  [ 5000/21964]\n",
      "loss: 2.704633  [10000/21964]\n",
      "loss: 2.785330  [15000/21964]\n",
      "loss: 2.751798  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 2.741340 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 2.742768  [    0/21964]\n",
      "loss: 2.719731  [ 5000/21964]\n",
      "loss: 2.752641  [10000/21964]\n",
      "loss: 2.763526  [15000/21964]\n",
      "loss: 2.694381  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 2.740640 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 2.782856  [    0/21964]\n",
      "loss: 2.761206  [ 5000/21964]\n",
      "loss: 2.683385  [10000/21964]\n",
      "loss: 2.755318  [15000/21964]\n",
      "loss: 2.706773  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 2.741329 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 2.773309  [    0/21964]\n",
      "loss: 2.728713  [ 5000/21964]\n",
      "loss: 2.710052  [10000/21964]\n",
      "loss: 2.834847  [15000/21964]\n",
      "loss: 2.786720  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 2.749504 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 2.696137  [    0/21964]\n",
      "loss: 2.780967  [ 5000/21964]\n",
      "loss: 2.713835  [10000/21964]\n",
      "loss: 2.715760  [15000/21964]\n",
      "loss: 2.816122  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 2.724936 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 2.679673  [    0/21964]\n",
      "loss: 2.640097  [ 5000/21964]\n",
      "loss: 2.748438  [10000/21964]\n",
      "loss: 2.738990  [15000/21964]\n",
      "loss: 2.770715  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 2.735211 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 2.745235  [    0/21964]\n",
      "loss: 2.806226  [ 5000/21964]\n",
      "loss: 2.678923  [10000/21964]\n",
      "loss: 2.714333  [15000/21964]\n",
      "loss: 2.722755  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 2.722132 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 2.704122  [    0/21964]\n",
      "loss: 2.745927  [ 5000/21964]\n",
      "loss: 2.785697  [10000/21964]\n",
      "loss: 2.703026  [15000/21964]\n",
      "loss: 2.788013  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 2.711862 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 2.681834  [    0/21964]\n",
      "loss: 2.767493  [ 5000/21964]\n",
      "loss: 2.687149  [10000/21964]\n",
      "loss: 2.689223  [15000/21964]\n",
      "loss: 2.729369  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 2.707728 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 2.727441  [    0/21964]\n",
      "loss: 2.708401  [ 5000/21964]\n",
      "loss: 2.757138  [10000/21964]\n",
      "loss: 2.700205  [15000/21964]\n",
      "loss: 2.744162  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 2.704757 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 2.648220  [    0/21964]\n",
      "loss: 2.687416  [ 5000/21964]\n",
      "loss: 2.687485  [10000/21964]\n",
      "loss: 2.841254  [15000/21964]\n",
      "loss: 2.823055  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 2.704261 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 2.745068  [    0/21964]\n",
      "loss: 2.674795  [ 5000/21964]\n",
      "loss: 2.795378  [10000/21964]\n",
      "loss: 2.716195  [15000/21964]\n",
      "loss: 2.774485  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 2.706168 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 2.794142  [    0/21964]\n",
      "loss: 2.732341  [ 5000/21964]\n",
      "loss: 2.726075  [10000/21964]\n",
      "loss: 2.705236  [15000/21964]\n",
      "loss: 2.701725  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 2.703230 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 2.790594  [    0/21964]\n",
      "loss: 2.657186  [ 5000/21964]\n",
      "loss: 2.737033  [10000/21964]\n",
      "loss: 2.668273  [15000/21964]\n",
      "loss: 2.668467  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 2.701221 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 2.743055  [    0/21964]\n",
      "loss: 2.759053  [ 5000/21964]\n",
      "loss: 2.774902  [10000/21964]\n",
      "loss: 2.725814  [15000/21964]\n",
      "loss: 2.716285  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 2.700682 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 2.709483  [    0/21964]\n",
      "loss: 2.724028  [ 5000/21964]\n",
      "loss: 2.614403  [10000/21964]\n",
      "loss: 2.719981  [15000/21964]\n",
      "loss: 2.780785  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 2.699471 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 2.811633  [    0/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.683709  [ 5000/21964]\n",
      "loss: 2.703320  [10000/21964]\n",
      "loss: 2.710164  [15000/21964]\n",
      "loss: 2.751626  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 2.698698 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 2.742736  [    0/21964]\n",
      "loss: 2.646204  [ 5000/21964]\n",
      "loss: 2.702471  [10000/21964]\n",
      "loss: 2.707984  [15000/21964]\n",
      "loss: 2.679810  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 2.698064 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 2.709792  [    0/21964]\n",
      "loss: 2.761318  [ 5000/21964]\n",
      "loss: 2.795918  [10000/21964]\n",
      "loss: 2.642484  [15000/21964]\n",
      "loss: 2.793802  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 2.697738 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 2.772353  [    0/21964]\n",
      "loss: 2.665244  [ 5000/21964]\n",
      "loss: 2.761941  [10000/21964]\n",
      "loss: 2.681700  [15000/21964]\n",
      "loss: 2.714581  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 2.696893 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 2.711398  [    0/21964]\n",
      "loss: 2.747859  [ 5000/21964]\n",
      "loss: 2.621548  [10000/21964]\n",
      "loss: 2.723341  [15000/21964]\n",
      "loss: 2.680307  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 2.695676 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 2.746512  [    0/21964]\n",
      "loss: 2.714246  [ 5000/21964]\n",
      "loss: 2.716829  [10000/21964]\n",
      "loss: 2.776988  [15000/21964]\n",
      "loss: 2.649823  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 2.692024 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 2.707787  [    0/21964]\n",
      "loss: 2.648569  [ 5000/21964]\n",
      "loss: 2.682554  [10000/21964]\n",
      "loss: 2.677056  [15000/21964]\n",
      "loss: 2.782935  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 2.699234 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 2.704373  [    0/21964]\n",
      "loss: 2.722034  [ 5000/21964]\n",
      "loss: 2.780845  [10000/21964]\n",
      "loss: 2.673411  [15000/21964]\n",
      "loss: 2.703755  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 2.686295 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 2.706951  [    0/21964]\n",
      "loss: 2.712232  [ 5000/21964]\n",
      "loss: 2.695600  [10000/21964]\n",
      "loss: 2.779027  [15000/21964]\n",
      "loss: 2.715421  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 2.694229 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 2.757873  [    0/21964]\n",
      "loss: 2.726681  [ 5000/21964]\n",
      "loss: 2.653444  [10000/21964]\n",
      "loss: 2.817489  [15000/21964]\n",
      "loss: 2.744887  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 2.682353 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 2.751262  [    0/21964]\n",
      "loss: 2.698613  [ 5000/21964]\n",
      "loss: 2.739276  [10000/21964]\n",
      "loss: 2.599310  [15000/21964]\n",
      "loss: 2.652338  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 2.680360 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 2.686719  [    0/21964]\n",
      "loss: 2.678398  [ 5000/21964]\n",
      "loss: 2.668577  [10000/21964]\n",
      "loss: 2.664454  [15000/21964]\n",
      "loss: 2.697173  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 2.683203 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 2.691386  [    0/21964]\n",
      "loss: 2.641592  [ 5000/21964]\n",
      "loss: 2.686697  [10000/21964]\n",
      "loss: 2.812812  [15000/21964]\n",
      "loss: 2.630620  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 2.673419 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 2.668865  [    0/21964]\n",
      "loss: 2.708226  [ 5000/21964]\n",
      "loss: 2.761723  [10000/21964]\n",
      "loss: 2.695447  [15000/21964]\n",
      "loss: 2.653529  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 2.669534 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 2.666582  [    0/21964]\n",
      "loss: 2.739299  [ 5000/21964]\n",
      "loss: 2.700445  [10000/21964]\n",
      "loss: 2.685492  [15000/21964]\n",
      "loss: 2.692771  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 2.668236 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 2.708328  [    0/21964]\n",
      "loss: 2.740312  [ 5000/21964]\n",
      "loss: 2.719489  [10000/21964]\n",
      "loss: 2.603124  [15000/21964]\n",
      "loss: 2.629195  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 2.681025 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 2.835641  [    0/21964]\n",
      "loss: 2.660516  [ 5000/21964]\n",
      "loss: 2.655666  [10000/21964]\n",
      "loss: 2.623002  [15000/21964]\n",
      "loss: 2.667463  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 2.663370 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 2.625754  [    0/21964]\n",
      "loss: 2.618370  [ 5000/21964]\n",
      "loss: 2.647705  [10000/21964]\n",
      "loss: 2.731590  [15000/21964]\n",
      "loss: 2.635450  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 2.672674 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 2.725917  [    0/21964]\n",
      "loss: 2.655892  [ 5000/21964]\n",
      "loss: 2.633977  [10000/21964]\n",
      "loss: 2.609459  [15000/21964]\n",
      "loss: 2.707379  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 2.663533 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 2.671904  [    0/21964]\n",
      "loss: 2.684748  [ 5000/21964]\n",
      "loss: 2.725891  [10000/21964]\n",
      "loss: 2.724673  [15000/21964]\n",
      "loss: 2.716234  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 2.661448 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 2.722452  [    0/21964]\n",
      "loss: 2.694400  [ 5000/21964]\n",
      "loss: 2.585928  [10000/21964]\n",
      "loss: 2.646199  [15000/21964]\n",
      "loss: 2.702685  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 2.660576 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 2.665378  [    0/21964]\n",
      "loss: 2.643878  [ 5000/21964]\n",
      "loss: 2.664034  [10000/21964]\n",
      "loss: 2.674507  [15000/21964]\n",
      "loss: 2.655649  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 2.672118 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 2.646010  [    0/21964]\n",
      "loss: 2.711952  [ 5000/21964]\n",
      "loss: 2.701184  [10000/21964]\n",
      "loss: 2.667352  [15000/21964]\n",
      "loss: 2.689589  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 2.679026 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 2.718470  [    0/21964]\n",
      "loss: 2.666880  [ 5000/21964]\n",
      "loss: 2.704060  [10000/21964]\n",
      "loss: 2.628215  [15000/21964]\n",
      "loss: 2.547284  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 2.645589 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 2.604370  [    0/21964]\n",
      "loss: 2.673104  [ 5000/21964]\n",
      "loss: 2.624780  [10000/21964]\n",
      "loss: 2.695823  [15000/21964]\n",
      "loss: 2.717947  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 2.641989 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 2.645828  [    0/21964]\n",
      "loss: 2.638706  [ 5000/21964]\n",
      "loss: 2.602973  [10000/21964]\n",
      "loss: 2.680922  [15000/21964]\n",
      "loss: 2.679759  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 2.639809 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 2.718457  [    0/21964]\n",
      "loss: 2.670449  [ 5000/21964]\n",
      "loss: 2.634389  [10000/21964]\n",
      "loss: 2.617136  [15000/21964]\n",
      "loss: 2.633092  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 2.636808 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 2.610627  [    0/21964]\n",
      "loss: 2.674724  [ 5000/21964]\n",
      "loss: 2.638123  [10000/21964]\n",
      "loss: 2.734538  [15000/21964]\n",
      "loss: 2.659216  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 2.634355 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 2.643108  [    0/21964]\n",
      "loss: 2.673469  [ 5000/21964]\n",
      "loss: 2.676979  [10000/21964]\n",
      "loss: 2.550378  [15000/21964]\n",
      "loss: 2.569963  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 2.628320 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 2.587159  [    0/21964]\n",
      "loss: 2.678957  [ 5000/21964]\n",
      "loss: 2.633908  [10000/21964]\n",
      "loss: 2.610069  [15000/21964]\n",
      "loss: 2.579418  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 2.626094 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 2.673866  [    0/21964]\n",
      "loss: 2.527414  [ 5000/21964]\n",
      "loss: 2.591157  [10000/21964]\n",
      "loss: 2.664771  [15000/21964]\n",
      "loss: 2.585560  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 2.624535 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 2.634427  [    0/21964]\n",
      "loss: 2.675742  [ 5000/21964]\n",
      "loss: 2.685236  [10000/21964]\n",
      "loss: 2.539462  [15000/21964]\n",
      "loss: 2.658000  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 2.625153 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 2.654468  [    0/21964]\n",
      "loss: 2.586892  [ 5000/21964]\n",
      "loss: 2.667618  [10000/21964]\n",
      "loss: 2.713094  [15000/21964]\n",
      "loss: 2.694458  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 2.622770 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 2.620492  [    0/21964]\n",
      "loss: 2.595111  [ 5000/21964]\n",
      "loss: 2.675179  [10000/21964]\n",
      "loss: 2.683351  [15000/21964]\n",
      "loss: 2.645511  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 2.622245 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 2.615533  [    0/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.606363  [ 5000/21964]\n",
      "loss: 2.615755  [10000/21964]\n",
      "loss: 2.610818  [15000/21964]\n",
      "loss: 2.626535  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 2.622661 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 2.503038  [    0/21964]\n",
      "loss: 2.567748  [ 5000/21964]\n",
      "loss: 2.582165  [10000/21964]\n",
      "loss: 2.657546  [15000/21964]\n",
      "loss: 2.614339  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.619971 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 2.722831  [    0/21964]\n",
      "loss: 2.633507  [ 5000/21964]\n",
      "loss: 2.619080  [10000/21964]\n",
      "loss: 2.725459  [15000/21964]\n",
      "loss: 2.645722  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.619457 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 2.665479  [    0/21964]\n",
      "loss: 2.634283  [ 5000/21964]\n",
      "loss: 2.645511  [10000/21964]\n",
      "loss: 2.683824  [15000/21964]\n",
      "loss: 2.616388  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.619544 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 2.595944  [    0/21964]\n",
      "loss: 2.615023  [ 5000/21964]\n",
      "loss: 2.634589  [10000/21964]\n",
      "loss: 2.576814  [15000/21964]\n",
      "loss: 2.701714  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.618712 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 2.652068  [    0/21964]\n",
      "loss: 2.604426  [ 5000/21964]\n",
      "loss: 2.712290  [10000/21964]\n",
      "loss: 2.653120  [15000/21964]\n",
      "loss: 2.593437  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.618349 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 2.722669  [    0/21964]\n",
      "loss: 2.665496  [ 5000/21964]\n",
      "loss: 2.545646  [10000/21964]\n",
      "loss: 2.584397  [15000/21964]\n",
      "loss: 2.614529  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.618371 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 2.565968  [    0/21964]\n",
      "loss: 2.673964  [ 5000/21964]\n",
      "loss: 2.721868  [10000/21964]\n",
      "loss: 2.645178  [15000/21964]\n",
      "loss: 2.622123  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.618213 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 2.586809  [    0/21964]\n",
      "loss: 2.672634  [ 5000/21964]\n",
      "loss: 2.732192  [10000/21964]\n",
      "loss: 2.653091  [15000/21964]\n",
      "loss: 2.642915  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.617675 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 2.623170  [    0/21964]\n",
      "loss: 2.575413  [ 5000/21964]\n",
      "loss: 2.604287  [10000/21964]\n",
      "loss: 2.691133  [15000/21964]\n",
      "loss: 2.671618  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.617815 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 2.586023  [    0/21964]\n",
      "loss: 2.683402  [ 5000/21964]\n",
      "loss: 2.712189  [10000/21964]\n",
      "loss: 2.601567  [15000/21964]\n",
      "loss: 2.663390  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.617401 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 2.663735  [    0/21964]\n",
      "loss: 2.635109  [ 5000/21964]\n",
      "loss: 2.653325  [10000/21964]\n",
      "loss: 2.584424  [15000/21964]\n",
      "loss: 2.622562  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.617641 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 2.614297  [    0/21964]\n",
      "loss: 2.613540  [ 5000/21964]\n",
      "loss: 2.622885  [10000/21964]\n",
      "loss: 2.653276  [15000/21964]\n",
      "loss: 2.603288  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.617251 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 2.682665  [    0/21964]\n",
      "loss: 2.604044  [ 5000/21964]\n",
      "loss: 2.622633  [10000/21964]\n",
      "loss: 2.547374  [15000/21964]\n",
      "loss: 2.612067  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.617287 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 2.612884  [    0/21964]\n",
      "loss: 2.624409  [ 5000/21964]\n",
      "loss: 2.652386  [10000/21964]\n",
      "loss: 2.612723  [15000/21964]\n",
      "loss: 2.683409  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616828 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 2.623424  [    0/21964]\n",
      "loss: 2.612299  [ 5000/21964]\n",
      "loss: 2.611896  [10000/21964]\n",
      "loss: 2.682322  [15000/21964]\n",
      "loss: 2.632915  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616860 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 2.572521  [    0/21964]\n",
      "loss: 2.681637  [ 5000/21964]\n",
      "loss: 2.592378  [10000/21964]\n",
      "loss: 2.592642  [15000/21964]\n",
      "loss: 2.642117  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616641 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 2.651208  [    0/21964]\n",
      "loss: 2.671660  [ 5000/21964]\n",
      "loss: 2.653212  [10000/21964]\n",
      "loss: 2.622846  [15000/21964]\n",
      "loss: 2.632273  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616657 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 2.582508  [    0/21964]\n",
      "loss: 2.720855  [ 5000/21964]\n",
      "loss: 2.662723  [10000/21964]\n",
      "loss: 2.642438  [15000/21964]\n",
      "loss: 2.641860  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616571 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 2.690683  [    0/21964]\n",
      "loss: 2.662904  [ 5000/21964]\n",
      "loss: 2.632800  [10000/21964]\n",
      "loss: 2.582967  [15000/21964]\n",
      "loss: 2.642739  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616529 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 2.632787  [    0/21964]\n",
      "loss: 2.632169  [ 5000/21964]\n",
      "loss: 2.592811  [10000/21964]\n",
      "loss: 2.563559  [15000/21964]\n",
      "loss: 2.640900  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616675 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 2.603377  [    0/21964]\n",
      "loss: 2.583070  [ 5000/21964]\n",
      "loss: 2.623460  [10000/21964]\n",
      "loss: 2.642082  [15000/21964]\n",
      "loss: 2.583493  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616433 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 2.553417  [    0/21964]\n",
      "loss: 2.632334  [ 5000/21964]\n",
      "loss: 2.681780  [10000/21964]\n",
      "loss: 2.563169  [15000/21964]\n",
      "loss: 2.593541  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616280 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 2.740670  [    0/21964]\n",
      "loss: 2.641404  [ 5000/21964]\n",
      "loss: 2.602493  [10000/21964]\n",
      "loss: 2.622403  [15000/21964]\n",
      "loss: 2.533522  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616316 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 2.632226  [    0/21964]\n",
      "loss: 2.778556  [ 5000/21964]\n",
      "loss: 2.700231  [10000/21964]\n",
      "loss: 2.603103  [15000/21964]\n",
      "loss: 2.612805  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.616045 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 2.651759  [    0/21964]\n",
      "loss: 2.650851  [ 5000/21964]\n",
      "loss: 2.625537  [10000/21964]\n",
      "loss: 2.646906  [15000/21964]\n",
      "loss: 2.642405  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 2.623645 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 2.584952  [    0/21964]\n",
      "loss: 2.612327  [ 5000/21964]\n",
      "loss: 2.557940  [10000/21964]\n",
      "loss: 2.612957  [15000/21964]\n",
      "loss: 2.573676  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.615188 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 2.603883  [    0/21964]\n",
      "loss: 2.622778  [ 5000/21964]\n",
      "loss: 2.582454  [10000/21964]\n",
      "loss: 2.562600  [15000/21964]\n",
      "loss: 2.683979  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.615322 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 2.670686  [    0/21964]\n",
      "loss: 2.582755  [ 5000/21964]\n",
      "loss: 2.612556  [10000/21964]\n",
      "loss: 2.623191  [15000/21964]\n",
      "loss: 2.563001  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614827 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 2.602485  [    0/21964]\n",
      "loss: 2.612803  [ 5000/21964]\n",
      "loss: 2.670794  [10000/21964]\n",
      "loss: 2.709356  [15000/21964]\n",
      "loss: 2.632501  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614622 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 2.612553  [    0/21964]\n",
      "loss: 2.671405  [ 5000/21964]\n",
      "loss: 2.611660  [10000/21964]\n",
      "loss: 2.583547  [15000/21964]\n",
      "loss: 2.631231  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614481 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 2.641512  [    0/21964]\n",
      "loss: 2.573534  [ 5000/21964]\n",
      "loss: 2.719495  [10000/21964]\n",
      "loss: 2.669870  [15000/21964]\n",
      "loss: 2.622068  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614593 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 2.602365  [    0/21964]\n",
      "loss: 2.700657  [ 5000/21964]\n",
      "loss: 2.571931  [10000/21964]\n",
      "loss: 2.642211  [15000/21964]\n",
      "loss: 2.621640  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614639 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 2.544661  [    0/21964]\n",
      "loss: 2.602499  [ 5000/21964]\n",
      "loss: 2.563325  [10000/21964]\n",
      "loss: 2.640626  [15000/21964]\n",
      "loss: 2.633667  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614372 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 2.611921  [    0/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.564435  [ 5000/21964]\n",
      "loss: 2.572902  [10000/21964]\n",
      "loss: 2.561597  [15000/21964]\n",
      "loss: 2.671222  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614340 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 2.583200  [    0/21964]\n",
      "loss: 2.592235  [ 5000/21964]\n",
      "loss: 2.710830  [10000/21964]\n",
      "loss: 2.591742  [15000/21964]\n",
      "loss: 2.640981  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614907 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 2.633382  [    0/21964]\n",
      "loss: 2.680332  [ 5000/21964]\n",
      "loss: 2.553553  [10000/21964]\n",
      "loss: 2.671026  [15000/21964]\n",
      "loss: 2.582816  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614255 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 2.651484  [    0/21964]\n",
      "loss: 2.562511  [ 5000/21964]\n",
      "loss: 2.653111  [10000/21964]\n",
      "loss: 2.680589  [15000/21964]\n",
      "loss: 2.660908  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614238 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 2.573038  [    0/21964]\n",
      "loss: 2.651211  [ 5000/21964]\n",
      "loss: 2.582419  [10000/21964]\n",
      "loss: 2.611162  [15000/21964]\n",
      "loss: 2.631911  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614232 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 2.621111  [    0/21964]\n",
      "loss: 2.660821  [ 5000/21964]\n",
      "loss: 2.571932  [10000/21964]\n",
      "loss: 2.602430  [15000/21964]\n",
      "loss: 2.592290  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614297 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 2.612606  [    0/21964]\n",
      "loss: 2.533501  [ 5000/21964]\n",
      "loss: 2.573014  [10000/21964]\n",
      "loss: 2.573260  [15000/21964]\n",
      "loss: 2.681360  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.614158 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 2.621558  [    0/21964]\n",
      "loss: 2.690534  [ 5000/21964]\n",
      "loss: 2.562578  [10000/21964]\n",
      "loss: 2.571805  [15000/21964]\n",
      "loss: 2.720777  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 2.613948 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 2.611438  [    0/21964]\n",
      "loss: 2.661611  [ 5000/21964]\n",
      "loss: 2.641168  [10000/21964]\n",
      "loss: 2.631197  [15000/21964]\n",
      "loss: 2.552940  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 2.613297 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 2.679264  [    0/21964]\n",
      "loss: 2.569044  [ 5000/21964]\n",
      "loss: 2.657672  [10000/21964]\n",
      "loss: 2.647845  [15000/21964]\n",
      "loss: 2.674791  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 2.617110 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 2.662737  [    0/21964]\n",
      "loss: 2.623445  [ 5000/21964]\n",
      "loss: 2.604944  [10000/21964]\n",
      "loss: 2.646390  [15000/21964]\n",
      "loss: 2.610732  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 2.587610 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 2.536500  [    0/21964]\n",
      "loss: 2.581055  [ 5000/21964]\n",
      "loss: 2.656353  [10000/21964]\n",
      "loss: 2.743177  [15000/21964]\n",
      "loss: 2.609750  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 2.581056 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 2.610491  [    0/21964]\n",
      "loss: 2.615937  [ 5000/21964]\n",
      "loss: 2.548916  [10000/21964]\n",
      "loss: 2.640360  [15000/21964]\n",
      "loss: 2.570025  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 2.583014 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 2.617441  [    0/21964]\n",
      "loss: 2.609347  [ 5000/21964]\n",
      "loss: 2.528428  [10000/21964]\n",
      "loss: 2.545896  [15000/21964]\n",
      "loss: 2.609540  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.578760 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 2.628067  [    0/21964]\n",
      "loss: 2.603774  [ 5000/21964]\n",
      "loss: 2.536872  [10000/21964]\n",
      "loss: 2.613357  [15000/21964]\n",
      "loss: 2.555964  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.577430 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 2.633245  [    0/21964]\n",
      "loss: 2.545157  [ 5000/21964]\n",
      "loss: 2.604761  [10000/21964]\n",
      "loss: 2.615292  [15000/21964]\n",
      "loss: 2.604207  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.576299 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 2.543860  [    0/21964]\n",
      "loss: 2.544754  [ 5000/21964]\n",
      "loss: 2.485251  [10000/21964]\n",
      "loss: 2.564396  [15000/21964]\n",
      "loss: 2.643357  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.576144 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 2.547255  [    0/21964]\n",
      "loss: 2.574593  [ 5000/21964]\n",
      "loss: 2.467292  [10000/21964]\n",
      "loss: 2.604400  [15000/21964]\n",
      "loss: 2.623866  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.575521 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 2.603509  [    0/21964]\n",
      "loss: 2.563708  [ 5000/21964]\n",
      "loss: 2.575812  [10000/21964]\n",
      "loss: 2.623732  [15000/21964]\n",
      "loss: 2.574008  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.575565 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 2.574865  [    0/21964]\n",
      "loss: 2.556420  [ 5000/21964]\n",
      "loss: 2.564058  [10000/21964]\n",
      "loss: 2.564521  [15000/21964]\n",
      "loss: 2.572610  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.575555 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 2.621949  [    0/21964]\n",
      "loss: 2.602870  [ 5000/21964]\n",
      "loss: 2.513614  [10000/21964]\n",
      "loss: 2.583477  [15000/21964]\n",
      "loss: 2.642888  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574891 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 2.573972  [    0/21964]\n",
      "loss: 2.516294  [ 5000/21964]\n",
      "loss: 2.574899  [10000/21964]\n",
      "loss: 2.584219  [15000/21964]\n",
      "loss: 2.524547  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574757 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 2.584842  [    0/21964]\n",
      "loss: 2.574934  [ 5000/21964]\n",
      "loss: 2.622201  [10000/21964]\n",
      "loss: 2.504432  [15000/21964]\n",
      "loss: 2.603192  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574757 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 2.563231  [    0/21964]\n",
      "loss: 2.593355  [ 5000/21964]\n",
      "loss: 2.642022  [10000/21964]\n",
      "loss: 2.524683  [15000/21964]\n",
      "loss: 2.584748  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574575 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 2.612489  [    0/21964]\n",
      "loss: 2.612305  [ 5000/21964]\n",
      "loss: 2.523385  [10000/21964]\n",
      "loss: 2.602985  [15000/21964]\n",
      "loss: 2.583470  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574568 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 2.601662  [    0/21964]\n",
      "loss: 2.524248  [ 5000/21964]\n",
      "loss: 2.553986  [10000/21964]\n",
      "loss: 2.553889  [15000/21964]\n",
      "loss: 2.544217  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574432 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 2.623269  [    0/21964]\n",
      "loss: 2.553316  [ 5000/21964]\n",
      "loss: 2.632827  [10000/21964]\n",
      "loss: 2.572215  [15000/21964]\n",
      "loss: 2.679743  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574216 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 2.621186  [    0/21964]\n",
      "loss: 2.622025  [ 5000/21964]\n",
      "loss: 2.524451  [10000/21964]\n",
      "loss: 2.493671  [15000/21964]\n",
      "loss: 2.593202  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574291 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 2.553041  [    0/21964]\n",
      "loss: 2.593208  [ 5000/21964]\n",
      "loss: 2.513741  [10000/21964]\n",
      "loss: 2.653069  [15000/21964]\n",
      "loss: 2.690155  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574008 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 2.573615  [    0/21964]\n",
      "loss: 2.503738  [ 5000/21964]\n",
      "loss: 2.544517  [10000/21964]\n",
      "loss: 2.572400  [15000/21964]\n",
      "loss: 2.642640  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574061 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 2.583312  [    0/21964]\n",
      "loss: 2.573114  [ 5000/21964]\n",
      "loss: 2.651200  [10000/21964]\n",
      "loss: 2.573387  [15000/21964]\n",
      "loss: 2.670674  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573992 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 2.592398  [    0/21964]\n",
      "loss: 2.563332  [ 5000/21964]\n",
      "loss: 2.545039  [10000/21964]\n",
      "loss: 2.632239  [15000/21964]\n",
      "loss: 2.444197  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.574114 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 2.494022  [    0/21964]\n",
      "loss: 2.622005  [ 5000/21964]\n",
      "loss: 2.582791  [10000/21964]\n",
      "loss: 2.562612  [15000/21964]\n",
      "loss: 2.592234  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573944 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 2.483688  [    0/21964]\n",
      "loss: 2.581891  [ 5000/21964]\n",
      "loss: 2.602664  [10000/21964]\n",
      "loss: 2.552903  [15000/21964]\n",
      "loss: 2.602306  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573824 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 2.573170  [    0/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.602506  [ 5000/21964]\n",
      "loss: 2.611346  [10000/21964]\n",
      "loss: 2.641165  [15000/21964]\n",
      "loss: 2.670335  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573843 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 2.553338  [    0/21964]\n",
      "loss: 2.601893  [ 5000/21964]\n",
      "loss: 2.612195  [10000/21964]\n",
      "loss: 2.601515  [15000/21964]\n",
      "loss: 2.631768  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573792 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 2.650933  [    0/21964]\n",
      "loss: 2.591610  [ 5000/21964]\n",
      "loss: 2.592988  [10000/21964]\n",
      "loss: 2.563150  [15000/21964]\n",
      "loss: 2.562585  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573791 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 2.601545  [    0/21964]\n",
      "loss: 2.660852  [ 5000/21964]\n",
      "loss: 2.591946  [10000/21964]\n",
      "loss: 2.622656  [15000/21964]\n",
      "loss: 2.602187  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573820 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 2.543534  [    0/21964]\n",
      "loss: 2.621942  [ 5000/21964]\n",
      "loss: 2.572167  [10000/21964]\n",
      "loss: 2.593648  [15000/21964]\n",
      "loss: 2.640904  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573574 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 2.611819  [    0/21964]\n",
      "loss: 2.542489  [ 5000/21964]\n",
      "loss: 2.611926  [10000/21964]\n",
      "loss: 2.522858  [15000/21964]\n",
      "loss: 2.552234  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573462 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 2.620407  [    0/21964]\n",
      "loss: 2.651257  [ 5000/21964]\n",
      "loss: 2.523216  [10000/21964]\n",
      "loss: 2.532247  [15000/21964]\n",
      "loss: 2.591731  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573519 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 2.583040  [    0/21964]\n",
      "loss: 2.552273  [ 5000/21964]\n",
      "loss: 2.650808  [10000/21964]\n",
      "loss: 2.621663  [15000/21964]\n",
      "loss: 2.691025  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573389 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 2.473278  [    0/21964]\n",
      "loss: 2.542549  [ 5000/21964]\n",
      "loss: 2.484238  [10000/21964]\n",
      "loss: 2.601325  [15000/21964]\n",
      "loss: 2.533635  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573514 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 2.582089  [    0/21964]\n",
      "loss: 2.581283  [ 5000/21964]\n",
      "loss: 2.581856  [10000/21964]\n",
      "loss: 2.513435  [15000/21964]\n",
      "loss: 2.581264  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573452 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 2.533131  [    0/21964]\n",
      "loss: 2.630382  [ 5000/21964]\n",
      "loss: 2.473923  [10000/21964]\n",
      "loss: 2.601847  [15000/21964]\n",
      "loss: 2.532660  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573476 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 2.522718  [    0/21964]\n",
      "loss: 2.651624  [ 5000/21964]\n",
      "loss: 2.562705  [10000/21964]\n",
      "loss: 2.611513  [15000/21964]\n",
      "loss: 2.620949  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573225 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 2.640910  [    0/21964]\n",
      "loss: 2.541512  [ 5000/21964]\n",
      "loss: 2.562300  [10000/21964]\n",
      "loss: 2.601143  [15000/21964]\n",
      "loss: 2.611679  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573215 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 2.641283  [    0/21964]\n",
      "loss: 2.601677  [ 5000/21964]\n",
      "loss: 2.581812  [10000/21964]\n",
      "loss: 2.572046  [15000/21964]\n",
      "loss: 2.591939  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573271 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 2.592445  [    0/21964]\n",
      "loss: 2.542744  [ 5000/21964]\n",
      "loss: 2.542410  [10000/21964]\n",
      "loss: 2.590826  [15000/21964]\n",
      "loss: 2.581268  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573124 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 2.542853  [    0/21964]\n",
      "loss: 2.602119  [ 5000/21964]\n",
      "loss: 2.630963  [10000/21964]\n",
      "loss: 2.591626  [15000/21964]\n",
      "loss: 2.601604  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573218 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 2.551775  [    0/21964]\n",
      "loss: 2.502575  [ 5000/21964]\n",
      "loss: 2.590898  [10000/21964]\n",
      "loss: 2.561920  [15000/21964]\n",
      "loss: 2.543555  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573185 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 2.581486  [    0/21964]\n",
      "loss: 2.512940  [ 5000/21964]\n",
      "loss: 2.611836  [10000/21964]\n",
      "loss: 2.522679  [15000/21964]\n",
      "loss: 2.552298  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573333 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 2.640751  [    0/21964]\n",
      "loss: 2.523204  [ 5000/21964]\n",
      "loss: 2.552579  [10000/21964]\n",
      "loss: 2.513074  [15000/21964]\n",
      "loss: 2.592003  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573109 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 2.591831  [    0/21964]\n",
      "loss: 2.582683  [ 5000/21964]\n",
      "loss: 2.611046  [10000/21964]\n",
      "loss: 2.571170  [15000/21964]\n",
      "loss: 2.541940  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573134 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 2.552134  [    0/21964]\n",
      "loss: 2.533222  [ 5000/21964]\n",
      "loss: 2.610720  [10000/21964]\n",
      "loss: 2.561929  [15000/21964]\n",
      "loss: 2.659952  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573156 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 2.532633  [    0/21964]\n",
      "loss: 2.533608  [ 5000/21964]\n",
      "loss: 2.562203  [10000/21964]\n",
      "loss: 2.610176  [15000/21964]\n",
      "loss: 2.611154  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573158 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 2.670350  [    0/21964]\n",
      "loss: 2.561830  [ 5000/21964]\n",
      "loss: 2.552301  [10000/21964]\n",
      "loss: 2.552080  [15000/21964]\n",
      "loss: 2.542026  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572926 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 2.582624  [    0/21964]\n",
      "loss: 2.640771  [ 5000/21964]\n",
      "loss: 2.630923  [10000/21964]\n",
      "loss: 2.640907  [15000/21964]\n",
      "loss: 2.630680  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572928 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 2.561211  [    0/21964]\n",
      "loss: 2.521825  [ 5000/21964]\n",
      "loss: 2.601895  [10000/21964]\n",
      "loss: 2.641050  [15000/21964]\n",
      "loss: 2.571850  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572849 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 2.522476  [    0/21964]\n",
      "loss: 2.621002  [ 5000/21964]\n",
      "loss: 2.502817  [10000/21964]\n",
      "loss: 2.542123  [15000/21964]\n",
      "loss: 2.679948  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572911 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 2.620831  [    0/21964]\n",
      "loss: 2.591301  [ 5000/21964]\n",
      "loss: 2.620652  [10000/21964]\n",
      "loss: 2.591704  [15000/21964]\n",
      "loss: 2.551431  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572955 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 2.621425  [    0/21964]\n",
      "loss: 2.602348  [ 5000/21964]\n",
      "loss: 2.542746  [10000/21964]\n",
      "loss: 2.552561  [15000/21964]\n",
      "loss: 2.610536  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572910 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 2.493235  [    0/21964]\n",
      "loss: 2.610919  [ 5000/21964]\n",
      "loss: 2.659692  [10000/21964]\n",
      "loss: 2.670454  [15000/21964]\n",
      "loss: 2.641549  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572793 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 2.562619  [    0/21964]\n",
      "loss: 2.620625  [ 5000/21964]\n",
      "loss: 2.591420  [10000/21964]\n",
      "loss: 2.561543  [15000/21964]\n",
      "loss: 2.562458  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572784 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 2.561466  [    0/21964]\n",
      "loss: 2.572019  [ 5000/21964]\n",
      "loss: 2.512358  [10000/21964]\n",
      "loss: 2.611072  [15000/21964]\n",
      "loss: 2.631245  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572902 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 2.611010  [    0/21964]\n",
      "loss: 2.551866  [ 5000/21964]\n",
      "loss: 2.620240  [10000/21964]\n",
      "loss: 2.610726  [15000/21964]\n",
      "loss: 2.591787  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572882 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 2.591199  [    0/21964]\n",
      "loss: 2.513515  [ 5000/21964]\n",
      "loss: 2.531827  [10000/21964]\n",
      "loss: 2.581959  [15000/21964]\n",
      "loss: 2.620779  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572878 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 2.620734  [    0/21964]\n",
      "loss: 2.621235  [ 5000/21964]\n",
      "loss: 2.601300  [10000/21964]\n",
      "loss: 2.581127  [15000/21964]\n",
      "loss: 2.571826  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572886 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 2.581576  [    0/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.571194  [ 5000/21964]\n",
      "loss: 2.531858  [10000/21964]\n",
      "loss: 2.601436  [15000/21964]\n",
      "loss: 2.571361  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572791 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 2.552060  [    0/21964]\n",
      "loss: 2.553105  [ 5000/21964]\n",
      "loss: 2.670726  [10000/21964]\n",
      "loss: 2.562351  [15000/21964]\n",
      "loss: 2.512908  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572762 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 2.620334  [    0/21964]\n",
      "loss: 2.601088  [ 5000/21964]\n",
      "loss: 2.522831  [10000/21964]\n",
      "loss: 2.512473  [15000/21964]\n",
      "loss: 2.571960  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572798 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 2.581267  [    0/21964]\n",
      "loss: 2.591322  [ 5000/21964]\n",
      "loss: 2.473431  [10000/21964]\n",
      "loss: 2.522005  [15000/21964]\n",
      "loss: 2.501873  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572728 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 2.512972  [    0/21964]\n",
      "loss: 2.669308  [ 5000/21964]\n",
      "loss: 2.582007  [10000/21964]\n",
      "loss: 2.561460  [15000/21964]\n",
      "loss: 2.553265  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572878 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 2.620749  [    0/21964]\n",
      "loss: 2.512445  [ 5000/21964]\n",
      "loss: 2.640819  [10000/21964]\n",
      "loss: 2.581461  [15000/21964]\n",
      "loss: 2.562268  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572568 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 2.522621  [    0/21964]\n",
      "loss: 2.631163  [ 5000/21964]\n",
      "loss: 2.551591  [10000/21964]\n",
      "loss: 2.629895  [15000/21964]\n",
      "loss: 2.592134  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572789 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 2.531674  [    0/21964]\n",
      "loss: 2.590999  [ 5000/21964]\n",
      "loss: 2.659512  [10000/21964]\n",
      "loss: 2.532446  [15000/21964]\n",
      "loss: 2.492386  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572717 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 2.561375  [    0/21964]\n",
      "loss: 2.542475  [ 5000/21964]\n",
      "loss: 2.640315  [10000/21964]\n",
      "loss: 2.540926  [15000/21964]\n",
      "loss: 2.620708  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572623 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 2.660348  [    0/21964]\n",
      "loss: 2.610487  [ 5000/21964]\n",
      "loss: 2.610644  [10000/21964]\n",
      "loss: 2.532448  [15000/21964]\n",
      "loss: 2.670377  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572564 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 2.581385  [    0/21964]\n",
      "loss: 2.611251  [ 5000/21964]\n",
      "loss: 2.581462  [10000/21964]\n",
      "loss: 2.532743  [15000/21964]\n",
      "loss: 2.522355  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572590 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 2.590637  [    0/21964]\n",
      "loss: 2.483380  [ 5000/21964]\n",
      "loss: 2.561877  [10000/21964]\n",
      "loss: 2.522930  [15000/21964]\n",
      "loss: 2.610722  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572633 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 2.610847  [    0/21964]\n",
      "loss: 2.631543  [ 5000/21964]\n",
      "loss: 2.620631  [10000/21964]\n",
      "loss: 2.600899  [15000/21964]\n",
      "loss: 2.542243  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572671 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 2.640161  [    0/21964]\n",
      "loss: 2.620611  [ 5000/21964]\n",
      "loss: 2.523408  [10000/21964]\n",
      "loss: 2.561375  [15000/21964]\n",
      "loss: 2.797021  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572610 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 2.571248  [    0/21964]\n",
      "loss: 2.620674  [ 5000/21964]\n",
      "loss: 2.591353  [10000/21964]\n",
      "loss: 2.610115  [15000/21964]\n",
      "loss: 2.580964  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572567 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 2.590680  [    0/21964]\n",
      "loss: 2.620730  [ 5000/21964]\n",
      "loss: 2.492461  [10000/21964]\n",
      "loss: 2.620678  [15000/21964]\n",
      "loss: 2.610163  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572684 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 2.571579  [    0/21964]\n",
      "loss: 2.610313  [ 5000/21964]\n",
      "loss: 2.639802  [10000/21964]\n",
      "loss: 2.620500  [15000/21964]\n",
      "loss: 2.640550  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572558 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 2.542283  [    0/21964]\n",
      "loss: 2.640086  [ 5000/21964]\n",
      "loss: 2.561234  [10000/21964]\n",
      "loss: 2.532287  [15000/21964]\n",
      "loss: 2.640491  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572426 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 2.581404  [    0/21964]\n",
      "loss: 2.600927  [ 5000/21964]\n",
      "loss: 2.570723  [10000/21964]\n",
      "loss: 2.630345  [15000/21964]\n",
      "loss: 2.669401  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572538 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 2.649795  [    0/21964]\n",
      "loss: 2.532584  [ 5000/21964]\n",
      "loss: 2.600530  [10000/21964]\n",
      "loss: 2.580290  [15000/21964]\n",
      "loss: 2.610644  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572637 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 2.581350  [    0/21964]\n",
      "loss: 2.531827  [ 5000/21964]\n",
      "loss: 2.591258  [10000/21964]\n",
      "loss: 2.590804  [15000/21964]\n",
      "loss: 2.572762  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572446 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 2.581804  [    0/21964]\n",
      "loss: 2.619837  [ 5000/21964]\n",
      "loss: 2.629981  [10000/21964]\n",
      "loss: 2.619252  [15000/21964]\n",
      "loss: 2.590985  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572486 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 2.639360  [    0/21964]\n",
      "loss: 2.571923  [ 5000/21964]\n",
      "loss: 2.522348  [10000/21964]\n",
      "loss: 2.571917  [15000/21964]\n",
      "loss: 2.551814  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572436 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 2.551705  [    0/21964]\n",
      "loss: 2.532807  [ 5000/21964]\n",
      "loss: 2.600456  [10000/21964]\n",
      "loss: 2.610447  [15000/21964]\n",
      "loss: 2.542304  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572433 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 2.581565  [    0/21964]\n",
      "loss: 2.610475  [ 5000/21964]\n",
      "loss: 2.610630  [10000/21964]\n",
      "loss: 2.590994  [15000/21964]\n",
      "loss: 2.601151  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572384 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 2.532608  [    0/21964]\n",
      "loss: 2.522511  [ 5000/21964]\n",
      "loss: 2.580827  [10000/21964]\n",
      "loss: 2.551516  [15000/21964]\n",
      "loss: 2.698790  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572338 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 2.522845  [    0/21964]\n",
      "loss: 2.551896  [ 5000/21964]\n",
      "loss: 2.580412  [10000/21964]\n",
      "loss: 2.600811  [15000/21964]\n",
      "loss: 2.590706  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572249 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 2.562053  [    0/21964]\n",
      "loss: 2.619596  [ 5000/21964]\n",
      "loss: 2.542298  [10000/21964]\n",
      "loss: 2.600587  [15000/21964]\n",
      "loss: 2.590643  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572150 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 2.551609  [    0/21964]\n",
      "loss: 2.571454  [ 5000/21964]\n",
      "loss: 2.639565  [10000/21964]\n",
      "loss: 2.512445  [15000/21964]\n",
      "loss: 2.512077  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572156 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 2.590645  [    0/21964]\n",
      "loss: 2.600698  [ 5000/21964]\n",
      "loss: 2.601073  [10000/21964]\n",
      "loss: 2.552070  [15000/21964]\n",
      "loss: 2.512657  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.572157 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 2.668589  [    0/21964]\n",
      "loss: 2.600921  [ 5000/21964]\n",
      "loss: 2.580877  [10000/21964]\n",
      "loss: 2.539895  [15000/21964]\n",
      "loss: 2.503108  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571927 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 2.541494  [    0/21964]\n",
      "loss: 2.561403  [ 5000/21964]\n",
      "loss: 2.619456  [10000/21964]\n",
      "loss: 2.599822  [15000/21964]\n",
      "loss: 2.483845  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.573477 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 2.601772  [    0/21964]\n",
      "loss: 2.601576  [ 5000/21964]\n",
      "loss: 2.571426  [10000/21964]\n",
      "loss: 2.521683  [15000/21964]\n",
      "loss: 2.581063  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.572455 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 2.542083  [    0/21964]\n",
      "loss: 2.561097  [ 5000/21964]\n",
      "loss: 2.600681  [10000/21964]\n",
      "loss: 2.551455  [15000/21964]\n",
      "loss: 2.610025  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571558 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 2.610888  [    0/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.581407  [ 5000/21964]\n",
      "loss: 2.581212  [10000/21964]\n",
      "loss: 2.591413  [15000/21964]\n",
      "loss: 2.553984  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.572379 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 2.542101  [    0/21964]\n",
      "loss: 2.581143  [ 5000/21964]\n",
      "loss: 2.533310  [10000/21964]\n",
      "loss: 2.512529  [15000/21964]\n",
      "loss: 2.639474  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571620 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 2.600865  [    0/21964]\n",
      "loss: 2.571180  [ 5000/21964]\n",
      "loss: 2.542262  [10000/21964]\n",
      "loss: 2.610213  [15000/21964]\n",
      "loss: 2.571512  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571774 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 2.590695  [    0/21964]\n",
      "loss: 2.551915  [ 5000/21964]\n",
      "loss: 2.552013  [10000/21964]\n",
      "loss: 2.659398  [15000/21964]\n",
      "loss: 2.590684  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571320 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 2.610477  [    0/21964]\n",
      "loss: 2.541558  [ 5000/21964]\n",
      "loss: 2.561229  [10000/21964]\n",
      "loss: 2.581258  [15000/21964]\n",
      "loss: 2.571823  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571413 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 2.551811  [    0/21964]\n",
      "loss: 2.532221  [ 5000/21964]\n",
      "loss: 2.590504  [10000/21964]\n",
      "loss: 2.551705  [15000/21964]\n",
      "loss: 2.580568  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571415 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 2.718766  [    0/21964]\n",
      "loss: 2.571994  [ 5000/21964]\n",
      "loss: 2.551504  [10000/21964]\n",
      "loss: 2.571784  [15000/21964]\n",
      "loss: 2.581087  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571392 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 2.541828  [    0/21964]\n",
      "loss: 2.611253  [ 5000/21964]\n",
      "loss: 2.610839  [10000/21964]\n",
      "loss: 2.668999  [15000/21964]\n",
      "loss: 2.492632  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571339 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 2.611792  [    0/21964]\n",
      "loss: 2.581469  [ 5000/21964]\n",
      "loss: 2.502616  [10000/21964]\n",
      "loss: 2.511869  [15000/21964]\n",
      "loss: 2.522532  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571304 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 2.580834  [    0/21964]\n",
      "loss: 2.502235  [ 5000/21964]\n",
      "loss: 2.590030  [10000/21964]\n",
      "loss: 2.571123  [15000/21964]\n",
      "loss: 2.571362  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571202 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 2.619881  [    0/21964]\n",
      "loss: 2.561109  [ 5000/21964]\n",
      "loss: 2.551338  [10000/21964]\n",
      "loss: 2.512611  [15000/21964]\n",
      "loss: 2.542084  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571390 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 2.609951  [    0/21964]\n",
      "loss: 2.698537  [ 5000/21964]\n",
      "loss: 2.609941  [10000/21964]\n",
      "loss: 2.502115  [15000/21964]\n",
      "loss: 2.551068  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571172 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 2.659756  [    0/21964]\n",
      "loss: 2.541153  [ 5000/21964]\n",
      "loss: 2.522458  [10000/21964]\n",
      "loss: 2.668930  [15000/21964]\n",
      "loss: 2.600331  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571207 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 2.502400  [    0/21964]\n",
      "loss: 2.571048  [ 5000/21964]\n",
      "loss: 2.591398  [10000/21964]\n",
      "loss: 2.580824  [15000/21964]\n",
      "loss: 2.551342  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571304 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 2.590760  [    0/21964]\n",
      "loss: 2.600383  [ 5000/21964]\n",
      "loss: 2.639246  [10000/21964]\n",
      "loss: 2.571174  [15000/21964]\n",
      "loss: 2.580702  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571233 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 2.620605  [    0/21964]\n",
      "loss: 2.629550  [ 5000/21964]\n",
      "loss: 2.630184  [10000/21964]\n",
      "loss: 2.521684  [15000/21964]\n",
      "loss: 2.561123  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571290 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 2.531802  [    0/21964]\n",
      "loss: 2.531413  [ 5000/21964]\n",
      "loss: 2.561093  [10000/21964]\n",
      "loss: 2.551382  [15000/21964]\n",
      "loss: 2.610147  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571197 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 2.610302  [    0/21964]\n",
      "loss: 2.630722  [ 5000/21964]\n",
      "loss: 2.599916  [10000/21964]\n",
      "loss: 2.609568  [15000/21964]\n",
      "loss: 2.619769  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571128 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 2.531644  [    0/21964]\n",
      "loss: 2.580960  [ 5000/21964]\n",
      "loss: 2.551665  [10000/21964]\n",
      "loss: 2.640042  [15000/21964]\n",
      "loss: 2.552132  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571170 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 2.560913  [    0/21964]\n",
      "loss: 2.571058  [ 5000/21964]\n",
      "loss: 2.630267  [10000/21964]\n",
      "loss: 2.570912  [15000/21964]\n",
      "loss: 2.580478  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571222 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 2.551577  [    0/21964]\n",
      "loss: 2.600627  [ 5000/21964]\n",
      "loss: 2.581029  [10000/21964]\n",
      "loss: 2.639944  [15000/21964]\n",
      "loss: 2.532360  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571249 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 2.570516  [    0/21964]\n",
      "loss: 2.639738  [ 5000/21964]\n",
      "loss: 2.600291  [10000/21964]\n",
      "loss: 2.542046  [15000/21964]\n",
      "loss: 2.561651  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571147 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 2.610046  [    0/21964]\n",
      "loss: 2.502027  [ 5000/21964]\n",
      "loss: 2.610875  [10000/21964]\n",
      "loss: 2.581280  [15000/21964]\n",
      "loss: 2.551026  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571173 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 2.659065  [    0/21964]\n",
      "loss: 2.619534  [ 5000/21964]\n",
      "loss: 2.561254  [10000/21964]\n",
      "loss: 2.609874  [15000/21964]\n",
      "loss: 2.591225  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571193 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 2.561093  [    0/21964]\n",
      "loss: 2.512476  [ 5000/21964]\n",
      "loss: 2.570997  [10000/21964]\n",
      "loss: 2.511744  [15000/21964]\n",
      "loss: 2.522065  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571110 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 2.561296  [    0/21964]\n",
      "loss: 2.501976  [ 5000/21964]\n",
      "loss: 2.560912  [10000/21964]\n",
      "loss: 2.571002  [15000/21964]\n",
      "loss: 2.717792  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571028 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 2.571162  [    0/21964]\n",
      "loss: 2.580961  [ 5000/21964]\n",
      "loss: 2.532017  [10000/21964]\n",
      "loss: 2.571288  [15000/21964]\n",
      "loss: 2.541534  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571113 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 2.619861  [    0/21964]\n",
      "loss: 2.610111  [ 5000/21964]\n",
      "loss: 2.580654  [10000/21964]\n",
      "loss: 2.591352  [15000/21964]\n",
      "loss: 2.619672  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.570992 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 2.550995  [    0/21964]\n",
      "loss: 2.541063  [ 5000/21964]\n",
      "loss: 2.580008  [10000/21964]\n",
      "loss: 2.551195  [15000/21964]\n",
      "loss: 2.649134  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571072 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 2.629840  [    0/21964]\n",
      "loss: 2.521966  [ 5000/21964]\n",
      "loss: 2.512486  [10000/21964]\n",
      "loss: 2.649275  [15000/21964]\n",
      "loss: 2.571012  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571038 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 2.600315  [    0/21964]\n",
      "loss: 2.629900  [ 5000/21964]\n",
      "loss: 2.531386  [10000/21964]\n",
      "loss: 2.541337  [15000/21964]\n",
      "loss: 2.600327  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571102 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 2.600727  [    0/21964]\n",
      "loss: 2.541489  [ 5000/21964]\n",
      "loss: 2.491924  [10000/21964]\n",
      "loss: 2.659596  [15000/21964]\n",
      "loss: 2.542186  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571156 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 2.561264  [    0/21964]\n",
      "loss: 2.571917  [ 5000/21964]\n",
      "loss: 2.630034  [10000/21964]\n",
      "loss: 2.610085  [15000/21964]\n",
      "loss: 2.600676  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571085 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 2.590542  [    0/21964]\n",
      "loss: 2.601184  [ 5000/21964]\n",
      "loss: 2.619173  [10000/21964]\n",
      "loss: 2.668979  [15000/21964]\n",
      "loss: 2.580890  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.570969 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 2.658418  [    0/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.561090  [ 5000/21964]\n",
      "loss: 2.570352  [10000/21964]\n",
      "loss: 2.560987  [15000/21964]\n",
      "loss: 2.561419  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.570992 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 2.629663  [    0/21964]\n",
      "loss: 2.619887  [ 5000/21964]\n",
      "loss: 2.629815  [10000/21964]\n",
      "loss: 2.590232  [15000/21964]\n",
      "loss: 2.492396  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571028 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 2.502112  [    0/21964]\n",
      "loss: 2.551216  [ 5000/21964]\n",
      "loss: 2.541167  [10000/21964]\n",
      "loss: 2.629999  [15000/21964]\n",
      "loss: 2.620174  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571193 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 2.620519  [    0/21964]\n",
      "loss: 2.511721  [ 5000/21964]\n",
      "loss: 2.580451  [10000/21964]\n",
      "loss: 2.580623  [15000/21964]\n",
      "loss: 2.590133  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.570931 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 2.521823  [    0/21964]\n",
      "loss: 2.571316  [ 5000/21964]\n",
      "loss: 2.551573  [10000/21964]\n",
      "loss: 2.590019  [15000/21964]\n",
      "loss: 2.600559  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.570921 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 2.540967  [    0/21964]\n",
      "loss: 2.521604  [ 5000/21964]\n",
      "loss: 2.600668  [10000/21964]\n",
      "loss: 2.541968  [15000/21964]\n",
      "loss: 2.599580  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.571043 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 2.678071  [    0/21964]\n",
      "loss: 2.560963  [ 5000/21964]\n",
      "loss: 2.560736  [10000/21964]\n",
      "loss: 2.541394  [15000/21964]\n",
      "loss: 2.560585  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.570893 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 2.531616  [    0/21964]\n",
      "loss: 2.521646  [ 5000/21964]\n",
      "loss: 2.512223  [10000/21964]\n",
      "loss: 2.590066  [15000/21964]\n",
      "loss: 2.550700  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 2.570751 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 2.521963  [    0/21964]\n",
      "loss: 2.593339  [ 5000/21964]\n",
      "loss: 2.542048  [10000/21964]\n",
      "loss: 2.531400  [15000/21964]\n",
      "loss: 2.531284  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570905 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 2.580962  [    0/21964]\n",
      "loss: 2.620249  [ 5000/21964]\n",
      "loss: 2.492926  [10000/21964]\n",
      "loss: 2.619592  [15000/21964]\n",
      "loss: 2.600986  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570588 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 2.629851  [    0/21964]\n",
      "loss: 2.590779  [ 5000/21964]\n",
      "loss: 2.609941  [10000/21964]\n",
      "loss: 2.482049  [15000/21964]\n",
      "loss: 2.688646  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570528 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 2.542489  [    0/21964]\n",
      "loss: 2.629529  [ 5000/21964]\n",
      "loss: 2.590737  [10000/21964]\n",
      "loss: 2.630358  [15000/21964]\n",
      "loss: 2.620036  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570481 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 2.561225  [    0/21964]\n",
      "loss: 2.492784  [ 5000/21964]\n",
      "loss: 2.511947  [10000/21964]\n",
      "loss: 2.630157  [15000/21964]\n",
      "loss: 2.541460  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570307 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 2.620272  [    0/21964]\n",
      "loss: 2.562011  [ 5000/21964]\n",
      "loss: 2.570961  [10000/21964]\n",
      "loss: 2.640425  [15000/21964]\n",
      "loss: 2.620299  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570454 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 2.570859  [    0/21964]\n",
      "loss: 2.542077  [ 5000/21964]\n",
      "loss: 2.531734  [10000/21964]\n",
      "loss: 2.619365  [15000/21964]\n",
      "loss: 2.620577  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570582 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 2.590894  [    0/21964]\n",
      "loss: 2.620128  [ 5000/21964]\n",
      "loss: 2.502318  [10000/21964]\n",
      "loss: 2.522301  [15000/21964]\n",
      "loss: 2.630480  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570482 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 2.560791  [    0/21964]\n",
      "loss: 2.541350  [ 5000/21964]\n",
      "loss: 2.580965  [10000/21964]\n",
      "loss: 2.620079  [15000/21964]\n",
      "loss: 2.610347  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570518 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 2.551815  [    0/21964]\n",
      "loss: 2.600226  [ 5000/21964]\n",
      "loss: 2.570789  [10000/21964]\n",
      "loss: 2.561069  [15000/21964]\n",
      "loss: 2.620346  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570547 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 2.659307  [    0/21964]\n",
      "loss: 2.550881  [ 5000/21964]\n",
      "loss: 2.541364  [10000/21964]\n",
      "loss: 2.619990  [15000/21964]\n",
      "loss: 2.541360  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570472 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 2.571545  [    0/21964]\n",
      "loss: 2.521688  [ 5000/21964]\n",
      "loss: 2.521362  [10000/21964]\n",
      "loss: 2.502365  [15000/21964]\n",
      "loss: 2.600786  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570325 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 2.540817  [    0/21964]\n",
      "loss: 2.551196  [ 5000/21964]\n",
      "loss: 2.551005  [10000/21964]\n",
      "loss: 2.648812  [15000/21964]\n",
      "loss: 2.570145  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570377 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 2.542040  [    0/21964]\n",
      "loss: 2.580235  [ 5000/21964]\n",
      "loss: 2.560572  [10000/21964]\n",
      "loss: 2.540684  [15000/21964]\n",
      "loss: 2.580255  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570406 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 2.531716  [    0/21964]\n",
      "loss: 2.580379  [ 5000/21964]\n",
      "loss: 2.600004  [10000/21964]\n",
      "loss: 2.560127  [15000/21964]\n",
      "loss: 2.502302  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570324 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 2.580812  [    0/21964]\n",
      "loss: 2.581076  [ 5000/21964]\n",
      "loss: 2.609719  [10000/21964]\n",
      "loss: 2.590668  [15000/21964]\n",
      "loss: 2.590485  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570377 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 2.502355  [    0/21964]\n",
      "loss: 2.561098  [ 5000/21964]\n",
      "loss: 2.600150  [10000/21964]\n",
      "loss: 2.619783  [15000/21964]\n",
      "loss: 2.531811  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570528 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 2.541259  [    0/21964]\n",
      "loss: 2.629514  [ 5000/21964]\n",
      "loss: 2.551801  [10000/21964]\n",
      "loss: 2.619941  [15000/21964]\n",
      "loss: 2.619470  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570315 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 2.707814  [    0/21964]\n",
      "loss: 2.580389  [ 5000/21964]\n",
      "loss: 2.649206  [10000/21964]\n",
      "loss: 2.531902  [15000/21964]\n",
      "loss: 2.581065  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.570269 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "batch_size = 100\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "epochs = 400\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3227504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'modelq3-71-SGD-noDO.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fcc703b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.235330  [    0/21964]\n",
      "loss: 3.231508  [ 5000/21964]\n",
      "loss: 3.213837  [10000/21964]\n",
      "loss: 3.138402  [15000/21964]\n",
      "loss: 3.137434  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 18.6%, Avg loss: 3.105877 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.141965  [    0/21964]\n",
      "loss: 3.079570  [ 5000/21964]\n",
      "loss: 2.988491  [10000/21964]\n",
      "loss: 3.068799  [15000/21964]\n",
      "loss: 2.988299  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.993165 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.011457  [    0/21964]\n",
      "loss: 3.012992  [ 5000/21964]\n",
      "loss: 2.999309  [10000/21964]\n",
      "loss: 2.984644  [15000/21964]\n",
      "loss: 2.997837  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 2.953283 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.943183  [    0/21964]\n",
      "loss: 2.951387  [ 5000/21964]\n",
      "loss: 2.950468  [10000/21964]\n",
      "loss: 3.032573  [15000/21964]\n",
      "loss: 2.941542  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 36.4%, Avg loss: 2.926222 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.895365  [    0/21964]\n",
      "loss: 2.940312  [ 5000/21964]\n",
      "loss: 2.869217  [10000/21964]\n",
      "loss: 2.886775  [15000/21964]\n",
      "loss: 2.952043  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 40.5%, Avg loss: 2.889159 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.865205  [    0/21964]\n",
      "loss: 2.777190  [ 5000/21964]\n",
      "loss: 2.853681  [10000/21964]\n",
      "loss: 2.877098  [15000/21964]\n",
      "loss: 2.891687  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 2.877648 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.884033  [    0/21964]\n",
      "loss: 2.887636  [ 5000/21964]\n",
      "loss: 2.866386  [10000/21964]\n",
      "loss: 2.846155  [15000/21964]\n",
      "loss: 2.804697  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 2.874149 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.902190  [    0/21964]\n",
      "loss: 3.028572  [ 5000/21964]\n",
      "loss: 2.895907  [10000/21964]\n",
      "loss: 2.785693  [15000/21964]\n",
      "loss: 2.834978  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 2.869238 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.871948  [    0/21964]\n",
      "loss: 2.844306  [ 5000/21964]\n",
      "loss: 2.906621  [10000/21964]\n",
      "loss: 2.930441  [15000/21964]\n",
      "loss: 2.861268  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 2.867807 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.863502  [    0/21964]\n",
      "loss: 2.842287  [ 5000/21964]\n",
      "loss: 2.926449  [10000/21964]\n",
      "loss: 2.941784  [15000/21964]\n",
      "loss: 2.875911  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 47.3%, Avg loss: 2.823731 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.899831  [    0/21964]\n",
      "loss: 2.829962  [ 5000/21964]\n",
      "loss: 2.720171  [10000/21964]\n",
      "loss: 2.785081  [15000/21964]\n",
      "loss: 2.767656  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 2.802913 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.768438  [    0/21964]\n",
      "loss: 2.755744  [ 5000/21964]\n",
      "loss: 2.768303  [10000/21964]\n",
      "loss: 2.796923  [15000/21964]\n",
      "loss: 2.759145  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 2.796312 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.746557  [    0/21964]\n",
      "loss: 2.777230  [ 5000/21964]\n",
      "loss: 2.794875  [10000/21964]\n",
      "loss: 2.823093  [15000/21964]\n",
      "loss: 2.881296  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 2.793681 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.785532  [    0/21964]\n",
      "loss: 2.804272  [ 5000/21964]\n",
      "loss: 2.765600  [10000/21964]\n",
      "loss: 2.850143  [15000/21964]\n",
      "loss: 2.792843  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 2.791865 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.832465  [    0/21964]\n",
      "loss: 2.744395  [ 5000/21964]\n",
      "loss: 2.821551  [10000/21964]\n",
      "loss: 2.733867  [15000/21964]\n",
      "loss: 2.702941  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 2.791038 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.802130  [    0/21964]\n",
      "loss: 2.771487  [ 5000/21964]\n",
      "loss: 2.790150  [10000/21964]\n",
      "loss: 2.736334  [15000/21964]\n",
      "loss: 2.836924  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 2.754648 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.735743  [    0/21964]\n",
      "loss: 2.752479  [ 5000/21964]\n",
      "loss: 2.750356  [10000/21964]\n",
      "loss: 2.746767  [15000/21964]\n",
      "loss: 2.682312  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 2.722534 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.689919  [    0/21964]\n",
      "loss: 2.708020  [ 5000/21964]\n",
      "loss: 2.758317  [10000/21964]\n",
      "loss: 2.694862  [15000/21964]\n",
      "loss: 2.607072  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 2.718914 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.763991  [    0/21964]\n",
      "loss: 2.795844  [ 5000/21964]\n",
      "loss: 2.683887  [10000/21964]\n",
      "loss: 2.685373  [15000/21964]\n",
      "loss: 2.716720  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 2.709734 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.711978  [    0/21964]\n",
      "loss: 2.708848  [ 5000/21964]\n",
      "loss: 2.716270  [10000/21964]\n",
      "loss: 2.739412  [15000/21964]\n",
      "loss: 2.638054  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 2.686933 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.671218  [    0/21964]\n",
      "loss: 2.668092  [ 5000/21964]\n",
      "loss: 2.654477  [10000/21964]\n",
      "loss: 2.654458  [15000/21964]\n",
      "loss: 2.686923  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 2.673308 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.719354  [    0/21964]\n",
      "loss: 2.724868  [ 5000/21964]\n",
      "loss: 2.687205  [10000/21964]\n",
      "loss: 2.748351  [15000/21964]\n",
      "loss: 2.594745  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 2.644472 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.680998  [    0/21964]\n",
      "loss: 2.639673  [ 5000/21964]\n",
      "loss: 2.686421  [10000/21964]\n",
      "loss: 2.668573  [15000/21964]\n",
      "loss: 2.643098  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 2.641899 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.695406  [    0/21964]\n",
      "loss: 2.739156  [ 5000/21964]\n",
      "loss: 2.642206  [10000/21964]\n",
      "loss: 2.649171  [15000/21964]\n",
      "loss: 2.661742  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 2.616263 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.635450  [    0/21964]\n",
      "loss: 2.663150  [ 5000/21964]\n",
      "loss: 2.599853  [10000/21964]\n",
      "loss: 2.605484  [15000/21964]\n",
      "loss: 2.551582  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.601405 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.644494  [    0/21964]\n",
      "loss: 2.698058  [ 5000/21964]\n",
      "loss: 2.575955  [10000/21964]\n",
      "loss: 2.624762  [15000/21964]\n",
      "loss: 2.615049  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.594077 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.558112  [    0/21964]\n",
      "loss: 2.600461  [ 5000/21964]\n",
      "loss: 2.565882  [10000/21964]\n",
      "loss: 2.654032  [15000/21964]\n",
      "loss: 2.560551  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 2.592435 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.595422  [    0/21964]\n",
      "loss: 2.674732  [ 5000/21964]\n",
      "loss: 2.624745  [10000/21964]\n",
      "loss: 2.616819  [15000/21964]\n",
      "loss: 2.535623  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 2.591494 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.655199  [    0/21964]\n",
      "loss: 2.517167  [ 5000/21964]\n",
      "loss: 2.546419  [10000/21964]\n",
      "loss: 2.683110  [15000/21964]\n",
      "loss: 2.565436  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 2.590870 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.566277  [    0/21964]\n",
      "loss: 2.692636  [ 5000/21964]\n",
      "loss: 2.615497  [10000/21964]\n",
      "loss: 2.577169  [15000/21964]\n",
      "loss: 2.595067  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.590034 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.574616  [    0/21964]\n",
      "loss: 2.535466  [ 5000/21964]\n",
      "loss: 2.573638  [10000/21964]\n",
      "loss: 2.603889  [15000/21964]\n",
      "loss: 2.603621  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.589336 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.593919  [    0/21964]\n",
      "loss: 2.613965  [ 5000/21964]\n",
      "loss: 2.528584  [10000/21964]\n",
      "loss: 2.535920  [15000/21964]\n",
      "loss: 2.614047  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.587006 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.557015  [    0/21964]\n",
      "loss: 2.568292  [ 5000/21964]\n",
      "loss: 2.563385  [10000/21964]\n",
      "loss: 2.612685  [15000/21964]\n",
      "loss: 2.537154  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 2.581105 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.550148  [    0/21964]\n",
      "loss: 2.557955  [ 5000/21964]\n",
      "loss: 2.591218  [10000/21964]\n",
      "loss: 2.587820  [15000/21964]\n",
      "loss: 2.584178  [20000/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 2.569895 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.505221  [    0/21964]\n",
      "loss: 2.579863  [ 5000/21964]\n",
      "loss: 2.594874  [10000/21964]\n",
      "loss: 2.582203  [15000/21964]\n",
      "loss: 2.557405  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 2.561646 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.456264  [    0/21964]\n",
      "loss: 2.553991  [ 5000/21964]\n",
      "loss: 2.558101  [10000/21964]\n",
      "loss: 2.555749  [15000/21964]\n",
      "loss: 2.569047  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 2.553241 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.505880  [    0/21964]\n",
      "loss: 2.614731  [ 5000/21964]\n",
      "loss: 2.632721  [10000/21964]\n",
      "loss: 2.632211  [15000/21964]\n",
      "loss: 2.574406  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 2.552321 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.484951  [    0/21964]\n",
      "loss: 2.533165  [ 5000/21964]\n",
      "loss: 2.447984  [10000/21964]\n",
      "loss: 2.489759  [15000/21964]\n",
      "loss: 2.485472  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 2.550159 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.573625  [    0/21964]\n",
      "loss: 2.544113  [ 5000/21964]\n",
      "loss: 2.552618  [10000/21964]\n",
      "loss: 2.573151  [15000/21964]\n",
      "loss: 2.632149  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 2.549379 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.593120  [    0/21964]\n",
      "loss: 2.612218  [ 5000/21964]\n",
      "loss: 2.491543  [10000/21964]\n",
      "loss: 2.553660  [15000/21964]\n",
      "loss: 2.522238  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 2.548545 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.563055  [    0/21964]\n",
      "loss: 2.591473  [ 5000/21964]\n",
      "loss: 2.567038  [10000/21964]\n",
      "loss: 2.575618  [15000/21964]\n",
      "loss: 2.582583  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 2.535525 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.523392  [    0/21964]\n",
      "loss: 2.539352  [ 5000/21964]\n",
      "loss: 2.579453  [10000/21964]\n",
      "loss: 2.484892  [15000/21964]\n",
      "loss: 2.505731  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 2.521949 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.500564  [    0/21964]\n",
      "loss: 2.503843  [ 5000/21964]\n",
      "loss: 2.526489  [10000/21964]\n",
      "loss: 2.538708  [15000/21964]\n",
      "loss: 2.499632  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 2.513228 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.572988  [    0/21964]\n",
      "loss: 2.536765  [ 5000/21964]\n",
      "loss: 2.519536  [10000/21964]\n",
      "loss: 2.563250  [15000/21964]\n",
      "loss: 2.583909  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 2.512069 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.555224  [    0/21964]\n",
      "loss: 2.495640  [ 5000/21964]\n",
      "loss: 2.514012  [10000/21964]\n",
      "loss: 2.563334  [15000/21964]\n",
      "loss: 2.494684  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 2.512288 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.536222  [    0/21964]\n",
      "loss: 2.456414  [ 5000/21964]\n",
      "loss: 2.581395  [10000/21964]\n",
      "loss: 2.532632  [15000/21964]\n",
      "loss: 2.524664  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 2.510781 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.514030  [    0/21964]\n",
      "loss: 2.523350  [ 5000/21964]\n",
      "loss: 2.415960  [10000/21964]\n",
      "loss: 2.552567  [15000/21964]\n",
      "loss: 2.446543  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 2.510677 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.522443  [    0/21964]\n",
      "loss: 2.455253  [ 5000/21964]\n",
      "loss: 2.513701  [10000/21964]\n",
      "loss: 2.486984  [15000/21964]\n",
      "loss: 2.513030  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 2.510258 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.600927  [    0/21964]\n",
      "loss: 2.522955  [ 5000/21964]\n",
      "loss: 2.471200  [10000/21964]\n",
      "loss: 2.572260  [15000/21964]\n",
      "loss: 2.473917  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 2.510085 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.504175  [    0/21964]\n",
      "loss: 2.494045  [ 5000/21964]\n",
      "loss: 2.453943  [10000/21964]\n",
      "loss: 2.494308  [15000/21964]\n",
      "loss: 2.553969  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 2.509229 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.611459  [    0/21964]\n",
      "loss: 2.561589  [ 5000/21964]\n",
      "loss: 2.443822  [10000/21964]\n",
      "loss: 2.474043  [15000/21964]\n",
      "loss: 2.464046  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 2.508996 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.443417  [    0/21964]\n",
      "loss: 2.552359  [ 5000/21964]\n",
      "loss: 2.512524  [10000/21964]\n",
      "loss: 2.532744  [15000/21964]\n",
      "loss: 2.483220  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 2.508499 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.493227  [    0/21964]\n",
      "loss: 2.505338  [ 5000/21964]\n",
      "loss: 2.466410  [10000/21964]\n",
      "loss: 2.543361  [15000/21964]\n",
      "loss: 2.434133  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 2.507023 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.523450  [    0/21964]\n",
      "loss: 2.492921  [ 5000/21964]\n",
      "loss: 2.504169  [10000/21964]\n",
      "loss: 2.463988  [15000/21964]\n",
      "loss: 2.522626  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 2.506903 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.522766  [    0/21964]\n",
      "loss: 2.512884  [ 5000/21964]\n",
      "loss: 2.443354  [10000/21964]\n",
      "loss: 2.452943  [15000/21964]\n",
      "loss: 2.455306  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 2.506748 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.463453  [    0/21964]\n",
      "loss: 2.541949  [ 5000/21964]\n",
      "loss: 2.512674  [10000/21964]\n",
      "loss: 2.549378  [15000/21964]\n",
      "loss: 2.449044  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 2.511634 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.495013  [    0/21964]\n",
      "loss: 2.546294  [ 5000/21964]\n",
      "loss: 2.487468  [10000/21964]\n",
      "loss: 2.485787  [15000/21964]\n",
      "loss: 2.464602  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 2.468458 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.475807  [    0/21964]\n",
      "loss: 2.485021  [ 5000/21964]\n",
      "loss: 2.474480  [10000/21964]\n",
      "loss: 2.464183  [15000/21964]\n",
      "loss: 2.465324  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 2.466353 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.425256  [    0/21964]\n",
      "loss: 2.484054  [ 5000/21964]\n",
      "loss: 2.454173  [10000/21964]\n",
      "loss: 2.415107  [15000/21964]\n",
      "loss: 2.454278  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 2.466046 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.474052  [    0/21964]\n",
      "loss: 2.405985  [ 5000/21964]\n",
      "loss: 2.522402  [10000/21964]\n",
      "loss: 2.473660  [15000/21964]\n",
      "loss: 2.453239  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 2.465695 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 2.482790  [    0/21964]\n",
      "loss: 2.455408  [ 5000/21964]\n",
      "loss: 2.384678  [10000/21964]\n",
      "loss: 2.443676  [15000/21964]\n",
      "loss: 2.443726  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 2.465269 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 2.424064  [    0/21964]\n",
      "loss: 2.454663  [ 5000/21964]\n",
      "loss: 2.463068  [10000/21964]\n",
      "loss: 2.483056  [15000/21964]\n",
      "loss: 2.443517  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 2.464950 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 2.453677  [    0/21964]\n",
      "loss: 2.463170  [ 5000/21964]\n",
      "loss: 2.482878  [10000/21964]\n",
      "loss: 2.492639  [15000/21964]\n",
      "loss: 2.424449  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 2.464962 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 2.434463  [    0/21964]\n",
      "loss: 2.443830  [ 5000/21964]\n",
      "loss: 2.522333  [10000/21964]\n",
      "loss: 2.414134  [15000/21964]\n",
      "loss: 2.424803  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 2.464899 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 2.483060  [    0/21964]\n",
      "loss: 2.473465  [ 5000/21964]\n",
      "loss: 2.443393  [10000/21964]\n",
      "loss: 2.443359  [15000/21964]\n",
      "loss: 2.374422  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 2.463993 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 2.424215  [    0/21964]\n",
      "loss: 2.511804  [ 5000/21964]\n",
      "loss: 2.442914  [10000/21964]\n",
      "loss: 2.512364  [15000/21964]\n",
      "loss: 2.452998  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 2.463801 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 2.433414  [    0/21964]\n",
      "loss: 2.443518  [ 5000/21964]\n",
      "loss: 2.433234  [10000/21964]\n",
      "loss: 2.560925  [15000/21964]\n",
      "loss: 2.433717  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 2.463682 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 2.433452  [    0/21964]\n",
      "loss: 2.570408  [ 5000/21964]\n",
      "loss: 2.453068  [10000/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.482093  [15000/21964]\n",
      "loss: 2.561644  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 2.463677 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 2.482708  [    0/21964]\n",
      "loss: 2.452954  [ 5000/21964]\n",
      "loss: 2.521232  [10000/21964]\n",
      "loss: 2.472353  [15000/21964]\n",
      "loss: 2.482484  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 2.463532 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 2.433625  [    0/21964]\n",
      "loss: 2.530783  [ 5000/21964]\n",
      "loss: 2.462316  [10000/21964]\n",
      "loss: 2.550943  [15000/21964]\n",
      "loss: 2.492183  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 2.463355 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 2.462599  [    0/21964]\n",
      "loss: 2.511870  [ 5000/21964]\n",
      "loss: 2.482333  [10000/21964]\n",
      "loss: 2.472729  [15000/21964]\n",
      "loss: 2.482050  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 2.463306 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 2.511880  [    0/21964]\n",
      "loss: 2.413197  [ 5000/21964]\n",
      "loss: 2.472579  [10000/21964]\n",
      "loss: 2.501470  [15000/21964]\n",
      "loss: 2.472268  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 2.463351 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 2.502089  [    0/21964]\n",
      "loss: 2.413361  [ 5000/21964]\n",
      "loss: 2.442605  [10000/21964]\n",
      "loss: 2.452695  [15000/21964]\n",
      "loss: 2.442656  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 2.463150 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 2.462607  [    0/21964]\n",
      "loss: 2.471915  [ 5000/21964]\n",
      "loss: 2.452637  [10000/21964]\n",
      "loss: 2.442865  [15000/21964]\n",
      "loss: 2.444687  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 2.463290 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 2.443787  [    0/21964]\n",
      "loss: 2.485954  [ 5000/21964]\n",
      "loss: 2.449328  [10000/21964]\n",
      "loss: 2.483441  [15000/21964]\n",
      "loss: 2.482780  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 2.461820 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 2.512746  [    0/21964]\n",
      "loss: 2.452686  [ 5000/21964]\n",
      "loss: 2.482041  [10000/21964]\n",
      "loss: 2.443306  [15000/21964]\n",
      "loss: 2.433850  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.461195 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 2.462313  [    0/21964]\n",
      "loss: 2.414104  [ 5000/21964]\n",
      "loss: 2.492135  [10000/21964]\n",
      "loss: 2.452701  [15000/21964]\n",
      "loss: 2.491514  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.460902 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 2.462080  [    0/21964]\n",
      "loss: 2.461951  [ 5000/21964]\n",
      "loss: 2.482181  [10000/21964]\n",
      "loss: 2.442847  [15000/21964]\n",
      "loss: 2.443058  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.461023 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 2.462469  [    0/21964]\n",
      "loss: 2.442445  [ 5000/21964]\n",
      "loss: 2.413268  [10000/21964]\n",
      "loss: 2.442174  [15000/21964]\n",
      "loss: 2.491709  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.461031 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 2.452315  [    0/21964]\n",
      "loss: 2.481735  [ 5000/21964]\n",
      "loss: 2.462382  [10000/21964]\n",
      "loss: 2.491655  [15000/21964]\n",
      "loss: 2.422970  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.460889 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 2.472510  [    0/21964]\n",
      "loss: 2.433581  [ 5000/21964]\n",
      "loss: 2.471652  [10000/21964]\n",
      "loss: 2.492069  [15000/21964]\n",
      "loss: 2.442686  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.460734 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 2.442991  [    0/21964]\n",
      "loss: 2.510985  [ 5000/21964]\n",
      "loss: 2.491644  [10000/21964]\n",
      "loss: 2.452481  [15000/21964]\n",
      "loss: 2.492254  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.460754 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 2.462306  [    0/21964]\n",
      "loss: 2.472021  [ 5000/21964]\n",
      "loss: 2.442558  [10000/21964]\n",
      "loss: 2.413268  [15000/21964]\n",
      "loss: 2.462447  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.460830 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 2.472156  [    0/21964]\n",
      "loss: 2.511068  [ 5000/21964]\n",
      "loss: 2.511528  [10000/21964]\n",
      "loss: 2.472102  [15000/21964]\n",
      "loss: 2.478695  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 2.476751 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 2.443731  [    0/21964]\n",
      "loss: 2.488041  [ 5000/21964]\n",
      "loss: 2.482327  [10000/21964]\n",
      "loss: 2.433296  [15000/21964]\n",
      "loss: 2.472504  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.460453 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 2.491634  [    0/21964]\n",
      "loss: 2.442799  [ 5000/21964]\n",
      "loss: 2.462313  [10000/21964]\n",
      "loss: 2.461852  [15000/21964]\n",
      "loss: 2.472164  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.460105 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 2.491314  [    0/21964]\n",
      "loss: 2.393357  [ 5000/21964]\n",
      "loss: 2.491160  [10000/21964]\n",
      "loss: 2.520895  [15000/21964]\n",
      "loss: 2.491664  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459990 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 2.501324  [    0/21964]\n",
      "loss: 2.491382  [ 5000/21964]\n",
      "loss: 2.413247  [10000/21964]\n",
      "loss: 2.422923  [15000/21964]\n",
      "loss: 2.413018  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.460019 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 2.422931  [    0/21964]\n",
      "loss: 2.402999  [ 5000/21964]\n",
      "loss: 2.412831  [10000/21964]\n",
      "loss: 2.491598  [15000/21964]\n",
      "loss: 2.491496  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459980 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 2.491418  [    0/21964]\n",
      "loss: 2.462601  [ 5000/21964]\n",
      "loss: 2.413148  [10000/21964]\n",
      "loss: 2.481783  [15000/21964]\n",
      "loss: 2.422754  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459914 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 2.481553  [    0/21964]\n",
      "loss: 2.423131  [ 5000/21964]\n",
      "loss: 2.462592  [10000/21964]\n",
      "loss: 2.423176  [15000/21964]\n",
      "loss: 2.412856  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459862 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 2.432355  [    0/21964]\n",
      "loss: 2.442228  [ 5000/21964]\n",
      "loss: 2.452374  [10000/21964]\n",
      "loss: 2.393826  [15000/21964]\n",
      "loss: 2.471349  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.460028 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 2.413267  [    0/21964]\n",
      "loss: 2.442221  [ 5000/21964]\n",
      "loss: 2.472177  [10000/21964]\n",
      "loss: 2.491489  [15000/21964]\n",
      "loss: 2.462281  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459933 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 2.442573  [    0/21964]\n",
      "loss: 2.452203  [ 5000/21964]\n",
      "loss: 2.491456  [10000/21964]\n",
      "loss: 2.500977  [15000/21964]\n",
      "loss: 2.520667  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459935 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 2.540755  [    0/21964]\n",
      "loss: 2.491077  [ 5000/21964]\n",
      "loss: 2.521050  [10000/21964]\n",
      "loss: 2.520855  [15000/21964]\n",
      "loss: 2.500981  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459881 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 2.520756  [    0/21964]\n",
      "loss: 2.461771  [ 5000/21964]\n",
      "loss: 2.413144  [10000/21964]\n",
      "loss: 2.413253  [15000/21964]\n",
      "loss: 2.491305  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459733 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 2.423361  [    0/21964]\n",
      "loss: 2.472223  [ 5000/21964]\n",
      "loss: 2.422592  [10000/21964]\n",
      "loss: 2.442045  [15000/21964]\n",
      "loss: 2.461964  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459796 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 2.462584  [    0/21964]\n",
      "loss: 2.462224  [ 5000/21964]\n",
      "loss: 2.412692  [10000/21964]\n",
      "loss: 2.481832  [15000/21964]\n",
      "loss: 2.501391  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459796 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 2.422780  [    0/21964]\n",
      "loss: 2.422832  [ 5000/21964]\n",
      "loss: 2.500906  [10000/21964]\n",
      "loss: 2.471711  [15000/21964]\n",
      "loss: 2.550150  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459978 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 2.462323  [    0/21964]\n",
      "loss: 2.471771  [ 5000/21964]\n",
      "loss: 2.462115  [10000/21964]\n",
      "loss: 2.491940  [15000/21964]\n",
      "loss: 2.491181  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.459759 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model2 = NeuralNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-5\n",
    "batch_size = 100\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model2, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8456efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2, 'modelq3-82-Adam-noDO.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe76bba",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "### we add Drouput layer with p =10% and we saw that our accuracy get better  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "24740fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkDo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkDo, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 25),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        #y_pred = logits.argmax(dim=1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e0f0649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.208376  [    0/21964]\n",
      "loss: 3.239563  [ 5000/21964]\n",
      "loss: 3.149308  [10000/21964]\n",
      "loss: 3.202781  [15000/21964]\n",
      "loss: 3.106319  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 11.2%, Avg loss: 3.168290 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.194784  [    0/21964]\n",
      "loss: 3.184693  [ 5000/21964]\n",
      "loss: 3.109911  [10000/21964]\n",
      "loss: 3.125273  [15000/21964]\n",
      "loss: 3.056277  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 17.5%, Avg loss: 3.110212 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.142745  [    0/21964]\n",
      "loss: 3.082219  [ 5000/21964]\n",
      "loss: 3.034920  [10000/21964]\n",
      "loss: 3.119250  [15000/21964]\n",
      "loss: 3.118903  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 20.8%, Avg loss: 3.077843 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.030807  [    0/21964]\n",
      "loss: 3.032887  [ 5000/21964]\n",
      "loss: 3.101214  [10000/21964]\n",
      "loss: 3.084191  [15000/21964]\n",
      "loss: 3.055541  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 22.9%, Avg loss: 3.058642 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.020005  [    0/21964]\n",
      "loss: 3.036294  [ 5000/21964]\n",
      "loss: 3.075103  [10000/21964]\n",
      "loss: 3.022948  [15000/21964]\n",
      "loss: 3.047524  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 26.0%, Avg loss: 3.030102 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.046989  [    0/21964]\n",
      "loss: 3.034212  [ 5000/21964]\n",
      "loss: 3.028655  [10000/21964]\n",
      "loss: 2.925924  [15000/21964]\n",
      "loss: 2.990370  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 31.8%, Avg loss: 2.976009 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.025421  [    0/21964]\n",
      "loss: 2.934636  [ 5000/21964]\n",
      "loss: 2.981982  [10000/21964]\n",
      "loss: 2.992749  [15000/21964]\n",
      "loss: 2.954550  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.944156 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.903291  [    0/21964]\n",
      "loss: 2.891768  [ 5000/21964]\n",
      "loss: 2.992326  [10000/21964]\n",
      "loss: 2.995666  [15000/21964]\n",
      "loss: 2.879796  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 2.923791 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.791147  [    0/21964]\n",
      "loss: 2.901967  [ 5000/21964]\n",
      "loss: 2.947008  [10000/21964]\n",
      "loss: 2.969921  [15000/21964]\n",
      "loss: 2.935619  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.897822 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.962169  [    0/21964]\n",
      "loss: 2.860912  [ 5000/21964]\n",
      "loss: 2.869993  [10000/21964]\n",
      "loss: 2.865933  [15000/21964]\n",
      "loss: 2.799200  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 2.883584 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.922914  [    0/21964]\n",
      "loss: 2.922078  [ 5000/21964]\n",
      "loss: 2.864854  [10000/21964]\n",
      "loss: 2.865003  [15000/21964]\n",
      "loss: 2.852593  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 42.6%, Avg loss: 2.869213 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.848026  [    0/21964]\n",
      "loss: 2.803213  [ 5000/21964]\n",
      "loss: 2.887567  [10000/21964]\n",
      "loss: 2.880495  [15000/21964]\n",
      "loss: 2.794153  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 46.2%, Avg loss: 2.837841 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.778416  [    0/21964]\n",
      "loss: 2.846419  [ 5000/21964]\n",
      "loss: 2.866998  [10000/21964]\n",
      "loss: 2.856633  [15000/21964]\n",
      "loss: 2.797385  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 2.818666 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.774963  [    0/21964]\n",
      "loss: 2.817956  [ 5000/21964]\n",
      "loss: 2.789662  [10000/21964]\n",
      "loss: 2.749712  [15000/21964]\n",
      "loss: 2.840690  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 2.790190 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.721092  [    0/21964]\n",
      "loss: 2.832801  [ 5000/21964]\n",
      "loss: 2.745037  [10000/21964]\n",
      "loss: 2.778139  [15000/21964]\n",
      "loss: 2.731552  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 2.774077 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.825036  [    0/21964]\n",
      "loss: 2.790894  [ 5000/21964]\n",
      "loss: 2.784346  [10000/21964]\n",
      "loss: 2.755757  [15000/21964]\n",
      "loss: 2.789679  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 2.763106 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.766837  [    0/21964]\n",
      "loss: 2.807779  [ 5000/21964]\n",
      "loss: 2.707604  [10000/21964]\n",
      "loss: 2.755768  [15000/21964]\n",
      "loss: 2.673135  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 2.757645 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.801533  [    0/21964]\n",
      "loss: 2.761713  [ 5000/21964]\n",
      "loss: 2.667891  [10000/21964]\n",
      "loss: 2.762219  [15000/21964]\n",
      "loss: 2.748144  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 2.750000 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.810403  [    0/21964]\n",
      "loss: 2.693308  [ 5000/21964]\n",
      "loss: 2.753074  [10000/21964]\n",
      "loss: 2.696023  [15000/21964]\n",
      "loss: 2.830809  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 2.742809 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.832437  [    0/21964]\n",
      "loss: 2.800860  [ 5000/21964]\n",
      "loss: 2.813296  [10000/21964]\n",
      "loss: 2.707725  [15000/21964]\n",
      "loss: 2.734930  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 2.738517 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.734733  [    0/21964]\n",
      "loss: 2.688959  [ 5000/21964]\n",
      "loss: 2.785052  [10000/21964]\n",
      "loss: 2.657210  [15000/21964]\n",
      "loss: 2.795718  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 2.734545 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.743966  [    0/21964]\n",
      "loss: 2.699732  [ 5000/21964]\n",
      "loss: 2.746912  [10000/21964]\n",
      "loss: 2.707250  [15000/21964]\n",
      "loss: 2.639435  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 2.728892 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.795833  [    0/21964]\n",
      "loss: 2.651956  [ 5000/21964]\n",
      "loss: 2.786047  [10000/21964]\n",
      "loss: 2.717408  [15000/21964]\n",
      "loss: 2.781650  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 2.720042 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.731254  [    0/21964]\n",
      "loss: 2.762039  [ 5000/21964]\n",
      "loss: 2.686939  [10000/21964]\n",
      "loss: 2.670464  [15000/21964]\n",
      "loss: 2.791566  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 2.713226 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.733882  [    0/21964]\n",
      "loss: 2.699877  [ 5000/21964]\n",
      "loss: 2.778177  [10000/21964]\n",
      "loss: 2.707650  [15000/21964]\n",
      "loss: 2.720129  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 2.704157 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.747429  [    0/21964]\n",
      "loss: 2.800086  [ 5000/21964]\n",
      "loss: 2.706233  [10000/21964]\n",
      "loss: 2.704637  [15000/21964]\n",
      "loss: 2.681004  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 2.697870 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.685774  [    0/21964]\n",
      "loss: 2.720760  [ 5000/21964]\n",
      "loss: 2.664918  [10000/21964]\n",
      "loss: 2.688318  [15000/21964]\n",
      "loss: 2.733308  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 2.692143 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.724684  [    0/21964]\n",
      "loss: 2.773159  [ 5000/21964]\n",
      "loss: 2.642879  [10000/21964]\n",
      "loss: 2.702508  [15000/21964]\n",
      "loss: 2.736508  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 2.682199 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.729878  [    0/21964]\n",
      "loss: 2.718738  [ 5000/21964]\n",
      "loss: 2.604305  [10000/21964]\n",
      "loss: 2.674480  [15000/21964]\n",
      "loss: 2.612285  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 2.674036 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.648457  [    0/21964]\n",
      "loss: 2.583624  [ 5000/21964]\n",
      "loss: 2.722533  [10000/21964]\n",
      "loss: 2.644444  [15000/21964]\n",
      "loss: 2.601691  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 2.664143 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.663908  [    0/21964]\n",
      "loss: 2.630854  [ 5000/21964]\n",
      "loss: 2.651087  [10000/21964]\n",
      "loss: 2.549820  [15000/21964]\n",
      "loss: 2.694105  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 2.654321 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.698149  [    0/21964]\n",
      "loss: 2.609283  [ 5000/21964]\n",
      "loss: 2.614222  [10000/21964]\n",
      "loss: 2.630873  [15000/21964]\n",
      "loss: 2.674844  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 2.645587 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.636427  [    0/21964]\n",
      "loss: 2.675591  [ 5000/21964]\n",
      "loss: 2.668750  [10000/21964]\n",
      "loss: 2.705019  [15000/21964]\n",
      "loss: 2.641753  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 2.630271 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.649609  [    0/21964]\n",
      "loss: 2.625776  [ 5000/21964]\n",
      "loss: 2.647885  [10000/21964]\n",
      "loss: 2.633950  [15000/21964]\n",
      "loss: 2.600297  [20000/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 2.619292 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.535650  [    0/21964]\n",
      "loss: 2.642673  [ 5000/21964]\n",
      "loss: 2.624371  [10000/21964]\n",
      "loss: 2.623555  [15000/21964]\n",
      "loss: 2.594264  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 2.613654 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.585063  [    0/21964]\n",
      "loss: 2.579064  [ 5000/21964]\n",
      "loss: 2.655384  [10000/21964]\n",
      "loss: 2.571418  [15000/21964]\n",
      "loss: 2.586741  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.606673 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.602686  [    0/21964]\n",
      "loss: 2.584917  [ 5000/21964]\n",
      "loss: 2.547983  [10000/21964]\n",
      "loss: 2.569663  [15000/21964]\n",
      "loss: 2.609326  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.604024 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.564687  [    0/21964]\n",
      "loss: 2.580811  [ 5000/21964]\n",
      "loss: 2.611631  [10000/21964]\n",
      "loss: 2.649681  [15000/21964]\n",
      "loss: 2.600689  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 2.587478 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.575334  [    0/21964]\n",
      "loss: 2.552833  [ 5000/21964]\n",
      "loss: 2.536851  [10000/21964]\n",
      "loss: 2.533893  [15000/21964]\n",
      "loss: 2.657916  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 2.573128 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.524355  [    0/21964]\n",
      "loss: 2.594730  [ 5000/21964]\n",
      "loss: 2.633012  [10000/21964]\n",
      "loss: 2.566247  [15000/21964]\n",
      "loss: 2.529855  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 2.567750 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.625304  [    0/21964]\n",
      "loss: 2.540561  [ 5000/21964]\n",
      "loss: 2.553485  [10000/21964]\n",
      "loss: 2.599352  [15000/21964]\n",
      "loss: 2.525493  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 2.562266 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.549390  [    0/21964]\n",
      "loss: 2.532168  [ 5000/21964]\n",
      "loss: 2.477648  [10000/21964]\n",
      "loss: 2.568369  [15000/21964]\n",
      "loss: 2.546811  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 2.553281 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.526769  [    0/21964]\n",
      "loss: 2.660235  [ 5000/21964]\n",
      "loss: 2.585311  [10000/21964]\n",
      "loss: 2.533311  [15000/21964]\n",
      "loss: 2.495920  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 2.534765 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.607556  [    0/21964]\n",
      "loss: 2.575454  [ 5000/21964]\n",
      "loss: 2.539244  [10000/21964]\n",
      "loss: 2.484159  [15000/21964]\n",
      "loss: 2.522581  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 2.512659 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.584562  [    0/21964]\n",
      "loss: 2.467449  [ 5000/21964]\n",
      "loss: 2.470948  [10000/21964]\n",
      "loss: 2.451648  [15000/21964]\n",
      "loss: 2.436879  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 2.505955 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.474591  [    0/21964]\n",
      "loss: 2.483328  [ 5000/21964]\n",
      "loss: 2.461421  [10000/21964]\n",
      "loss: 2.528373  [15000/21964]\n",
      "loss: 2.490548  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 2.498183 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.505726  [    0/21964]\n",
      "loss: 2.530312  [ 5000/21964]\n",
      "loss: 2.532914  [10000/21964]\n",
      "loss: 2.528996  [15000/21964]\n",
      "loss: 2.517336  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 2.493958 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.511462  [    0/21964]\n",
      "loss: 2.456730  [ 5000/21964]\n",
      "loss: 2.490886  [10000/21964]\n",
      "loss: 2.472796  [15000/21964]\n",
      "loss: 2.520856  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 2.487653 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.535235  [    0/21964]\n",
      "loss: 2.483262  [ 5000/21964]\n",
      "loss: 2.479016  [10000/21964]\n",
      "loss: 2.528748  [15000/21964]\n",
      "loss: 2.496462  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 2.484285 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.467430  [    0/21964]\n",
      "loss: 2.509384  [ 5000/21964]\n",
      "loss: 2.486322  [10000/21964]\n",
      "loss: 2.446789  [15000/21964]\n",
      "loss: 2.516962  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 2.474932 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.501612  [    0/21964]\n",
      "loss: 2.491331  [ 5000/21964]\n",
      "loss: 2.462094  [10000/21964]\n",
      "loss: 2.440799  [15000/21964]\n",
      "loss: 2.441353  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 2.464949 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.463785  [    0/21964]\n",
      "loss: 2.501597  [ 5000/21964]\n",
      "loss: 2.474439  [10000/21964]\n",
      "loss: 2.503386  [15000/21964]\n",
      "loss: 2.439688  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 2.458207 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.496408  [    0/21964]\n",
      "loss: 2.436441  [ 5000/21964]\n",
      "loss: 2.472814  [10000/21964]\n",
      "loss: 2.430235  [15000/21964]\n",
      "loss: 2.422594  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 2.448966 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.404961  [    0/21964]\n",
      "loss: 2.445871  [ 5000/21964]\n",
      "loss: 2.447278  [10000/21964]\n",
      "loss: 2.425263  [15000/21964]\n",
      "loss: 2.392985  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 2.441102 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.435741  [    0/21964]\n",
      "loss: 2.408531  [ 5000/21964]\n",
      "loss: 2.509351  [10000/21964]\n",
      "loss: 2.381240  [15000/21964]\n",
      "loss: 2.424122  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 2.438581 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.424676  [    0/21964]\n",
      "loss: 2.472116  [ 5000/21964]\n",
      "loss: 2.457087  [10000/21964]\n",
      "loss: 2.431075  [15000/21964]\n",
      "loss: 2.404112  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 2.433697 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.472894  [    0/21964]\n",
      "loss: 2.425084  [ 5000/21964]\n",
      "loss: 2.476910  [10000/21964]\n",
      "loss: 2.423253  [15000/21964]\n",
      "loss: 2.416400  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 2.426881 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.440255  [    0/21964]\n",
      "loss: 2.417446  [ 5000/21964]\n",
      "loss: 2.430893  [10000/21964]\n",
      "loss: 2.401098  [15000/21964]\n",
      "loss: 2.438975  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 2.422767 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.426438  [    0/21964]\n",
      "loss: 2.402443  [ 5000/21964]\n",
      "loss: 2.450406  [10000/21964]\n",
      "loss: 2.462450  [15000/21964]\n",
      "loss: 2.475199  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 2.419893 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.408669  [    0/21964]\n",
      "loss: 2.404880  [ 5000/21964]\n",
      "loss: 2.400057  [10000/21964]\n",
      "loss: 2.447956  [15000/21964]\n",
      "loss: 2.364216  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 2.418002 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 2.362638  [    0/21964]\n",
      "loss: 2.378349  [ 5000/21964]\n",
      "loss: 2.399508  [10000/21964]\n",
      "loss: 2.430791  [15000/21964]\n",
      "loss: 2.432493  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 2.415935 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 2.471849  [    0/21964]\n",
      "loss: 2.413779  [ 5000/21964]\n",
      "loss: 2.373584  [10000/21964]\n",
      "loss: 2.426300  [15000/21964]\n",
      "loss: 2.480061  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 2.410153 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 2.406328  [    0/21964]\n",
      "loss: 2.345996  [ 5000/21964]\n",
      "loss: 2.399913  [10000/21964]\n",
      "loss: 2.375809  [15000/21964]\n",
      "loss: 2.417111  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 2.408447 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 2.390328  [    0/21964]\n",
      "loss: 2.468189  [ 5000/21964]\n",
      "loss: 2.442223  [10000/21964]\n",
      "loss: 2.423493  [15000/21964]\n",
      "loss: 2.419112  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 2.409402 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 2.410546  [    0/21964]\n",
      "loss: 2.347680  [ 5000/21964]\n",
      "loss: 2.400064  [10000/21964]\n",
      "loss: 2.458852  [15000/21964]\n",
      "loss: 2.400306  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 2.407679 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 2.421611  [    0/21964]\n",
      "loss: 2.350638  [ 5000/21964]\n",
      "loss: 2.413561  [10000/21964]\n",
      "loss: 2.380095  [15000/21964]\n",
      "loss: 2.400341  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 2.404380 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 2.358260  [    0/21964]\n",
      "loss: 2.420506  [ 5000/21964]\n",
      "loss: 2.379460  [10000/21964]\n",
      "loss: 2.339019  [15000/21964]\n",
      "loss: 2.469814  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 2.403385 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 2.430314  [    0/21964]\n",
      "loss: 2.410178  [ 5000/21964]\n",
      "loss: 2.463878  [10000/21964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.385137  [15000/21964]\n",
      "loss: 2.431690  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 2.402339 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 2.389345  [    0/21964]\n",
      "loss: 2.378355  [ 5000/21964]\n",
      "loss: 2.386674  [10000/21964]\n",
      "loss: 2.395140  [15000/21964]\n",
      "loss: 2.426989  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 2.406006 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 2.382778  [    0/21964]\n",
      "loss: 2.427016  [ 5000/21964]\n",
      "loss: 2.374531  [10000/21964]\n",
      "loss: 2.387939  [15000/21964]\n",
      "loss: 2.399994  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 2.400693 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 2.416373  [    0/21964]\n",
      "loss: 2.388457  [ 5000/21964]\n",
      "loss: 2.415884  [10000/21964]\n",
      "loss: 2.418427  [15000/21964]\n",
      "loss: 2.332915  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 2.400360 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 2.399824  [    0/21964]\n",
      "loss: 2.387015  [ 5000/21964]\n",
      "loss: 2.366758  [10000/21964]\n",
      "loss: 2.388520  [15000/21964]\n",
      "loss: 2.436870  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 2.401925 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 2.379068  [    0/21964]\n",
      "loss: 2.388360  [ 5000/21964]\n",
      "loss: 2.445015  [10000/21964]\n",
      "loss: 2.384537  [15000/21964]\n",
      "loss: 2.336157  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 2.398482 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 2.342663  [    0/21964]\n",
      "loss: 2.408997  [ 5000/21964]\n",
      "loss: 2.393561  [10000/21964]\n",
      "loss: 2.405041  [15000/21964]\n",
      "loss: 2.398666  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 2.398439 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 2.367756  [    0/21964]\n",
      "loss: 2.453202  [ 5000/21964]\n",
      "loss: 2.370073  [10000/21964]\n",
      "loss: 2.368011  [15000/21964]\n",
      "loss: 2.370653  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 2.398747 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 2.413898  [    0/21964]\n",
      "loss: 2.409129  [ 5000/21964]\n",
      "loss: 2.369010  [10000/21964]\n",
      "loss: 2.389675  [15000/21964]\n",
      "loss: 2.407751  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 2.399200 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 2.379909  [    0/21964]\n",
      "loss: 2.409339  [ 5000/21964]\n",
      "loss: 2.400879  [10000/21964]\n",
      "loss: 2.434141  [15000/21964]\n",
      "loss: 2.377538  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 2.396112 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 2.383890  [    0/21964]\n",
      "loss: 2.379016  [ 5000/21964]\n",
      "loss: 2.396425  [10000/21964]\n",
      "loss: 2.379952  [15000/21964]\n",
      "loss: 2.377685  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 2.395890 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 2.435851  [    0/21964]\n",
      "loss: 2.408373  [ 5000/21964]\n",
      "loss: 2.389408  [10000/21964]\n",
      "loss: 2.362773  [15000/21964]\n",
      "loss: 2.390307  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 2.395798 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 2.357984  [    0/21964]\n",
      "loss: 2.380754  [ 5000/21964]\n",
      "loss: 2.450513  [10000/21964]\n",
      "loss: 2.404591  [15000/21964]\n",
      "loss: 2.406651  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 2.395402 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 2.422082  [    0/21964]\n",
      "loss: 2.373411  [ 5000/21964]\n",
      "loss: 2.411544  [10000/21964]\n",
      "loss: 2.420377  [15000/21964]\n",
      "loss: 2.354269  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 2.394008 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 2.377374  [    0/21964]\n",
      "loss: 2.380742  [ 5000/21964]\n",
      "loss: 2.353944  [10000/21964]\n",
      "loss: 2.353869  [15000/21964]\n",
      "loss: 2.366036  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 2.394940 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 2.416152  [    0/21964]\n",
      "loss: 2.396279  [ 5000/21964]\n",
      "loss: 2.350009  [10000/21964]\n",
      "loss: 2.413311  [15000/21964]\n",
      "loss: 2.393686  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 2.392671 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 2.445571  [    0/21964]\n",
      "loss: 2.349297  [ 5000/21964]\n",
      "loss: 2.442528  [10000/21964]\n",
      "loss: 2.396676  [15000/21964]\n",
      "loss: 2.394453  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 2.393205 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 2.405171  [    0/21964]\n",
      "loss: 2.375832  [ 5000/21964]\n",
      "loss: 2.415472  [10000/21964]\n",
      "loss: 2.391379  [15000/21964]\n",
      "loss: 2.387270  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 2.390551 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 2.443362  [    0/21964]\n",
      "loss: 2.396502  [ 5000/21964]\n",
      "loss: 2.375666  [10000/21964]\n",
      "loss: 2.316075  [15000/21964]\n",
      "loss: 2.407099  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 2.387061 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 2.389161  [    0/21964]\n",
      "loss: 2.365161  [ 5000/21964]\n",
      "loss: 2.367188  [10000/21964]\n",
      "loss: 2.413708  [15000/21964]\n",
      "loss: 2.359656  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 2.378363 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 2.348178  [    0/21964]\n",
      "loss: 2.413750  [ 5000/21964]\n",
      "loss: 2.373288  [10000/21964]\n",
      "loss: 2.318760  [15000/21964]\n",
      "loss: 2.388546  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 2.369306 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 2.408735  [    0/21964]\n",
      "loss: 2.343429  [ 5000/21964]\n",
      "loss: 2.375691  [10000/21964]\n",
      "loss: 2.376818  [15000/21964]\n",
      "loss: 2.333831  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 2.370292 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 2.400112  [    0/21964]\n",
      "loss: 2.375522  [ 5000/21964]\n",
      "loss: 2.360932  [10000/21964]\n",
      "loss: 2.334653  [15000/21964]\n",
      "loss: 2.393050  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 92.8%, Avg loss: 2.362172 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 2.360240  [    0/21964]\n",
      "loss: 2.313465  [ 5000/21964]\n",
      "loss: 2.329867  [10000/21964]\n",
      "loss: 2.360846  [15000/21964]\n",
      "loss: 2.379474  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 2.359344 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 2.318246  [    0/21964]\n",
      "loss: 2.348093  [ 5000/21964]\n",
      "loss: 2.371342  [10000/21964]\n",
      "loss: 2.369078  [15000/21964]\n",
      "loss: 2.337414  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 2.359683 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 2.339248  [    0/21964]\n",
      "loss: 2.354029  [ 5000/21964]\n",
      "loss: 2.349871  [10000/21964]\n",
      "loss: 2.367865  [15000/21964]\n",
      "loss: 2.311460  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 2.355392 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 2.342087  [    0/21964]\n",
      "loss: 2.369192  [ 5000/21964]\n",
      "loss: 2.319870  [10000/21964]\n",
      "loss: 2.341438  [15000/21964]\n",
      "loss: 2.370238  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 2.355000 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 2.386290  [    0/21964]\n",
      "loss: 2.375559  [ 5000/21964]\n",
      "loss: 2.321308  [10000/21964]\n",
      "loss: 2.370512  [15000/21964]\n",
      "loss: 2.365465  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 2.354336 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 2.351395  [    0/21964]\n",
      "loss: 2.339400  [ 5000/21964]\n",
      "loss: 2.318029  [10000/21964]\n",
      "loss: 2.347222  [15000/21964]\n",
      "loss: 2.377718  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 2.354591 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 2.357740  [    0/21964]\n",
      "loss: 2.338227  [ 5000/21964]\n",
      "loss: 2.349250  [10000/21964]\n",
      "loss: 2.357220  [15000/21964]\n",
      "loss: 2.386451  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 2.353628 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 2.348240  [    0/21964]\n",
      "loss: 2.332061  [ 5000/21964]\n",
      "loss: 2.363752  [10000/21964]\n",
      "loss: 2.338063  [15000/21964]\n",
      "loss: 2.349996  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 2.353569 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 2.318140  [    0/21964]\n",
      "loss: 2.358539  [ 5000/21964]\n",
      "loss: 2.386204  [10000/21964]\n",
      "loss: 2.366142  [15000/21964]\n",
      "loss: 2.313390  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 2.352500 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 2.366899  [    0/21964]\n",
      "loss: 2.347601  [ 5000/21964]\n",
      "loss: 2.320965  [10000/21964]\n",
      "loss: 2.373453  [15000/21964]\n",
      "loss: 2.339421  [20000/21964]\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 2.352081 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model3 = NeuralNetworkDo().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-5\n",
    "batch_size = 100\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model3, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model3, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "788ba6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model3, 'modelq3-93-Adam-10DO.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7f0b9",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "### At the end we test our model on test set and the test accuracy was 75% exactly!  \n",
    "### this model was saved as 'modelq3-93-Adam-10DO.pth' in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b87b9365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.75\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('modelq3-93-Adam-10DO.pth')\n",
    "df_test = pd.read_csv('HW2_data\\Q3_test.csv')\n",
    "test = df_test.to_numpy()\n",
    "label = torch.tensor(test[:,0])\n",
    "test = torch.tensor(test[:,1:])\n",
    "X = test.float()\n",
    "logits = model(X).argmax(1)\n",
    "print(f'accuracy on test set is {float(torch.eq(logits,label).sum()/len(label)):0.2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
