# Implementation of Multi-Modal System for Sentiment Estimation

## Description 
In this project, we implement a multi-modal system using images and texts to estimate sentiments in sequences of movies. The implementation is carried out in four phases, [MSCTD: A Multimodal Sentiment Chat Translation Dataset](https://aclanthology.org/2022.acl-long.186/)


## Phase 0
In Phase 0, we conducted statistical analysis on dataset. You can find the report [linked] here.

## Phase 1
In Phase 1, we analyze emotions in images using the available images in the dataset through two approaches:

Firstly, we intend to use the faces present in the images to detect emotions.

Secondly, we aim to use the entire image to detect emotions.

Finally, our inference is based on the combination of these two methods.
You can find the report [linked] here.

## Phase 2

In this phase, we will focus on implementing the sentiment analysis section based on text. In this regard, we will utilize various representation methods for words (and sentences) and ultimately compare the outputs of different methods with each other.You can find the report [linked] here.

## Phase 3 

In the first phase, we became familiar with processing facial data and implemented a model for sentiment detection based on it. In the second phase, we performed the same task based on textual data. In this phase, we will attempt to combine the obtained results from image and text data to enhance the performance of the final model in sentiment representation detection. Additionally, we will become acquainted with utilizing various modalities in familiar applications. You can find the report [linked] here.



