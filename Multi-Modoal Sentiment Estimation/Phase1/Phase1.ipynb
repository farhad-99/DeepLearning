{"cells":[{"cell_type":"markdown","source":["## Loading Libraries and Classes "],"metadata":{"id":"5fDYW3XFeIVi"}},{"cell_type":"markdown","source":["you can mount this project models and datasets from drive:\n","https://drive.google.com/drive/folders/1ICCQoevk0oxKjs7P-omHGq5HpFCKgbAe?usp=share_link"],"metadata":{"id":"1pdDiylT525r"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZflRorFHYkDN","outputId":"a6af0eda-74ea-4559-baef-a6b98b73806d","executionInfo":{"status":"ok","timestamp":1676212412959,"user_tz":-210,"elapsed":111484,"user":{"displayName":"alone boy","userId":"12897264805395612473"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qM1DyjueYs1C","outputId":"0da67016-88d4-4c2e-d668-b14e72aae1c1","executionInfo":{"status":"ok","timestamp":1676212421513,"user_tz":-210,"elapsed":8559,"user":{"displayName":"alone boy","userId":"12897264805395612473"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["import os\n","import cv2\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets\n","from torchvision.io import read_image\n","from torchvision.transforms import ToTensor\n","from torchvision import transforms\n","from torch.utils.data import Dataset , DataLoader\n","from torchvision.models import resnet18,resnet50 ,ResNet50_Weights\n","\n","os.system('pip install facenet-pytorch')\n","from facenet_pytorch import MTCNN\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"V3Olp-ECYvDi","executionInfo":{"status":"ok","timestamp":1676212594936,"user_tz":-210,"elapsed":498,"user":{"displayName":"alone boy","userId":"12897264805395612473"}}},"outputs":[],"source":["class MSCTDDataset(Dataset):\n","    def __init__(self, img_dir, mode , transform=None, target_transform=None,resize = None):\n","        # downloading text files from github\n","        print('\\nLoading Text Files')\n","        os.system('git clone https://github.com/XL2248/MSCTD.git')\n","        #unziping images\n","        if mode == 'train':\n","          print('Loading Train Images')\n","          os.system('unzip -n '+ os.path.join(img_dir,'train_ende.zip'))\n","          os.system('mv train_ende train')\n","          print('Train Images Count:', len(os.listdir('train')))\n","          os.system('cp -r MSCTD/MSCTD_data/ende/english_train.txt -t train')\n","          os.system('cp -r MSCTD/MSCTD_data/ende/image_index_train.txt -t train')\n","          os.system('cp -r MSCTD/MSCTD_data/ende/sentiment_train.txt -t train')\n","        \n","\n","        if mode == 'dev':\n","          print('Loading Validation Images')\n","          os.system('unzip -n '+ os.path.join(img_dir,'dev.zip'))\n","          print('Dev Images Count:', len(os.listdir('dev')))\n","          os.system('cp -r MSCTD/MSCTD_data/ende/english_dev.txt -t dev')\n","          os.system('cp -r MSCTD/MSCTD_data/ende/image_index_dev.txt -t dev')\n","          os.system('cp -r MSCTD/MSCTD_data/ende/sentiment_dev.txt -t dev')\n","        \n","\n","        if mode == 'test':\n","          print('Loading Test Images')\n","          os.system('unzip -n '+ os.path.join(img_dir,'test.zip'))\n","          print('Test Images Count:', len(os.listdir('test')))\n","          os.system('cp -r MSCTD/MSCTD_data/ende/english_test.txt -t test')\n","          os.system('cp -r MSCTD/MSCTD_data/ende/image_index_test.txt -t test')\n","          os.system('cp -r MSCTD/MSCTD_data/ende/sentiment_test.txt -t test')\n","\n","        os.system('rm -r MSCTD')\n","        # processing text files and saving them as attribute of dataset\n","        if mode == 'val':\n","            mode = 'dev'\n","        file1 = open(mode + '/sentiment_' + mode + '.txt', 'r')\n","        Lines = file1.readlines()\n","        file1.close()\n","        label = []\n","        for line in Lines:\n","            line = line.strip()\n","            label.append(int(line))         \n","        self.sentiment = np.array(label)\n","\n","        file1 = open(mode + '/english_' + mode + '.txt', 'r')\n","        Lines = file1.readlines()\n","        file1.close()\n","        text = []\n","        for line in Lines:\n","            line = line.strip()\n","            text.append(line)  \n","        self.text = text\n","\n","        image_index = []\n","        file1 = open(mode + '/image_index_' + mode + '.txt', 'r')\n","        Lines = file1.readlines()\n","        file1.close()\n","        text = []\n","        for line in Lines:\n","            line = line.strip()\n","            image_index.append(line) \n","        self.image_index = image_index\n","\n","        self.mode = mode\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.resize = resize\n","        \n","    def __len__(self):\n","        return len(self.sentiment)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.mode, f'{idx}.jpg')\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n","        sentiment = self.sentiment[idx]\n","        text = self.text[idx]\n","        if self.resize:\n","              image = cv2.resize(image, self.resize) \n","        else :\n","              image = cv2.resize(image, (1280,633)) \n","\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            sentiment = self.target_transform(sentiment)\n","        return {'text':text ,'image':image, 'sentiment':(sentiment)}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wJM2xEGY1HY","outputId":"e4ec7221-59eb-451e-b5d8-f0ddc0269765"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading Text Files\n","Loading Train Images\n","Train Images Count: 2146\n","\n","Loading Text Files\n"]}],"source":["train_data = MSCTDDataset(img_dir = '/content/drive/MyDrive/Project/Phase0', mode = 'train', resize = (640,316))\n","val_data = MSCTDDataset(img_dir = '/content/drive/MyDrive/Project/Phase0', mode = 'dev', resize = (640,316))\n","test_data = MSCTDDataset(img_dir = '/content/drive/MyDrive/Project/Phase0', mode = 'test',  resize = (640,316))\n","\n","batch_size=128\n","train_dataloader =  DataLoader(train_data, batch_size=batch_size)\n","val_dataloader =  DataLoader(val_data, batch_size=batch_size)\n","test_dataloader =  DataLoader(test_data, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"tCa7pqptifE9"},"source":["# Part 1"]},{"cell_type":"markdown","source":["## Part 1-1"],"metadata":{"id":"34E9JjFa6_zm"}},{"cell_type":"markdown","metadata":{"id":"g8q0OEbeigtb"},"source":["### Part 1-1-1 (Face Extraction)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wBEh_M0GPqat"},"outputs":[],"source":["class faceDataset(Dataset):\n","    def __init__(self, mode, dir, transform=None, target_transform=None, resize=None):\n","        self.mode = mode\n","        os.system(f'unzip -n {dir}/face_{self.mode}.zip')\n","        \n","        file1 = open(f'face_{self.mode}/face_{self.mode}.txt', 'r')\n","        Lines = file1.readlines()\n","        file1.close()\n","        label = []\n","        for line in Lines:\n","            line = line.strip()\n","            label.append(int(line))         \n","        self.sentiment = np.array(label)\n","\n","        \n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.resize = resize\n","        \n","    def __len__(self):\n","        return len(self.sentiment)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(f'face_{self.mode}', f'{idx}.jpg')\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n","        sentiment = self.sentiment[idx] \n","        if self.resize:\n","              image = cv2.resize(image, self.resize) \n","\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            sentiment = self.target_transform(sentiment)\n","        return {'image':image, 'sentiment':(sentiment)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hR-AK8yinKD"},"outputs":[],"source":["def face_detect(mode, data_loader, dir):\n","    mtcnn = MTCNN(select_largest=False, post_process=False, device='cuda:0')\n","    i = 0\n","    face_sentiment = []\n","    if (os.path.isdir(f'face_{mode}')==0):\n","      os.mkdir(f'face_{mode}')\n","\n","    for batch in data_loader:\n","      images = batch['image'].numpy()\n","      sentiments = batch['sentiment'].numpy()\n","      img_snt = zip(images,sentiments) \n","\n","      for img,snt in img_snt:\n","        boxes, probs = mtcnn.detect(img,landmarks=False)\n","        try:\n","            boxes = np.array(boxes,dtype='uint64')\n","            for x1,y1,x2,y2 in boxes:\n","                face = img[y1:y2, x1:x2, :]\n","                face = cv2.resize(face,(40,60))\n","                cv2.imwrite(f'face_{mode}/{i}.jpg',cv2.cvtColor(face, cv2.COLOR_RGB2BGR))\n","                face_sentiment.append(snt)\n","                i+=1\n","        except:\n","              se=5\n","\n","    with open(f\"face_{mode}/face_{mode}.txt\", 'w') as output:\n","        for row in face_sentiment:\n","          output.write(str(row) + '\\n')\n","    os.system(f'zip -r {dir}/face_{mode}.zip face_{mode}')\n","    os.system(f'rm -r face_{mode}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQfWkWzcioId"},"outputs":[],"source":["face_detect('train', train_dataloader, '/content/drive/MyDrive/Project/Phase1')\n","face_detect('val', val_dataloader, '/content/drive/MyDrive/Project/Phase1')\n","face_detect('test', test_dataloader, '/content/drive/MyDrive/Project/Phase1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqlgjmEiiqCU"},"outputs":[],"source":["face_train = faceDataset(mode = 'train', dir='/content/drive/MyDrive/Project/Phase1', transform = ToTensor())\n","face_val = faceDataset(mode = 'val', dir='/content/drive/MyDrive/Project/Phase1', transform = ToTensor())\n","face_test = faceDataset(mode = 'test', dir='/content/drive/MyDrive/Project/Phase1', transform = ToTensor())\n","\n","batch_size = 128\n","face_train_dataloader = DataLoader(face_train, batch_size=batch_size, shuffle=True)\n","face_val_dataloader = DataLoader(face_val, batch_size=batch_size, shuffle=True)\n","face_test_dataloader = DataLoader(face_test, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"N2hVn1Iti30t"},"source":["### Part 1-1-2 (CNN Training)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSGoBk_o8PzD"},"outputs":[],"source":["# Defining the Neural Network Layers, Neurons and Activation Function\n","class CNN1(nn.Module):\n","    def __init__(self, p = 0):\n","        self.p = p\n","        \n","        super(CNN1, self).__init__()\n","        self.flatten = nn.Flatten()\n","        \n","        self.conv2d_relu_stack = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=7, stride=1, padding='same'),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.Dropout(p=self.p),\n","\n","            nn.Conv2d(16, 32, kernel_size=7, stride=1, padding='same'),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.Dropout(p=self.p),\n","            \n","            nn.Conv2d(32, 32, kernel_size=7, stride=1, padding='same'),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, stride=None, padding=0),\n","            nn.Dropout(p=self.p),\n","            \n","            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding='same'),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Dropout(p=self.p),\n","            \n","            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding='same'),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.Dropout(p=self.p),\n","            \n","            nn.Conv2d(128, 128, kernel_size=5, stride=1, padding='same'),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, stride=None, padding=0),\n","            nn.Dropout(p=self.p),\n","            \n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding='valid'),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Dropout(p=self.p),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding='valid'),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Dropout(p=self.p),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding='valid'),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, stride=None, padding=0),\n","            nn.Dropout(p=self.p),\n","            )\n","        \n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Dropout(p=self.p),\n","            nn.Linear(2048, 512),\n","            nn.ReLU(),\n","            nn.Dropout(p=self.p),\n","            nn.Linear(512, 128),\n","            nn.ReLU(),\n","            nn.Dropout(p=self.p),\n","            nn.Linear(128, 32),\n","            nn.ReLU(),\n","            nn.Dropout(p=self.p),\n","            nn.Linear(32, 8),\n","            nn.ReLU(),\n","            nn.Dropout(p=self.p),\n","            nn.Linear(8, 3),\n","            )\n","\n","    def forward(self, x):\n","        x = self.conv2d_relu_stack(x)\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RWNt18-ai3Lm"},"outputs":[],"source":["# Defining the function that will do calculation of Loss and gradiant updates for all Batches in Train Dataset in 1 epoch\n","def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    loss_av, correct = 0, 0\n","\n","    for batch in tqdm(dataloader):\n","        X,y = batch['image'].to(device), batch['sentiment'].to(device)\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        loss_av += float(loss.item())\n","        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    loss_av /= num_batches\n","    correct /= size\n","    return 100*correct, loss_av\n","\n","# Defining the function that will do calculation of Loss and Accuracy over Test Dataset\n","def Accuracy_Loss(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    loss, correct = 0, 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader):\n","            X,y = batch['image'].to(device), batch['sentiment'].to(device)\n","            pred = model(X)\n","            loss += float(loss_fn(pred, y).item())\n","            y_pred = pred.argmax(1)\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    loss /= num_batches\n","    correct /= size\n","    return 100*correct, loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fScGS1tnGW0K","outputId":"fde7ed8a-856e-40d1-d689-88129cb72605"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  9.10it/s]\n","100%|██████████| 48/48 [00:02<00:00, 21.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0903    train acc:38.223 ---- val loss:1.0969   val acc:36.527 \n","\n","epoch 2/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.98it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0896    train acc:38.576 ---- val loss:1.0936   val acc:36.950 \n","\n","epoch 3/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.24it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0896    train acc:38.335 ---- val loss:1.0954   val acc:36.233 \n","\n","epoch 4/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.27it/s]\n","100%|██████████| 48/48 [00:02<00:00, 21.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0892    train acc:38.487 ---- val loss:1.0949   val acc:35.174 \n","\n","epoch 5/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  9.08it/s]\n","100%|██████████| 48/48 [00:02<00:00, 19.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0886    train acc:38.885 ---- val loss:1.0952   val acc:35.207 \n","\n","epoch 6/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.19it/s]\n","100%|██████████| 48/48 [00:02<00:00, 18.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0871    train acc:38.889 ---- val loss:1.0967   val acc:35.859 \n","\n","epoch 7/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.23it/s]\n","100%|██████████| 48/48 [00:02<00:00, 20.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0853    train acc:39.506 ---- val loss:1.0952   val acc:35.060 \n","\n","epoch 8/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  9.13it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0848    train acc:39.506 ---- val loss:1.0966   val acc:35.109 \n","\n","epoch 9/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  9.13it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0850    train acc:39.361 ---- val loss:1.0985   val acc:35.826 \n","\n","epoch 10/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.20it/s]\n","100%|██████████| 48/48 [00:02<00:00, 19.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0846    train acc:39.521 ---- val loss:1.0943   val acc:36.429 \n","\n","epoch 11/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.21it/s]\n","100%|██████████| 48/48 [00:02<00:00, 23.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0846    train acc:39.629 ---- val loss:1.0971   val acc:34.995 \n","\n","epoch 12/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  9.17it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0832    train acc:39.755 ---- val loss:1.0958   val acc:36.152 \n","\n","epoch 13/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  9.17it/s]\n","100%|██████████| 48/48 [00:02<00:00, 20.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0811    train acc:40.242 ---- val loss:1.0930   val acc:38.205 \n","\n","epoch 14/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.20it/s]\n","100%|██████████| 48/48 [00:02<00:00, 18.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0807    train acc:40.220 ---- val loss:1.1006   val acc:36.022 \n","\n","epoch 15/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.20it/s]\n","100%|██████████| 48/48 [00:02<00:00, 20.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0803    train acc:40.231 ---- val loss:1.0979   val acc:36.054 \n","\n","epoch 16/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.19it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0787    train acc:40.242 ---- val loss:1.0941   val acc:37.814 \n","\n","epoch 17/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  9.16it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0778    train acc:40.666 ---- val loss:1.0987   val acc:35.712 \n","\n","epoch 18/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  9.16it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0774    train acc:40.659 ---- val loss:1.0959   val acc:37.129 \n","\n","epoch 19/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  9.17it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0748    train acc:41.165 ---- val loss:1.0976   val acc:36.983 \n","\n","epoch 20/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.22it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0734    train acc:41.329 ---- val loss:1.0989   val acc:36.543 \n","\n","CNN result for epoch 13:\n","Best Model Accuracy for Train Set: 40.24249637371221\n","Best Model Accuracy for Validation Set: 38.204626914304335\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 53/53 [00:02<00:00, 22.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Best Model Accuracy for Test Set: 37.22039964211154\n"]}],"source":["# setting hyper-parameters\n","learning_rate = 1e-4\n","batch_size = 128\n","epochs = 20\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# model and optimizer defenintion\n","CNN1_model = CNN1(0).to(device)\n","CNN1_model.train()\n","optimizer = torch.optim.Adam(CNN1_model.parameters(), lr=learning_rate)\n","face_train_dataloader = DataLoader(face_train, batch_size=batch_size, shuffle=True)\n","\n","# lists to save loss and accuracy\n","train_loss_cnn1 = []\n","val_loss_cnn1 = []\n","train_acu_cnn1 = []\n","val_acu_cnn1 = []\n","\n","best_acu = 0\n","for e in range(epochs):\n","    \n","    print(f'epoch {e+1}/{epochs}:')\n","\n","    CNN1_model.train()\n","    t_loss = train_loop(face_train_dataloader, CNN1_model, loss_fn, optimizer)\n","    train_loss_cnn1.append(t_loss[1]),train_acu_cnn1.append(t_loss[0])\n","\n","    CNN1_model.eval()\n","    v_loss = Accuracy_Loss(face_val_dataloader, CNN1_model, loss_fn)\n","    val_loss_cnn1.append(v_loss[1]),val_acu_cnn1.append(v_loss[0])\n","\n","    print(f'train loss:{t_loss[1]:0.4f}    train acc:{t_loss[0]:0.3f} ---- val loss:{v_loss[1]:0.4f}   val acc:{v_loss[0]:0.3f} \\n')\n","    if val_acu_cnn1[e]>best_acu:\n","        best_acu = val_acu_cnn1[e]\n","        torch.save(CNN1_model, 'CNN1_model.pth')\n","        best_epoch = e+1\n","        \n","best_CNN1_model = torch.load('CNN1_model.pth')\n","best_CNN1_model.eval()\n","print(f\"CNN result for epoch {best_epoch}:\")\n","print(\"Best Model Accuracy for Train Set:\", train_acu_cnn1[best_epoch-1])  \n","print(\"Best Model Accuracy for Validation Set:\",val_acu_cnn1[best_epoch-1])    \n","print(\"Best Model Accuracy for Test Set:\", Accuracy_Loss(face_test_dataloader, best_CNN1_model, loss_fn)[0])  \n","torch.save(best_CNN1_model, '/content/drive/MyDrive/Project/Phase1/CNN1_model.pth')     "]},{"cell_type":"markdown","metadata":{"id":"-6GtxQJGjAMt"},"source":["### Part 1-1-3 (Merging Models)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAdXD2XIqpDq"},"outputs":[],"source":["def merge_model(data_loader, CNN_model, Face_model):\n","  i0, i1, i2 = 0, 0, 0\n","  c0, c1, c2 = 0, 0, 0\n","  acu, acu0, acu1, acu2 = 0, 0, 0, 0\n","  size = len(data_loader.dataset)\n","  trans = transforms.ToTensor()\n","\n","  for batch in tqdm(data_loader):\n","      images = batch['image'].numpy()\n","      sentiments = batch['sentiment'].numpy()\n","      img_snt = zip(images,sentiments) \n","      \n","      for img,snt in img_snt:\n","        face_img = []\n","        boxes, probs = Face_model.detect(img,landmarks=False)\n","\n","        try:\n","            boxes = np.array(boxes,dtype='uint64')\n","            for x1,y1,x2,y2 in boxes:\n","                face = img[y1:y2, x1:x2, :]\n","                face = cv2.resize(face,(40,60))\n","                face_img.append(trans(face).numpy())\n","            X = np.array(face_img)\n","            X = torch.tensor(X).to(device)\n","\n","            pred = CNN_model(X)\n","            label = pred.argmax(1)\n","            label = label.cpu().detach().numpy()\n","            label = np.bincount(label).argmax()\n","            acu += (snt == label)\n","            if snt==0:\n","              c0+=1\n","              acu0+=(label==0)\n","            elif snt==1:\n","              c1+=1\n","              acu1+=(label==1)\n","            else:\n","              c2+=1\n","              acu2+=(label==2)\n","\n","        except:\n","            #guessing the label is 1\n","            acu += (snt == 1)\n","            if snt==0:\n","              i0+=1\n","            elif snt==1:\n","              i1+=1\n","            else:\n","              i2+=1\n","\n","  return acu/(size), (c0,i0,acu0/c0), (c1,i1,acu1/c1), (c2,i2,acu2/c1)"]},{"cell_type":"markdown","source":["#### Evaluation"],"metadata":{"id":"SvJYZyvbB1tk"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DI6I-83TLcHl","outputId":"4b3ad8be-3952-4303-8544-2b82394c8d94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on each face:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 48/48 [00:02<00:00, 22.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Face Model Accuracy for Val Set: 38.204626914304335\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 53/53 [00:02<00:00, 22.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Face Model Accuracy for Test Set: 37.22039964211154\n","Accuracy on each Image:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [04:16<00:00,  6.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Merge Model Val Accuracy: 0.3839620778194746\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [04:20<00:00,  6.52s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"," Merge Model Test Accuracy: 0.38819814485889087\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["best_CNN1_model = torch.load('/content/drive/MyDrive/Project/Phase1/CNN1_model.pth')\n","loss_fn = nn.CrossEntropyLoss()\n","print(\"Accuracy on each face:\")\n","print(\"Face Model Accuracy for Val Set:\", Accuracy_Loss(face_val_dataloader, best_CNN1_model, loss_fn)[0])  \n","print(\"Face Model Accuracy for Test Set:\", Accuracy_Loss(face_test_dataloader, best_CNN1_model, loss_fn)[0])  \n","\n","mtcnn = MTCNN(select_largest=False, post_process=False, device='cuda:0')\n","print(\"Accuracy on each Image:\")\n","a = merge_model(val_dataloader,best_CNN1_model,mtcnn)\n","print('\\n Merge Model Val Accuracy:',a[0])\n","a = merge_model(test_dataloader,best_CNN1_model,mtcnn)\n","print('\\n Merge Model Test Accuracy:',a[0])"]},{"cell_type":"code","source":["print(\"Each Class Details (face-detected/faceless/accuracy on face-detected) :\\n\",a[1:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcEb_LARs-vA","outputId":"5ea70ef9-a21e-4648-ba8b-d17f9beb94f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Each Class Details (face-detected/faceless/accuracy on face-detected) :\n"," ((1049, 249, 0.31649189704480457), (1799, 364, 0.7053918843802113), (1308, 298, 0.0011117287381878821))\n"]}]},{"cell_type":"markdown","source":["## Part 1-2"],"metadata":{"id":"oYuzKvEm7D7J"}},{"cell_type":"markdown","metadata":{"id":"A9KRZ8xDrdr7"},"source":["### Part 1-2-1 (Augmentation)"]},{"cell_type":"markdown","source":["#### Loading Classes and Functions"],"metadata":{"id":"OMNU_s7cX3Oc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"38eS1QIPrnaJ"},"outputs":[],"source":["os.system('git clone https://github.com/amodas/PRIME-augmentations.git')\n","os.system('mv /content/PRIME-augmentations/utils /content')\n","os.system('rm -r /content/PRIME-augmentations')\n","os.system('pip install einops')\n","##########################################################################################################################################\n","import utils\n","from utils.rand_filter import RandomFilter\n","from utils.color_jitter import RandomSmoothColor\n","from utils.diffeomorphism import Diffeo\n","from utils.prime import GeneralizedPRIMEModule\n","from utils.prime import PRIMEAugModule\n","from torch.distributions import Dirichlet, Beta\n","from einops import rearrange, repeat\n","from opt_einsum import contract\n","##########################################################################################################################################\n","class TransformLayer(nn.Module):\n","    def __init__(self, mean, std):\n","        super().__init__()\n","        mean = torch.as_tensor(mean, dtype=torch.float)[None, :, None, None].to(device)\n","        std = torch.as_tensor(std, dtype=torch.float)[None, :, None, None].to(device)\n","        self.mean = nn.Parameter(mean, requires_grad=False)\n","        self.std = nn.Parameter(std, requires_grad=False)\n","\n","    def forward(self, x):\n","        return (x.to(device)).sub(self.mean).div(self.std).to(device)\n","\n","class PRIMEAugModule(torch.nn.Module):\n","    def __init__(self, augmentations):\n","        super().__init__()\n","        self.augmentations = augmentations\n","        self.num_transforms = len(augmentations)\n","\n","    def forward(self, x, mask_t):\n","        aug_x = torch.zeros_like(x)\n","        for i in range(self.num_transforms):\n","            aug_x += self.augmentations[i](x) * mask_t[:, i]\n","        return aug_x"]},{"cell_type":"code","source":["class face_augDataset(Dataset):\n","    def __init__(self, mode, dir, transform=None, target_transform=None, resize=None):\n","        self.mode = mode\n","        os.system(f'unzip -n {dir}/face_aug_{self.mode}.zip')\n","        \n","        file1 = open(f'face_aug_{self.mode}/face_aug_{self.mode}.txt', 'r')\n","        Lines = file1.readlines()\n","        file1.close()\n","        label = []\n","        for line in Lines:\n","            line = line.strip()\n","            label.append(int(line))         \n","        self.sentiment = np.array(label)\n","\n","        \n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.resize = resize\n","        \n","    def __len__(self):\n","        return len(self.sentiment)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(f'face_aug_{self.mode}', f'{idx}.jpg')\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n","        sentiment = self.sentiment[idx] \n","        if self.resize:\n","              image = cv2.resize(image, self.resize) \n","       \n","\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            sentiment = self.target_transform(sentiment)\n","        return {'image':image, 'sentiment':(sentiment)}"],"metadata":{"id":"Rvzt_dMX3bQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4VQFyzbPNoe"},"outputs":[],"source":["def face_aug(mode, augmentation, data_loader, dir):\n","    mtcnn = MTCNN(select_largest=False, post_process=False, device='cuda:0')\n","    i = 0\n","    trans = transforms.ToTensor()\n","    face_sentiment = []\n","    if (os.path.isdir(f'face_aug_{mode}')==0):\n","      os.mkdir(f'face_aug_{mode}')\n","\n","    for batch in tqdm(data_loader):\n","      images = batch['image'].numpy()\n","      sentiments = batch['sentiment'].numpy()\n","      img_snt = zip(images,sentiments) \n","\n","      mean = torch.tensor(np.mean(images))\n","      std = torch.tensor(np.std(images))\n","      prime_module = GeneralizedPRIMEModule(\n","            preprocess=TransformLayer((mean,) ,(std,)),\n","            mixture_width=3,\n","            mixture_depth=-1,\n","            no_jsd=1, max_depth=3,\n","            aug_module=PRIMEAugModule(augmentation))\n","      \n","      for img,snt in img_snt:\n","        boxes, probs = mtcnn.detect(img,landmarks=False)\n","        try:\n","            boxes = np.array(boxes,dtype='uint64')\n","            for x1,y1,x2,y2 in boxes:\n","                #face detect\n","                face = img[y1:y2, x1:x2, :]\n","                face = cv2.resize(face,(60,60))\n","                face = trans(face)[None,:].to(device)\n","                #prime augment\n","                f = prime_module(face)[0]\n","                f = ((f-f.min())/(f.max()-f.min()))\n","                #process to save\n","                f = np.array(f.permute(1,2,0).cpu())\n","                f = cv2.resize(f,(40,60))\n","                f = cv2.cvtColor(f, cv2.COLOR_RGB2BGR)\n","                f = np.array(255*f,dtype='uint64')\n","                cv2.imwrite(f'face_aug_{mode}/{i}.jpg',f)\n","                face_sentiment.append(snt)\n","                i+=1\n","        except:\n","              se=5\n","              \n","    with open(f\"face_aug_{mode}/face_aug_{mode}.txt\", 'w') as output:\n","        for row in face_sentiment:\n","          output.write(str(row) + '\\n')\n","    os.system(f'zip -r {dir}/face_aug_{mode}.zip face_aug_{mode}')\n","    os.system(f'rm -r face_aug_{mode}')\n","    "]},{"cell_type":"markdown","source":["#### Casting  Augmentation"],"metadata":{"id":"YH4am1OUYAs1"}},{"cell_type":"markdown","source":["توضیح در مورد پارامتر های هر فیلتر: فیلتر دیفرم: پارامتر اول میزان دیفرم شدن را کنترل میکند و پارامتر دوم میزان اثر کات مکس را. هرچه کات مکس و کات مین کم و بهم نزدیک باشند اثر این فیلتر بیشتر است.\n","فیلتر رندم کالر: در این فیلتر پارامتر تی کنترل کننده ی قدرت اعمال فیلتر است و با 0 کردن آن فیلتر بی اثر می شود و تغیر پهنای باند و ضریب کات میزان این تغییر و نویزی بودن آن را کنترل می کند. در کل این فیلتر تم رنگی را تغییر میدهد.\n","فیلتر رندم فیلتر: این فیلتر تصویر را تار میکند و میزان تار شدن با ضریب سیگما کنترل می شود. پارامتر سایز کرنل نیز اندازه پنچره هایی که روی آن تارشدن اعمال می شود را تعیین می کند"],"metadata":{"id":"FU4ZCOOIrJPX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oERD8g1zJtM0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5c5659a-de61-4b62-cd37-f0c4fb7c4a05"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [25:38<00:00,  9.68s/it]\n","100%|██████████| 40/40 [05:38<00:00,  8.46s/it]\n","100%|██████████| 40/40 [05:46<00:00,  8.66s/it]\n"]}],"source":["# PRIME Filters\n","augmentations = []\n","diffeo = Diffeo(sT=1, rT=1, scut=1, rcut=1, cutmin=100, cutmax=200, alpha=1.0, stochastic=True)\n","augmentations.append(diffeo)\n","color = RandomSmoothColor(cut=1, T=0.01,  freq_bandwidth=1 , stochastic=True)\n","augmentations.append(color)\n","filt = RandomFilter(kernel_size=7, sigma=0.5, stochastic=True)\n","augmentations.append(filt)\n","\n","# Casting Filters\n","face_aug('train', augmentations, train_dataloader,'/content/drive/MyDrive/Project/Phase1')\n","face_aug('val', augmentations, val_dataloader,'/content/drive/MyDrive/Project/Phase1')\n","face_aug('test', augmentations, test_dataloader,'/content/drive/MyDrive/Project/Phase1')"]},{"cell_type":"markdown","source":["### Part 1-2-2 (Evaluate Model on Augmented Data)"],"metadata":{"id":"WGUwKqDdnnbK"}},{"cell_type":"code","source":["# Building Datasets and Dataloaders\n","face_aug_train = face_augDataset(mode = 'train', dir='/content/drive/MyDrive/Project/Phase1', transform = ToTensor())\n","face_aug_val = face_augDataset(mode = 'val', dir='/content/drive/MyDrive/Project/Phase1', transform = ToTensor())\n","face_aug_test = face_augDataset(mode = 'test', dir='/content/drive/MyDrive/Project/Phase1', transform = ToTensor())\n","\n","batch_size = 128\n","face_aug_train_dataloader = DataLoader(face_aug_train, batch_size=batch_size, shuffle=True)\n","face_aug_val_dataloader = DataLoader(face_aug_val, batch_size=batch_size, shuffle=True)\n","face_aug_test_dataloader = DataLoader(face_aug_test, batch_size=batch_size, shuffle=True)\n","\n","\n","\n","best_CNN1_model = torch.load('/content/drive/MyDrive/Project/Phase1/CNN1_model.pth')\n","best_CNN1_model.eval()\n","print(f\"Pre-Trained CNN result on Augmented Data :\")\n","print(\"Best Model Accuracy for Train Set:\", Accuracy_Loss(face_aug_train_dataloader, best_CNN1_model, loss_fn)[0])  \n","print(\"Best Model Accuracy for Validation Set:\", Accuracy_Loss(face_aug_val_dataloader, best_CNN1_model, loss_fn)[0])    \n","print(\"Best Model Accuracy for Test Set:\", Accuracy_Loss(face_aug_test_dataloader, best_CNN1_model, loss_fn)[0])  \n","torch.save(best_CNN1_model, '/content/drive/MyDrive/Project/Phase1/CNN1_model.pth')     "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7cyavwtHnvdp","outputId":"09b4302b-2d4b-4022-cdd4-bc94ef7cc30a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pre-Trained CNN result on Augmented Data :\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:10<00:00, 19.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Best Model Accuracy for Train Set: 37.63156915981701\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 48/48 [00:02<00:00, 22.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Best Model Accuracy for Validation Set: 36.55913978494624\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 53/53 [00:02<00:00, 22.72it/s]"]},{"output_type":"stream","name":"stdout","text":["Best Model Accuracy for Test Set: 36.74321503131524\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Part 1-2-3 (Tuning CNN1 Model with Augmented Data)"],"metadata":{"id":"6z0rz7wJCmP0"}},{"cell_type":"code","source":["# setting hyper-parameters\n","learning_rate = 1e-4\n","batch_size = 128\n","epochs = 20\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# model and data defenintion\n","CNN1_model = torch.load('/content/drive/MyDrive/Project/Phase1/CNN1_model.pth')\n","CNN1_model.train()\n","optimizer = torch.optim.Adam(CNN1_model.parameters(), lr=learning_rate)\n","\n","face_aug_train = face_augDataset(mode = 'train', dir='/content/drive/MyDrive/Project/Phase1', transform = ToTensor())\n","batch_size = 128\n","face_aug_train_dataloader = DataLoader(face_aug_train, batch_size=batch_size, shuffle=True)\n","\n","# lists to save loss and accuracy\n","train_loss_cnn1 = []\n","val_loss_cnn1 = []\n","train_acu_cnn1 = []\n","val_acu_cnn1 = []\n","\n","best_acu = 0\n","for e in range(epochs):\n","    \n","    print(f'epoch {e+1}/{epochs}:')\n","\n","    CNN1_model.train()\n","    t_loss = train_loop(face_aug_train_dataloader, CNN1_model, loss_fn, optimizer)\n","    train_loss_cnn1.append(t_loss[1]),train_acu_cnn1.append(t_loss[0])\n","\n","    CNN1_model.eval()\n","    v_loss = Accuracy_Loss(face_val_dataloader, CNN1_model, loss_fn)\n","    val_loss_cnn1.append(v_loss[1]),val_acu_cnn1.append(v_loss[0])\n","\n","    print(f'train loss:{t_loss[1]:0.4f}    train acc:{t_loss[0]:0.3f} ---- val loss:{v_loss[1]:0.4f}   val acc:{v_loss[0]:0.3f} \\n')\n","    if val_acu_cnn1[e]>best_acu:\n","        best_acu = val_acu_cnn1[e]\n","        torch.save(CNN1_model, 'aug_CNN1_model.pth')\n","        best_epoch = e+1\n","        \n","best_CNN1_model = torch.load('aug_CNN1_model.pth')\n","best_CNN1_model.eval()\n","print(f\"CNN result for epoch {best_epoch}:\")\n","print(\"Best Model Accuracy for Train Set:\", train_acu_cnn1[best_epoch-1])  \n","print(\"Best Model Accuracy for Validation Set:\",val_acu_cnn1[best_epoch-1])    \n","print(\"Best Model Accuracy for Test Set:\", Accuracy_Loss(face_test_dataloader, best_CNN1_model, loss_fn)[0])  \n","torch.save(best_CNN1_model, '/content/drive/MyDrive/Project/Phase1/aug_CNN1_model.pth')     "],"metadata":{"id":"TAO6hVTzFOIM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7ddb0781-762d-457b-8f21-db5dfa876e88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:22<00:00,  9.23it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0899    train acc:38.033 ---- val loss:1.0923   val acc:35.712 \n","\n","epoch 2/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.90it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0896    train acc:38.178 ---- val loss:1.0929   val acc:35.582 \n","\n","epoch 3/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.81it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0885    train acc:38.606 ---- val loss:1.0943   val acc:34.979 \n","\n","epoch 4/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.99it/s]\n","100%|██████████| 48/48 [00:02<00:00, 18.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0883    train acc:38.528 ---- val loss:1.0936   val acc:35.940 \n","\n","epoch 5/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.99it/s]\n","100%|██████████| 48/48 [00:02<00:00, 21.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0885    train acc:38.680 ---- val loss:1.0936   val acc:35.419 \n","\n","epoch 6/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.92it/s]\n","100%|██████████| 48/48 [00:02<00:00, 20.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0876    train acc:38.547 ---- val loss:1.0964   val acc:35.207 \n","\n","epoch 7/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.98it/s]\n","100%|██████████| 48/48 [00:02<00:00, 18.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0870    train acc:38.866 ---- val loss:1.0945   val acc:35.891 \n","\n","epoch 8/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.97it/s]\n","100%|██████████| 48/48 [00:02<00:00, 19.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0856    train acc:39.183 ---- val loss:1.0935   val acc:36.054 \n","\n","epoch 9/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.96it/s]\n","100%|██████████| 48/48 [00:02<00:00, 20.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0855    train acc:39.000 ---- val loss:1.0961   val acc:35.288 \n","\n","epoch 10/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.99it/s]\n","100%|██████████| 48/48 [00:02<00:00, 23.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0835    train acc:39.387 ---- val loss:1.0979   val acc:35.011 \n","\n","epoch 11/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.96it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0838    train acc:39.588 ---- val loss:1.0953   val acc:36.950 \n","\n","epoch 12/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.90it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0818    train acc:39.711 ---- val loss:1.1010   val acc:34.604 \n","\n","epoch 13/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.98it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0811    train acc:39.774 ---- val loss:1.0958   val acc:36.201 \n","\n","epoch 14/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.98it/s]\n","100%|██████████| 48/48 [00:02<00:00, 20.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0783    train acc:40.607 ---- val loss:1.0995   val acc:36.136 \n","\n","epoch 15/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.92it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0783    train acc:40.250 ---- val loss:1.1011   val acc:36.755 \n","\n","epoch 16/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.91it/s]\n","100%|██████████| 48/48 [00:02<00:00, 20.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0770    train acc:40.510 ---- val loss:1.0985   val acc:36.266 \n","\n","epoch 17/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.92it/s]\n","100%|██████████| 48/48 [00:02<00:00, 18.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0746    train acc:41.020 ---- val loss:1.1010   val acc:35.630 \n","\n","epoch 18/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.96it/s]\n","100%|██████████| 48/48 [00:02<00:00, 18.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0746    train acc:41.183 ---- val loss:1.0979   val acc:36.527 \n","\n","epoch 19/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.98it/s]\n","100%|██████████| 48/48 [00:02<00:00, 19.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0703    train acc:41.674 ---- val loss:1.1030   val acc:35.777 \n","\n","epoch 20/20:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 211/211 [00:23<00:00,  8.91it/s]\n","100%|██████████| 48/48 [00:02<00:00, 22.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0667    train acc:42.348 ---- val loss:1.1056   val acc:36.152 \n","\n","CNN result for epoch 11:\n","Best Model Accuracy for Train Set: 39.587904935470675\n","Best Model Accuracy for Validation Set: 36.950146627565985\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 53/53 [00:04<00:00, 11.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Best Model Accuracy for Test Set: 33.596779003877124\n"]}]},{"cell_type":"markdown","source":["همانطور که مشاهده میشود این اگمنتیشن تاثیر به سزایی در بهبود دقت ندارد،زیرا احساس از چهره ها استخراج میشود و با این کشیدگی در چهره و تغییراتی نظیر این دقت محدود شبکه را نیز دسخوش تغییر بیشتر کرده ایم."],"metadata":{"id":"_dFWv4NX4_wj"}},{"cell_type":"markdown","metadata":{"id":"l4D1jhE9jGju"},"source":["# Part 2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NX4LnA1Mldf"},"outputs":[],"source":["class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 8),\n","            nn.ReLU(),\n","            nn.Linear(8, 3),\n","        )\n","\n","    def forward(self, x):\n","        logits = self.linear_relu_stack(x)\n","        return logits"]},{"cell_type":"code","source":["train_data = MSCTDDataset(img_dir = '/content/drive/MyDrive/Project/Phase0', mode = 'train', resize = (640,316), transform = ToTensor())\n","val_data = MSCTDDataset(img_dir = '/content/drive/MyDrive/Project/Phase0', mode = 'dev', resize = (640,316), transform = ToTensor())\n","test_data = MSCTDDataset(img_dir = '/content/drive/MyDrive/Project/Phase0', mode = 'test',  resize = (640,316), transform = ToTensor())\n","\n","batch_size=128\n","train_dataloader =  DataLoader(train_data, batch_size=batch_size)\n","val_dataloader =  DataLoader(val_data, batch_size=batch_size)\n","test_dataloader =  DataLoader(test_data, batch_size=batch_size)"],"metadata":{"id":"fY20rr3GBmsh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac919d83-f4e4-4dfc-f35e-424eb0e5fca3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading Text Files\n","Loading Train Images\n","Train Images Count: 20244\n","\n","Loading Text Files\n","Loading Validation Images\n","Dev Images Count: 5066\n","\n","Loading Text Files\n","Loading Test Images\n","Test Images Count: 5070\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oun2cAYCTGRp","outputId":"9ce065ee-3bbe-40f0-fd05-2e9619b5f8d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1/10:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [09:25<00:00,  3.56s/it]\n","100%|██████████| 40/40 [02:15<00:00,  3.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0896    train acc:38.696 ---- val loss:1.0984   val acc:36.303 \n","\n","epoch 2/10:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [09:20<00:00,  3.53s/it]\n","100%|██████████| 40/40 [02:13<00:00,  3.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0897    train acc:38.710 ---- val loss:1.0968   val acc:36.303 \n","\n","epoch 3/10:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [09:15<00:00,  3.49s/it]\n","100%|██████████| 40/40 [02:11<00:00,  3.28s/it]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0894    train acc:38.710 ---- val loss:1.0966   val acc:36.303 \n","\n","epoch 4/10:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [09:15<00:00,  3.50s/it]\n","100%|██████████| 40/40 [02:11<00:00,  3.28s/it]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0891    train acc:38.631 ---- val loss:1.0966   val acc:36.303 \n","\n","epoch 5/10:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [09:19<00:00,  3.52s/it]\n","100%|██████████| 40/40 [02:14<00:00,  3.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0889    train acc:38.523 ---- val loss:1.0968   val acc:36.303 \n","\n","epoch 6/10:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [09:15<00:00,  3.49s/it]\n","100%|██████████| 40/40 [02:13<00:00,  3.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0888    train acc:38.340 ---- val loss:1.0970   val acc:36.303 \n","\n","epoch 7/10:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [09:11<00:00,  3.47s/it]\n","100%|██████████| 40/40 [02:14<00:00,  3.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0883    train acc:38.458 ---- val loss:1.0975   val acc:36.303 \n","\n","epoch 8/10:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [09:13<00:00,  3.48s/it]\n","100%|██████████| 40/40 [02:11<00:00,  3.28s/it]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0880    train acc:38.286 ---- val loss:1.0983   val acc:36.303 \n","\n","epoch 9/10:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [09:13<00:00,  3.48s/it]\n","100%|██████████| 40/40 [02:11<00:00,  3.30s/it]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0876    train acc:38.523 ---- val loss:1.0986   val acc:36.303 \n","\n","epoch 10/10:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [09:12<00:00,  3.48s/it]\n","100%|██████████| 40/40 [02:11<00:00,  3.30s/it]\n"]},{"output_type":"stream","name":"stdout","text":["train loss:1.0874    train acc:38.725 ---- val loss:1.0980   val acc:36.303 \n","\n","Resnet50 result for epoch 1:\n","Best Model Accuracy for Train Set: 38.69565217391304\n","Best Model Accuracy for Validation Set: 36.30258739877543\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [02:08<00:00,  3.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Best Model Accuracy for Test Set: 42.68798105387803\n"]}],"source":["# backbone model settings\n","backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n","for param in backbone.parameters():\n","      param.requires_grad = False\n","\n","backbone.fc = nn.Linear(2048, 1024)\n","Resnet50_model = nn.Sequential(backbone,MyModel()).to(device)\n","\n","# setting hyper-parameters\n","learning_rate = 1e-4\n","batch_size = 128\n","epochs = 10\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# model and optimizer defenintion\n","Resnet50_model.train()\n","optimizer = torch.optim.Adam(Resnet50_model.parameters(), lr=learning_rate)\n","\n","# lists to save loss and accuracy\n","train_loss_cnn1 = []\n","val_loss_cnn1 = []\n","train_acu_cnn1 = []\n","val_acu_cnn1 = []\n","\n","best_acu = 0\n","for e in (range(epochs)):\n","    \n","    print(f'epoch {e+1}/{epochs}:')\n","    Resnet50_model.train()\n","    t_loss = train_loop(train_dataloader, Resnet50_model, loss_fn, optimizer)\n","    \n","    train_loss_cnn1.append(t_loss[1]),train_acu_cnn1.append(t_loss[0])\n","\n","    v_loss = Accuracy_Loss(val_dataloader, Resnet50_model, loss_fn)\n","    val_loss_cnn1.append(v_loss[1]),val_acu_cnn1.append(v_loss[0])\n","\n","    print(f'train loss:{t_loss[1]:0.4f}    train acc:{t_loss[0]:0.3f} ---- val loss:{v_loss[1]:0.4f}   val acc:{v_loss[0]:0.3f} \\n')\n","    if val_acu_cnn1[e]>best_acu:\n","        best_acu = val_acu_cnn1[e]\n","        torch.save(Resnet50_model, 'Resnet50_model.pth')\n","        best_epoch = e+1\n","        \n","best_Resnet50_model = torch.load('Resnet50_model.pth')\n","best_Resnet50_model.eval()\n","print(f\"Resnet50 result for epoch {best_epoch}:\")\n","print(\"Best Model Accuracy for Train Set:\", train_acu_cnn1[best_epoch-1])  \n","print(\"Best Model Accuracy for Validation Set:\",val_acu_cnn1[best_epoch-1])  \n","print(\"Best Model Accuracy for Test Set:\", Accuracy_Loss(test_dataloader, best_Resnet50_model, loss_fn)[0])  \n","\n","torch.save(best_Resnet50_model, '/content/drive/MyDrive/Project/Phase1/Resnet50_model2.pth')  \n"]},{"cell_type":"markdown","source":["دقت این شبکه بهتر است، اما با مشاهده خروجی مشاهده میشود که این شبکه برای بخش عمده ای از ورودی ، خروجی 1 تولید میکند و از آنجایی که بیشتر داده تست ما 1 هستند، دقت بهتر شده پس بهتر است در بخش بعد انرا با مدل بخش قبل ترکیب کرد."],"metadata":{"id":"QKff3M0d5bwR"}},{"cell_type":"markdown","metadata":{"id":"BmZl_M0fue2z"},"source":["# Part3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K40ELaiAuhcr"},"outputs":[],"source":["def final_merge_model(data_loader, CNN_model, Face_model, resnet_model):\n","    i = 0\n","    acu = 0\n","    size = len(data_loader.dataset)\n","    trans = transforms.ToTensor()\n","    for batch in tqdm(data_loader):\n","        y_pred = []\n","        images = batch['image'].numpy()\n","        sentiments = batch['sentiment'].numpy()\n","        X3 = batch['image']\n","        img_snt = zip(images,sentiments,X3) \n","        \n","      \n","        for img,snt,X3 in img_snt:\n","          face_img = []\n","          final_labels = []\n","          boxes, probs = Face_model.detect(img,landmarks=False)\n","          try:\n","              boxes = np.array(boxes,dtype='uint64')\n","              for x1,y1,x2,y2 in boxes:\n","                  face = img[y1:y2, x1:x2, :]\n","                  face = cv2.resize(face,(40,60))\n","                  face_img.append(face)\n","                  X2 = trans(face).to(device)\n","                  x,y,z = X2.shape\n","                  X2 = X2.resize(1,x,y,z)\n","                  pred = CNN_model(X2)\n","                  label = pred.argmax(1).cpu().numpy()\n","                  label = label[0]\n","                  final_labels.append(label)\n","                  \n","              if(len(boxes)>1):\n","                  X3 = trans(X3.numpy()).to(device)\n","                  x,y,z = X3.shape\n","                  X3 = X3.resize(1,x,y,z)\n","                  logit2 = resnet_model(X3)\n","                  label = logit2.argmax(1).cpu().numpy()\n","                  label = label[0]\n","                  final_labels.append(label)\n","                  final_labels.append(label)\n","              \n","              label = np.bincount(final_labels).argmax()\n","              y_pred.append(label)\n","          except:\n","                X3 = trans(X3.numpy()).to(device)\n","                x,y,z = X3.shape\n","                X3 = X3.resize(1,x,y,z)\n","                logit2 = resnet_model(X3)\n","                label = logit2.argmax(1).cpu().numpy()\n","                label = label[0]\n","                y_pred.append(label)\n","\n","        acu += np.count_nonzero(np.equal(sentiments,y_pred))\n","    return acu/size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yltl8nzEkpyq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0c7c90e-b3d2-4340-d938-8f044b5f5b65"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading Text Files\n","\n","Loading Text Files\n","Loading Test Images\n","Test Images Count: 5070\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:761: UserWarning: non-inplace resize is deprecated\n","  warnings.warn(\"non-inplace resize is deprecated\")\n","100%|██████████| 40/40 [04:59<00:00,  7.49s/it]\n"]},{"output_type":"execute_result","data":{"text/plain":["0.40299980264456287"]},"metadata":{},"execution_count":17}],"source":["val_data = MSCTDDataset(img_dir = '/content/drive/MyDrive/Project/Phase0', mode = 'val',  resize = (640,316))\n","test_data = MSCTDDataset(img_dir = '/content/drive/MyDrive/Project/Phase0', mode = 'test',  resize = (640,316))\n","batch_size = 128\n","val_dataloader =  DataLoader(val_data, batch_size=batch_size)\n","test_dataloader =  DataLoader(test_data, batch_size=batch_size)\n","\n","best_CNN1_model = torch.load('/content/drive/MyDrive/Project/Phase1/CNN1_model.pth')\n","best_resnet_model = torch.load('/content/drive/MyDrive/Project/Phase1/Resnet50_model2.pth')\n","mtcnn = MTCNN(select_largest=False, post_process=False, device='cuda:0')\n","final_merge_model(test_dataloader,best_CNN1_model,mtcnn,best_resnet_model)"]},{"cell_type":"markdown","source":["برای تلفیق دو مدل اینگونه عمل میکنیم که اگر تصویر تنها یک چهره داشت از مدل بخش 1 استفاده میکنیم-اگرتعداد چهره ها بیش از یکی بود علاوه بر برچسب های حاصل از مدل 1 دو برچسب هم از مدل2 در نظر میگیریم و سپس رای گیری انجام میدهیم، در صورتی هم که چهره ای در تصویر نبود خروجی تمامی وابسته به مدل 2 می باشد."],"metadata":{"id":"SURC6hVd67xA"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["5fDYW3XFeIVi","34E9JjFa6_zm","g8q0OEbeigtb","N2hVn1Iti30t","-6GtxQJGjAMt","oYuzKvEm7D7J","OMNU_s7cX3Oc","YH4am1OUYAs1","WGUwKqDdnnbK","6z0rz7wJCmP0","l4D1jhE9jGju","BmZl_M0fue2z"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}